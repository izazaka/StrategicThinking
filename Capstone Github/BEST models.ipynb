{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24f605c1",
   "metadata": {},
   "source": [
    "# Best Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1eb4f518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category = DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "992f19f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib as mpl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns \n",
    "\n",
    "sns.set(color_codes = True)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from textblob import Word\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "import gender_guesser.detector as gender\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "\n",
    "import emoji\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "from matplotlib.patches import ConnectionPatch\n",
    "\n",
    "from langdetect import detect\n",
    "\n",
    "from nltk.probability import *\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk import pos_tag\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from afinn import Afinn\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from nltk.text import Text\n",
    "\n",
    "\n",
    "import string\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be21c719",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml = pd.read_csv(\"ml.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "541533bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml.dropna(subset = [\"cleaned_text\"], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "820eecff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>starRating</th>\n",
       "      <th>Gender_Numeric</th>\n",
       "      <th>Year</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>word_count</th>\n",
       "      <th>avg_word</th>\n",
       "      <th>sentiment_label</th>\n",
       "      <th>sentiment_ml</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1647</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "      <td>good</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      starRating  Gender_Numeric  Year cleaned_text  sentiment_score  \\\n",
       "1647           5               1  2018         good              0.7   \n",
       "\n",
       "      word_count  avg_word sentiment_label  sentiment_ml  sentiment  \n",
       "1647           1       4.0        Positive             2          0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4b1291f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "0    1645\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml[\"sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b97bfd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c46af0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range = (1, 1), max_features = 500, tokenizer = word_tokenize)\n",
    "X = tfidf.fit_transform(ml[\"cleaned_text\"])\n",
    "y = ml[\"sentiment_ml\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.10, \n",
    "                                                    random_state = 12, stratify = y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977d0db4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2af8c985",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02355375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8787878787878788\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.57      0.73         7\n",
      "           1       0.86      0.86      0.86        70\n",
      "           2       0.89      0.92      0.91        88\n",
      "\n",
      "    accuracy                           0.88       165\n",
      "   macro avg       0.92      0.78      0.83       165\n",
      "weighted avg       0.88      0.88      0.88       165\n",
      "\n",
      "Accuracy on Training Set: 0.9918918918918919\n",
      "Accuracy on Test Set: 0.8787878787878788\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xgb = XGBClassifier()\n",
    "xgb.fit(X_train,y_train)\n",
    "predicted_xgb = xgb.predict(X_test)\n",
    "accuracy_xgb = accuracy_score(y_test, predicted_xgb)\n",
    "print('Accuracy:', accuracy_xgb)\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, predicted_xgb))\n",
    "\n",
    "y_train_pred = xgb.predict(X_train)\n",
    "y_test_pred = xgb.predict(X_test)\n",
    "accuracy_train = accuracy_score(y_train, y_train_pred)\n",
    "accuracy_test = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Accuracy on Training Set:\", accuracy_train)\n",
    "print(\"Accuracy on Test Set:\", accuracy_test)\n",
    "\n",
    "\n",
    "cm_xgb = confusion_matrix(y_test, predicted_xgb)\n",
    "labels = ['Negative', 'Neutral', 'Positive']\n",
    "sns.heatmap(cm_xgb, annot = True, cmap = 'Blues', fmt = 'g', xticklabels = labels, yticklabels = labels)\n",
    "plt.xlabel('Predicted sentiment')\n",
    "plt.ylabel('True sentiment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197728b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "469b1fa2",
   "metadata": {},
   "source": [
    "## Linear Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d80aa30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8545454545454545\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.43      0.60         7\n",
      "           1       0.86      0.80      0.83        70\n",
      "           2       0.85      0.93      0.89        88\n",
      "\n",
      "    accuracy                           0.85       165\n",
      "   macro avg       0.90      0.72      0.77       165\n",
      "weighted avg       0.86      0.85      0.85       165\n",
      "\n",
      "Accuracy on Training Set: 0.9682432432432433\n",
      "Accuracy on Test Set: 0.8545454545454545\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "svc = LinearSVC(random_state=12)\n",
    "svc.fit(X_train, y_train)\n",
    "predicted_svc = svc.predict(X_test)\n",
    "accuracy_svc = accuracy_score(y_test, predicted_svc)\n",
    "print('Accuracy:', accuracy_svc)\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, predicted_svc))\n",
    "\n",
    "y_train_pred = svc.predict(X_train)\n",
    "y_test_pred = svc.predict(X_test)\n",
    "accuracy_train = accuracy_score(y_train, y_train_pred)\n",
    "accuracy_test = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Accuracy on Training Set:\", accuracy_train)\n",
    "print(\"Accuracy on Test Set:\", accuracy_test)\n",
    "\n",
    "\n",
    "cm_svc = confusion_matrix(y_test, predicted_svc)\n",
    "labels = ['Negative', 'Neutral', 'Positive']\n",
    "sns.heatmap(cm_svc, annot=True, cmap='Blues', fmt='g', xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel('Predicted sentiment')\n",
    "plt.ylabel('True sentiment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bde7725",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "620ec805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 1000, 'subsample': 1.0}\n",
      "Best Cross-Validation Score: 0.854054054054054\n",
      "Test Set Accuracy with Best Parameters: 0.8787878787878788\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "  'learning_rate': [0.05, 0.1, 0.5],\n",
    "  'max_depth': [2, 5, 10, 12],\n",
    "  'n_estimators': [100, 300, 500, 1000],\n",
    "  'subsample': [0.05, 0.25, 0.5, 1.0],\n",
    "  'colsample_bytree': [0.05, 0.25, 0.5, 1.0]\n",
    "}\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "grid_search = GridSearchCV(estimator = xgb, param_grid = param_grid, scoring = 'accuracy', cv = 5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print('Best Parameters:', grid_search.best_params_)\n",
    "print('Best Cross-Validation Score:', grid_search.best_score_)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "predicted_xgb = best_model.predict(X_test)\n",
    "accuracy_xgb = accuracy_score(y_test, predicted_xgb)\n",
    "print('Test Set Accuracy with Best Parameters:', accuracy_xgb)\n",
    "\n",
    "\n",
    "cv_results = grid_search.cv_results_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53c8f81a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.04302158, 0.05035191, 0.04195199, 0.04365063, 0.09771714,\n",
       "        0.1085722 , 0.11462021, 0.12034793, 0.18013186, 0.25600185,\n",
       "        0.30983362, 0.25665598, 0.44133158, 0.4952177 , 0.46804633,\n",
       "        0.48908544, 0.04740825, 0.07761769, 0.08280506, 0.08710427,\n",
       "        0.14133539, 0.23979125, 0.24706097, 0.33288622, 0.20947685,\n",
       "        0.41687112, 0.48736486, 0.38383808, 0.4844182 , 1.2552515 ,\n",
       "        0.84792223, 0.72958336, 0.04117789, 0.09930038, 0.19563737,\n",
       "        0.15786967, 0.12662907, 0.27344074, 0.34484901, 0.41713614,\n",
       "        0.2417028 , 0.44379044, 0.54212527, 0.63805833, 0.36885095,\n",
       "        0.85130844, 1.08674302, 1.10218339, 0.04107265, 0.09934173,\n",
       "        0.13511724, 0.17687869, 0.146134  , 0.3157608 , 0.4391665 ,\n",
       "        0.46971397, 0.19613843, 0.46305265, 0.74282846, 0.73326397,\n",
       "        0.38345976, 0.88433151, 1.06055908, 1.26097198, 0.05433083,\n",
       "        0.04297504, 0.0408927 , 0.039253  , 0.09666853, 0.11519442,\n",
       "        0.11300335, 0.11300001, 0.16199965, 0.18165383, 0.1870831 ,\n",
       "        0.18499441, 0.30305619, 0.35328555, 0.41447921, 0.37654123,\n",
       "        0.04913821, 0.06508799, 0.0833292 , 0.083781  , 0.16648011,\n",
       "        0.19988613, 0.22067409, 0.22347999, 0.18374538, 0.28934503,\n",
       "        0.34415989, 0.38829889, 0.34789214, 0.54649992, 0.67162337,\n",
       "        0.62364802, 0.0406096 , 0.091433  , 0.12201381, 0.14236679,\n",
       "        0.11545291, 0.25076098, 0.32587061, 0.36364994, 0.18678522,\n",
       "        0.40759234, 0.53387694, 0.54328456, 0.34907479, 0.66529474,\n",
       "        0.82393236, 0.92966542, 0.0409699 , 0.09781494, 0.13156857,\n",
       "        0.16635103, 0.11271682, 0.35787902, 0.39428301, 0.42481203,\n",
       "        0.19674821, 0.42048573, 0.58525743, 0.61060772, 0.35866952,\n",
       "        0.70249877, 0.85335178, 1.02311096, 0.03319378, 0.0380415 ,\n",
       "        0.03804908, 0.03925128, 0.0915154 , 0.10855331, 0.10955925,\n",
       "        0.11151423, 0.14938636, 0.17288718, 0.1781682 , 0.17565703,\n",
       "        0.28355293, 0.3437026 , 0.3627666 , 0.39065323, 0.03730001,\n",
       "        0.06005754, 0.06592283, 0.07308955, 0.10123348, 0.1548593 ,\n",
       "        0.17119212, 0.17991681, 0.15958738, 0.24369979, 0.2581634 ,\n",
       "        0.26746211, 0.30315766, 0.41069398, 0.42138495, 0.50533781,\n",
       "        0.04304385, 0.08135347, 0.10192175, 0.11683459, 0.10124407,\n",
       "        0.18332577, 0.2652586 , 0.28056765, 0.2000854 , 0.36839967,\n",
       "        0.38940592, 0.45495024, 0.42779336, 0.52023058, 0.59038005,\n",
       "        0.65344214, 0.04724193, 0.11339145, 0.14778962, 0.16490684,\n",
       "        0.14558358, 0.2773448 , 0.36126318, 0.33296099, 0.21987267,\n",
       "        0.44138503, 0.43539381, 0.48628292, 0.42072172, 0.61674137,\n",
       "        0.70849152, 0.68363142, 0.0547297 , 0.06670256, 0.07140503,\n",
       "        0.06780949, 0.14067726, 0.15477891, 0.16696415, 0.17023959,\n",
       "        0.20369782, 0.23505874, 0.25280261, 0.25564499, 0.44078755,\n",
       "        0.45107384, 0.49032364, 0.48917646, 0.06601882, 0.09707766,\n",
       "        0.11276169, 0.12860174, 0.18353782, 0.27882028, 0.30896883,\n",
       "        0.4043294 , 0.33040366, 0.48422384, 0.49708142, 0.57028751,\n",
       "        0.55496078, 0.86956797, 1.00133996, 1.22340741, 0.10049047,\n",
       "        0.26413088, 0.32204428, 0.40946116, 0.30279422, 0.5848762 ,\n",
       "        0.62557578, 0.80686727, 0.52144055, 0.80606508, 0.98569679,\n",
       "        1.11572118, 0.69981966, 2.21538329, 2.11008196, 2.23941636,\n",
       "        0.09563437, 0.2253695 , 0.28774424, 0.37371936, 0.2855814 ,\n",
       "        0.66070147, 0.74292002, 0.83759661, 0.40402212, 0.90991807,\n",
       "        1.26621361, 1.34342031, 0.81174803, 1.6086905 , 2.22243485,\n",
       "        2.46236796, 0.05514898, 0.04949341, 0.05356641, 0.06620193,\n",
       "        0.11680942, 0.14854727, 0.19451838, 0.15756617, 0.22856951,\n",
       "        0.23115845, 0.31587052, 0.30904489, 0.41653681, 0.62997203,\n",
       "        0.59044814, 0.72141538, 0.08328452, 0.11649623, 0.12271028,\n",
       "        0.1412334 , 0.19112816, 0.30392966, 0.3069488 , 0.34652033,\n",
       "        0.37416878, 0.45237751, 0.59089847, 0.71093345, 0.78209357,\n",
       "        1.07696872, 1.17398558, 1.10798702, 0.10421319, 0.18231201,\n",
       "        0.20365992, 0.25341358, 0.21629276, 0.57904458, 0.64364386,\n",
       "        0.71979289, 0.45989075, 0.86004925, 0.9756506 , 1.6035336 ,\n",
       "        1.25749202, 1.85566249, 2.17241473, 1.99945941, 0.17260885,\n",
       "        0.3648283 , 0.43287654, 0.48963408, 0.37860847, 0.65777411,\n",
       "        0.72841334, 0.82273979, 0.43918357, 1.06085682, 1.11768451,\n",
       "        1.82171845, 0.869519  , 2.00657935, 2.23539248, 2.69213033,\n",
       "        0.08260961, 0.09232907, 0.09352241, 0.0908145 , 0.23220921,\n",
       "        0.27290635, 0.2940176 , 0.26955447, 0.28902588, 0.29764986,\n",
       "        0.30109663, 0.33166242, 0.67849531, 0.89058304, 0.85655341,\n",
       "        0.78996906, 0.07678523, 0.1319057 , 0.103193  , 0.12930112,\n",
       "        0.17778077, 0.22859578, 0.26076422, 0.30590177, 0.25137215,\n",
       "        0.36057634, 0.40285702, 0.60840607, 0.65609713, 0.87004666,\n",
       "        1.24989181, 1.15202446, 0.16117706, 0.25375066, 0.27420802,\n",
       "        0.30770226, 0.19576464, 0.40394616, 0.42751584, 0.50602918,\n",
       "        0.40724258, 0.57750716, 0.566043  , 0.85893741, 0.82364359,\n",
       "        1.1624999 , 0.99237242, 1.62898273, 0.16793551, 0.38488083,\n",
       "        0.34288816, 0.40485606, 0.23375926, 0.39690957, 0.4939096 ,\n",
       "        0.54500465, 0.42788305, 0.70089216, 0.78830047, 0.6858849 ,\n",
       "        0.64342432, 0.9833324 , 1.13019357, 1.29127965, 0.05021582,\n",
       "        0.10068192, 0.11719723, 0.11777911, 0.28016276, 0.32016258,\n",
       "        0.35534234, 0.33511972, 0.4220006 , 0.43098493, 0.40016789,\n",
       "        0.40497341, 0.80371189, 1.05746403, 0.6674922 , 0.96761413,\n",
       "        0.15864658, 0.23182378, 0.26277461, 0.29046369, 0.4282608 ,\n",
       "        0.5911499 , 0.51011014, 0.56842861, 0.57844563, 0.82168264,\n",
       "        0.85024405, 1.23556361, 1.45250893, 1.85046034, 1.68166146,\n",
       "        1.77278266, 0.34691648, 0.3262311 , 0.35628386, 0.46342688,\n",
       "        0.39160128, 1.08245578, 1.0002871 , 1.09263139, 0.49565067,\n",
       "        1.00813465, 1.48621883, 1.61755438, 1.02286625, 2.08119054,\n",
       "        2.39478307, 2.70873842, 0.14662142, 0.31402917, 0.43906794,\n",
       "        0.59189372, 0.40045962, 0.87298245, 1.07994723, 1.27735624,\n",
       "        0.61272359, 1.28490219, 1.45387826, 1.70081763, 0.92256703,\n",
       "        1.67950888, 1.98984594, 2.66373506, 0.05962305, 0.07013545,\n",
       "        0.07366734, 0.07169104, 0.14539084, 0.17214222, 0.19584603,\n",
       "        0.29147091, 0.24256592, 0.30705647, 0.35506859, 0.37509623,\n",
       "        0.53703127, 0.68036113, 0.74261899, 0.62856746, 0.07869906,\n",
       "        0.13480659, 0.17417555, 0.19338994, 0.21868868, 0.33800793,\n",
       "        0.38550234, 0.44946833, 0.38916655, 0.56462059, 0.61484323,\n",
       "        0.67919331, 0.72211485, 0.98770781, 1.06171584, 1.19717331,\n",
       "        0.0981256 , 0.20801597, 0.32494817, 0.38979607, 0.26123343,\n",
       "        0.5208951 , 0.6224988 , 0.73942137, 0.38618455, 0.79618702,\n",
       "        0.90964317, 1.0472476 , 0.76503973, 1.36569529, 1.40936728,\n",
       "        1.72372823, 0.11547437, 0.27846665, 0.34318762, 0.42817636,\n",
       "        0.28544374, 0.61294641, 0.69502048, 0.88422103, 0.42533765,\n",
       "        0.85508642, 0.96642342, 1.18220382, 0.72572603, 1.43078671,\n",
       "        1.60132589, 1.87956033, 0.0523129 , 0.06266632, 0.06295285,\n",
       "        0.06839366, 0.13138337, 0.15911279, 0.17175713, 0.19925394,\n",
       "        0.23697081, 0.29985433, 0.30558372, 0.31713939, 0.43518605,\n",
       "        0.51546464, 0.50550718, 0.58043351, 0.08286924, 0.12614374,\n",
       "        0.12745638, 0.16146188, 0.18613734, 0.3281538 , 0.32486773,\n",
       "        0.31795855, 0.30947075, 0.45425997, 0.47807117, 0.500597  ,\n",
       "        0.54731293, 0.6826396 , 0.73395162, 0.8820888 , 0.07701793,\n",
       "        0.17534442, 0.22105026, 0.23270068, 0.26253858, 0.35908294,\n",
       "        0.39090195, 0.44655066, 0.3283401 , 0.46252084, 0.54050088,\n",
       "        0.65328746, 0.60054755, 0.78796206, 0.85259023, 1.00110283,\n",
       "        0.07933021, 0.16939001, 0.21886225, 0.26055427, 0.22270103,\n",
       "        0.4295846 , 0.40670457, 0.45907993, 0.31451163, 0.5534431 ,\n",
       "        0.55902796, 0.64102826, 0.50958886, 0.77630944, 0.85073657,\n",
       "        1.02935157, 0.06240363, 0.08511057, 0.10074267, 0.11124773,\n",
       "        0.17249117, 0.31021056, 0.32834754, 0.43436971, 0.47129741,\n",
       "        0.57067466, 0.64142351, 0.75745034, 1.05221758, 1.02251072,\n",
       "        1.14547276, 1.29774475, 0.12538147, 0.21812191, 0.27352815,\n",
       "        0.32104931, 0.39394107, 0.63194785, 0.81381145, 1.05537314,\n",
       "        0.75439739, 1.17883134, 1.33762555, 1.61280723, 1.54238887,\n",
       "        2.30299044, 2.80700903, 2.70324879, 0.2469027 , 0.49909258,\n",
       "        0.51738687, 0.64038858, 0.44554143, 0.95113816, 1.29452162,\n",
       "        1.52987156, 0.80968113, 1.45426955, 2.13367753, 2.18647718,\n",
       "        1.79487238, 3.36906719, 3.51604595, 4.17803054, 0.23065939,\n",
       "        0.52006459, 0.64147797, 0.88163419, 0.35180655, 1.28370242,\n",
       "        1.34314494, 1.67139897, 0.75674825, 1.69879251, 2.15884719,\n",
       "        2.57422009, 1.13785872, 3.17465854, 3.61430955, 4.43530555,\n",
       "        0.05973892, 0.07960448, 0.09266477, 0.13523746, 0.27758141,\n",
       "        0.32700706, 0.29584236, 0.32444305, 0.32020888, 0.50680017,\n",
       "        0.60920205, 0.50407515, 0.60825772, 1.03497109, 1.18436813,\n",
       "        0.97184148, 0.13641319, 0.27900825, 0.25542445, 0.29434199,\n",
       "        0.38471785, 0.57370734, 0.67126236, 0.73661146, 0.73391743,\n",
       "        1.15024376, 1.53511963, 1.45949984, 1.29677019, 1.93524542,\n",
       "        1.85788355, 1.94674206, 0.166188  , 0.32057424, 0.41885366,\n",
       "        0.52551432, 0.39246025, 0.80539298, 0.99360695, 1.16059308,\n",
       "        0.5278266 , 1.15604992, 1.33395953, 1.59917965, 0.97377419,\n",
       "        1.84406519, 2.06715984, 2.43072724, 0.13465381, 0.37933359,\n",
       "        0.51327739, 0.64080787, 0.41039963, 0.88902817, 1.12487431,\n",
       "        1.35357332, 0.58559704, 1.28668799, 1.74524641, 1.80473552,\n",
       "        0.9164032 , 1.94534292, 2.61566715, 3.67810316, 0.085566  ,\n",
       "        0.09203095, 0.1212266 , 0.11063447, 0.18723669, 0.23541431,\n",
       "        0.27345352, 0.27143636, 0.29359856, 0.51479955, 0.59946604,\n",
       "        0.50039892, 0.74615283, 0.69117641, 0.85723619, 1.24864936,\n",
       "        0.168365  , 0.24440317, 0.21173878, 0.22838082, 0.29946733,\n",
       "        0.41692119, 0.4515542 , 0.52345338, 0.62385292, 0.65384431,\n",
       "        0.87387061, 1.08414259, 1.12620378, 1.47751789, 1.26498003,\n",
       "        2.03137364, 0.19319768, 0.41909642, 0.48381243, 0.47838483,\n",
       "        0.32581439, 0.81304402, 0.65036001, 0.71503119, 0.46594181,\n",
       "        0.79475417, 1.00077558, 1.19260063, 0.97620978, 1.61871524,\n",
       "        1.84645023, 2.30902572, 0.20121655, 0.47388029, 0.32161627,\n",
       "        0.37814426, 0.32278218, 0.58492322, 0.83493028, 0.72521133,\n",
       "        0.63243847, 1.23291335, 0.86627574, 1.45944653, 1.04466877,\n",
       "        1.24696021, 1.18545623, 1.6585948 ]),\n",
       " 'std_fit_time': array([1.00839041e-02, 4.29425076e-03, 3.87967772e-03, 3.71154893e-03,\n",
       "        1.91398970e-03, 2.06754159e-03, 3.60992663e-03, 1.24646235e-02,\n",
       "        2.90052323e-02, 1.08728328e-01, 1.51786883e-02, 4.34449809e-02,\n",
       "        3.74931554e-02, 5.05639692e-02, 6.22073375e-02, 2.69300244e-02,\n",
       "        8.98506013e-03, 3.45969226e-03, 1.05662951e-02, 8.66537950e-03,\n",
       "        1.81675370e-02, 4.72055576e-02, 2.49229288e-02, 9.81433880e-02,\n",
       "        1.74896748e-02, 8.79558847e-02, 6.53808471e-02, 1.96114664e-02,\n",
       "        1.03564398e-01, 2.33702031e-01, 2.12389206e-01, 4.78261017e-02,\n",
       "        1.95427999e-03, 1.21537323e-02, 1.27079856e-02, 8.10907392e-03,\n",
       "        4.70841535e-03, 1.09369022e-02, 8.84314223e-03, 1.08422329e-02,\n",
       "        5.61355063e-02, 2.28343547e-02, 1.87689339e-02, 7.63336927e-03,\n",
       "        7.79213057e-03, 9.44397504e-02, 1.59975391e-01, 2.32697426e-02,\n",
       "        1.65966879e-03, 4.07371158e-03, 3.98487730e-03, 3.32145204e-03,\n",
       "        5.99157241e-02, 5.36854618e-02, 1.13296852e-01, 7.82760502e-03,\n",
       "        5.47286191e-03, 2.48368210e-02, 2.05667302e-01, 2.46727560e-02,\n",
       "        1.90622303e-02, 8.47078302e-02, 3.73262167e-02, 8.18171812e-02,\n",
       "        4.16754774e-03, 9.26031607e-03, 3.00065887e-03, 2.55154944e-03,\n",
       "        2.46370847e-03, 1.42336205e-02, 3.42526842e-03, 2.06501349e-03,\n",
       "        1.22804246e-02, 2.02954122e-03, 1.97717127e-03, 4.47856274e-03,\n",
       "        5.07554520e-03, 3.60329646e-03, 5.99549131e-02, 1.13661905e-02,\n",
       "        1.44600185e-02, 2.44494186e-03, 9.87012167e-03, 1.31985098e-02,\n",
       "        2.40444682e-02, 1.46304022e-02, 1.63990141e-02, 9.25762582e-03,\n",
       "        3.56653801e-03, 3.76125267e-03, 4.48325914e-02, 2.59620963e-02,\n",
       "        5.77117103e-03, 8.26209260e-03, 1.32295235e-01, 8.03111118e-03,\n",
       "        2.50860481e-03, 3.37261901e-03, 7.70308052e-03, 3.39092715e-03,\n",
       "        3.39873062e-03, 3.53931877e-03, 1.52989947e-02, 4.67925191e-03,\n",
       "        3.14846888e-03, 2.25852650e-02, 1.10881793e-01, 1.53479516e-02,\n",
       "        1.12029356e-02, 1.22091932e-02, 6.09040997e-02, 1.18191263e-01,\n",
       "        1.83708269e-03, 3.54382547e-03, 3.58015678e-03, 3.57307509e-03,\n",
       "        2.13527436e-03, 1.27837166e-01, 5.23373209e-02, 1.51863333e-02,\n",
       "        6.51570709e-03, 7.47851220e-03, 1.21689633e-01, 1.19371501e-02,\n",
       "        2.31952274e-02, 1.55809479e-02, 3.02711956e-02, 8.56752648e-02,\n",
       "        2.79028930e-03, 6.02166638e-04, 1.52093569e-03, 1.59753845e-03,\n",
       "        2.44635736e-03, 2.97104355e-03, 1.43266610e-03, 1.97147819e-03,\n",
       "        2.10595554e-03, 3.21705604e-03, 2.81089441e-03, 3.28570530e-03,\n",
       "        2.09326515e-03, 1.99915663e-02, 2.95061425e-02, 8.07156359e-02,\n",
       "        1.96026114e-03, 2.71899703e-03, 2.17837193e-03, 1.58664389e-03,\n",
       "        2.71017058e-03, 2.71743360e-03, 4.95794035e-03, 9.23072559e-03,\n",
       "        1.68695084e-03, 1.28976648e-02, 8.62221547e-03, 3.10895172e-03,\n",
       "        4.12569740e-03, 5.17323821e-03, 1.64516158e-03, 6.97173151e-02,\n",
       "        9.25025651e-03, 5.68275429e-03, 6.08806354e-03, 4.30363531e-03,\n",
       "        1.48107384e-03, 8.15822823e-03, 4.89820107e-02, 2.18454626e-02,\n",
       "        1.99377749e-02, 4.46961600e-03, 1.30520963e-02, 4.79769938e-02,\n",
       "        6.44893024e-02, 3.69342080e-02, 3.39421652e-02, 1.01835664e-02,\n",
       "        6.31132926e-03, 9.35307912e-03, 1.21479382e-02, 3.68498532e-03,\n",
       "        2.07841681e-02, 4.00677062e-02, 6.75041088e-02, 7.50896910e-03,\n",
       "        1.16365763e-02, 1.15970312e-01, 1.54066785e-02, 3.02099228e-02,\n",
       "        7.67912358e-03, 1.71637441e-02, 5.57746356e-02, 1.46177494e-02,\n",
       "        6.45657788e-03, 6.42921653e-03, 3.73648908e-03, 4.99633852e-03,\n",
       "        1.70260875e-02, 1.43880607e-02, 1.01085638e-02, 1.18284187e-02,\n",
       "        9.55139548e-03, 2.75351642e-03, 4.24276770e-03, 3.25712458e-03,\n",
       "        8.41532321e-02, 9.16451445e-03, 1.80328531e-02, 8.49396985e-03,\n",
       "        2.55519085e-03, 9.73171925e-04, 7.23322400e-04, 1.10175404e-03,\n",
       "        5.52407295e-03, 1.29780611e-02, 4.14451532e-03, 4.82459888e-02,\n",
       "        1.84508402e-02, 2.93266507e-02, 9.62115669e-03, 3.55799945e-02,\n",
       "        6.97167027e-03, 8.94964342e-02, 4.67384840e-02, 2.54527094e-01,\n",
       "        2.07633039e-02, 5.87062603e-02, 5.46320280e-02, 6.72608364e-02,\n",
       "        2.19339657e-02, 2.44434545e-02, 3.00609431e-02, 2.41312179e-02,\n",
       "        1.30651332e-01, 1.08380019e-01, 1.12777071e-01, 5.18682641e-02,\n",
       "        9.12139410e-02, 3.59212418e-01, 3.84985604e-01, 4.84290117e-01,\n",
       "        4.09799495e-03, 1.27448088e-02, 1.03793910e-02, 1.14548122e-02,\n",
       "        4.75974425e-03, 1.52294945e-01, 4.20569212e-02, 1.62127355e-02,\n",
       "        2.36949513e-02, 6.87047924e-02, 2.08757561e-01, 5.95270941e-02,\n",
       "        9.26987593e-02, 1.11418778e-01, 2.65908085e-01, 2.07110950e-01,\n",
       "        2.30962141e-02, 2.60392117e-03, 2.96479487e-03, 1.68436238e-02,\n",
       "        3.93738768e-03, 7.53490311e-03, 3.15304996e-02, 1.22866994e-02,\n",
       "        4.75436445e-02, 5.31387970e-03, 6.51892054e-02, 3.03145167e-02,\n",
       "        1.94349183e-02, 5.43339535e-02, 7.77463352e-02, 1.48045157e-01,\n",
       "        7.89967988e-03, 2.37765303e-02, 1.19427740e-02, 1.21348172e-02,\n",
       "        1.33366494e-02, 3.25825129e-02, 2.16099195e-02, 8.05790779e-03,\n",
       "        3.70154243e-02, 3.64434696e-02, 9.77368846e-02, 1.24599094e-01,\n",
       "        1.43668877e-01, 9.48584425e-02, 1.41748090e-01, 7.49372257e-02,\n",
       "        3.52644170e-02, 2.20706217e-02, 9.26202509e-03, 9.66334483e-03,\n",
       "        1.49481750e-02, 4.43851145e-02, 8.32729876e-03, 1.52113596e-02,\n",
       "        2.23983106e-02, 1.84023483e-02, 6.21156979e-02, 2.62126444e-02,\n",
       "        7.20999747e-02, 3.45668457e-01, 2.69382262e-01, 1.14702600e-01,\n",
       "        1.29551501e-02, 2.75607721e-02, 3.48663776e-02, 9.91520268e-03,\n",
       "        5.59147224e-02, 5.13590906e-02, 4.03241081e-02, 3.24926068e-02,\n",
       "        2.22094771e-02, 1.67014191e-01, 9.88602947e-02, 2.44810794e-01,\n",
       "        1.42525066e-01, 2.65962057e-01, 3.24038463e-01, 9.77732003e-02,\n",
       "        7.26025341e-03, 7.43610867e-03, 9.17853908e-03, 5.43591613e-03,\n",
       "        1.07727484e-02, 1.82481955e-02, 3.74748487e-02, 1.24385271e-02,\n",
       "        4.66464189e-02, 8.33821122e-03, 6.64621208e-03, 4.21265305e-02,\n",
       "        1.45032075e-01, 8.50149376e-02, 5.82114201e-03, 1.13616595e-01,\n",
       "        1.66214762e-03, 9.60331884e-03, 1.02774828e-02, 4.54261049e-02,\n",
       "        2.13748084e-02, 1.01966459e-02, 1.11023028e-02, 2.08629264e-02,\n",
       "        1.87725639e-02, 3.18754486e-02, 4.95181659e-02, 2.18707727e-02,\n",
       "        9.14410374e-02, 1.51722645e-01, 1.62102706e-01, 2.81346265e-01,\n",
       "        2.73380949e-02, 1.23968743e-02, 1.49466643e-02, 2.14099333e-02,\n",
       "        6.62112932e-02, 1.73229456e-02, 2.19541743e-02, 5.89505979e-02,\n",
       "        8.69329117e-02, 8.81425171e-02, 1.03637595e-02, 1.67936093e-01,\n",
       "        1.39961413e-01, 1.54824933e-01, 2.20773524e-01, 1.72238511e-01,\n",
       "        2.35799555e-02, 3.23863644e-02, 6.47397615e-03, 2.81076264e-02,\n",
       "        4.22854215e-02, 1.74567933e-02, 3.88387142e-02, 4.91125701e-02,\n",
       "        1.07280763e-01, 1.51528694e-01, 1.68383688e-01, 3.20006641e-02,\n",
       "        4.39960819e-02, 1.17011038e-01, 2.11542001e-01, 2.40625894e-01,\n",
       "        1.96007208e-03, 2.50786072e-02, 6.92867814e-03, 9.33353868e-03,\n",
       "        1.54298868e-02, 1.02692859e-02, 2.23076116e-02, 7.03440407e-03,\n",
       "        1.64443955e-02, 4.55164062e-02, 7.19837781e-03, 7.02950518e-03,\n",
       "        1.11744809e-01, 4.08288868e-02, 5.48614066e-02, 1.70934367e-01,\n",
       "        5.16025380e-03, 9.70857904e-03, 1.37707722e-02, 6.72813578e-03,\n",
       "        1.02219904e-02, 7.09772051e-02, 5.47477111e-03, 1.35904267e-02,\n",
       "        1.03520228e-01, 9.47495065e-02, 8.47607850e-02, 2.24196204e-02,\n",
       "        5.08703152e-02, 3.03502840e-01, 3.60060629e-01, 2.53940694e-01,\n",
       "        3.72207343e-02, 7.97079942e-02, 1.07029393e-02, 2.96211589e-02,\n",
       "        1.66392036e-02, 1.59254780e-01, 2.28211807e-01, 1.34965642e-01,\n",
       "        2.93237011e-02, 1.97160133e-02, 2.27252475e-01, 6.06413886e-02,\n",
       "        3.17394951e-02, 1.95025350e-01, 1.03177324e-01, 9.67430984e-02,\n",
       "        5.63885840e-03, 1.61252343e-02, 1.41643635e-02, 2.67439715e-02,\n",
       "        2.50151336e-02, 7.48388791e-02, 4.54484629e-02, 6.10970894e-02,\n",
       "        8.33188207e-02, 1.91084853e-01, 9.55886700e-02, 1.04972867e-01,\n",
       "        7.44359864e-02, 1.10308038e-01, 1.11257193e-01, 1.55420136e-01,\n",
       "        3.27230705e-03, 2.87308664e-03, 5.24850860e-03, 2.73052927e-03,\n",
       "        7.52952760e-03, 5.15729552e-03, 1.15640039e-02, 6.08681776e-02,\n",
       "        1.37547766e-02, 1.72686890e-02, 9.82223100e-03, 3.17022970e-02,\n",
       "        3.46870403e-02, 8.20169198e-02, 5.26054489e-02, 2.43576412e-02,\n",
       "        2.55260451e-03, 1.22716657e-02, 1.65053321e-02, 1.56417168e-02,\n",
       "        4.74835467e-03, 6.86445347e-03, 7.93299444e-03, 1.41970851e-02,\n",
       "        2.40298747e-02, 6.65968940e-02, 4.18829006e-02, 2.38667701e-02,\n",
       "        2.95086175e-02, 6.06662744e-02, 3.83729024e-02, 8.33399289e-02,\n",
       "        4.03046492e-03, 5.76870567e-03, 5.82675116e-02, 2.18137531e-02,\n",
       "        9.44681178e-03, 8.87707592e-03, 7.57714443e-03, 1.89991584e-02,\n",
       "        2.95162766e-03, 5.14932142e-02, 2.79221370e-02, 1.72139229e-02,\n",
       "        7.47672307e-02, 7.03634447e-02, 4.93542148e-02, 1.46509885e-01,\n",
       "        1.75260716e-02, 1.23401065e-02, 2.51460084e-02, 2.05297104e-02,\n",
       "        1.07745785e-02, 5.40427638e-02, 2.20546435e-02, 5.28305597e-02,\n",
       "        3.13287477e-02, 8.47231173e-02, 2.31593167e-02, 5.61496163e-02,\n",
       "        3.09998113e-02, 8.42069790e-02, 1.83286417e-01, 9.81207357e-02,\n",
       "        3.06366393e-03, 3.03956027e-03, 3.82934746e-03, 1.32737117e-03,\n",
       "        3.76001072e-03, 4.08260975e-03, 3.70461939e-03, 8.31926314e-03,\n",
       "        2.39819530e-02, 1.00885958e-02, 5.42137371e-02, 2.44515156e-02,\n",
       "        5.16685256e-02, 5.44058602e-02, 3.30257540e-02, 5.05765641e-02,\n",
       "        9.15230819e-03, 1.23914963e-02, 1.10778085e-02, 1.82375233e-02,\n",
       "        6.14160886e-03, 4.67468961e-02, 1.90836874e-02, 1.13806841e-02,\n",
       "        2.01365195e-02, 3.07257492e-02, 2.51274358e-02, 3.34803452e-02,\n",
       "        5.18953666e-02, 9.86492838e-02, 3.54304601e-02, 4.83435915e-02,\n",
       "        3.74172659e-03, 5.20323264e-03, 2.42716112e-02, 9.47726682e-03,\n",
       "        3.62339936e-02, 2.62733174e-02, 2.87034930e-02, 2.85330749e-02,\n",
       "        3.53753707e-02, 1.97833577e-02, 3.68976522e-02, 5.12809233e-02,\n",
       "        2.94062972e-02, 5.13289282e-02, 4.78221640e-02, 9.65571921e-02,\n",
       "        3.74851486e-03, 5.73128122e-03, 1.43769206e-02, 1.66475751e-02,\n",
       "        1.63931802e-02, 8.38603101e-02, 1.68259086e-02, 3.79175445e-02,\n",
       "        1.87999203e-02, 5.00112277e-02, 9.76712323e-03, 5.72079281e-02,\n",
       "        1.14851967e-02, 6.44209343e-02, 9.22264155e-02, 5.78490521e-02,\n",
       "        1.47548740e-03, 2.10715796e-03, 9.38931612e-03, 7.30534941e-03,\n",
       "        1.36673536e-02, 7.40840968e-02, 3.00791346e-02, 9.73757239e-02,\n",
       "        4.22560041e-02, 9.72578620e-02, 9.98855222e-02, 9.97510065e-02,\n",
       "        1.45678902e-01, 2.31740316e-01, 1.45010286e-01, 1.36110241e-01,\n",
       "        2.81904009e-03, 3.31418441e-03, 7.99129401e-03, 6.22741203e-03,\n",
       "        8.12158899e-03, 1.47683231e-02, 5.84388432e-02, 6.16341230e-02,\n",
       "        1.48264064e-02, 1.83781701e-02, 1.85618037e-01, 2.51822245e-01,\n",
       "        2.19958612e-01, 2.37757414e-01, 4.57172893e-02, 4.30671130e-01,\n",
       "        4.99922109e-03, 2.91264993e-02, 1.61258720e-02, 8.88663323e-03,\n",
       "        1.16699378e-02, 1.87509796e-02, 1.11814577e-01, 1.12585643e-01,\n",
       "        1.44999740e-01, 1.24702357e-01, 3.40084610e-01, 2.47000059e-02,\n",
       "        2.93770665e-01, 4.33071847e-01, 4.99206408e-01, 4.12981623e-01,\n",
       "        1.20067483e-02, 2.62205035e-02, 8.49624028e-02, 8.18545076e-02,\n",
       "        1.02356279e-02, 1.96887960e-01, 2.61536766e-02, 2.39463816e-02,\n",
       "        7.50158180e-02, 3.76403846e-01, 3.15201363e-01, 2.74498568e-01,\n",
       "        1.18453412e-01, 4.28283313e-01, 6.31143905e-01, 5.29663968e-01,\n",
       "        1.05312712e-02, 4.10758181e-03, 4.74942418e-03, 1.91390066e-02,\n",
       "        5.37922111e-03, 3.73710394e-02, 3.91069212e-03, 2.54653025e-03,\n",
       "        6.04811493e-03, 6.90622953e-02, 7.72218758e-02, 5.25736185e-03,\n",
       "        1.07053044e-02, 4.11934920e-01, 4.38592284e-01, 1.92052345e-02,\n",
       "        1.28689990e-02, 1.83414957e-02, 9.69830460e-03, 6.21553160e-03,\n",
       "        3.14158144e-02, 6.05547227e-02, 3.17877572e-02, 1.51167653e-02,\n",
       "        1.17574746e-01, 1.36910478e-01, 8.34057256e-02, 1.30947454e-01,\n",
       "        2.72976963e-01, 2.72051667e-01, 1.77793994e-01, 1.48538621e-01,\n",
       "        6.94702485e-03, 2.27572693e-02, 1.10684738e-02, 1.40107647e-02,\n",
       "        1.75114434e-02, 5.23697770e-02, 4.74865073e-02, 5.18616393e-02,\n",
       "        2.08804179e-02, 8.09109940e-02, 5.47921036e-02, 1.50705423e-01,\n",
       "        4.63361737e-02, 3.37174733e-02, 1.21694495e-01, 8.75901126e-02,\n",
       "        5.51800681e-03, 2.29559201e-02, 2.07399446e-02, 1.62355427e-02,\n",
       "        2.93922713e-02, 4.74859278e-02, 4.57441939e-02, 7.04446081e-02,\n",
       "        1.88492049e-02, 7.92648163e-02, 9.39420204e-02, 2.52693402e-01,\n",
       "        8.89712143e-02, 3.82542533e-01, 3.66604875e-01, 5.16823793e-01,\n",
       "        2.45316231e-02, 2.28499944e-03, 1.69676382e-02, 6.04183791e-03,\n",
       "        5.22319305e-03, 6.54589728e-03, 2.14380128e-02, 6.30043844e-03,\n",
       "        5.30398123e-03, 1.98024562e-02, 4.53718252e-02, 5.41677992e-02,\n",
       "        1.60444720e-01, 4.64223959e-02, 2.33199958e-01, 4.82800977e-02,\n",
       "        8.30877319e-03, 1.77565857e-02, 1.99067707e-02, 4.00628846e-03,\n",
       "        1.13282814e-02, 1.58734834e-02, 7.32473168e-03, 2.44490872e-02,\n",
       "        1.22807479e-01, 3.56929421e-02, 1.66372006e-01, 4.89155940e-02,\n",
       "        1.04780479e-01, 1.43595103e-01, 2.37390993e-01, 1.66010455e-01,\n",
       "        2.01201550e-02, 1.33287801e-02, 2.37291812e-02, 5.48018480e-02,\n",
       "        1.00456468e-02, 1.66437485e-01, 1.38353607e-01, 3.04998356e-02,\n",
       "        1.14381989e-02, 6.62746283e-02, 2.74354798e-01, 1.33660374e-01,\n",
       "        6.87518070e-02, 1.97478371e-01, 2.91638254e-01, 1.29715843e-01,\n",
       "        1.72024910e-02, 2.40674635e-02, 5.95902673e-03, 8.19439969e-03,\n",
       "        2.65301240e-02, 3.19457134e-02, 7.04547441e-02, 4.98574528e-02,\n",
       "        1.31893191e-01, 4.78979140e-02, 6.83102909e-02, 2.77945683e-01,\n",
       "        1.86637188e-01, 7.92333222e-02, 1.17349782e-01, 9.84255805e-02]),\n",
       " 'mean_score_time': array([0.00097332, 0.00069542, 0.00075355, 0.0006815 , 0.00112281,\n",
       "        0.00137672, 0.00133338, 0.00131006, 0.00205503, 0.00224094,\n",
       "        0.00195646, 0.00255775, 0.0033    , 0.00320158, 0.0035944 ,\n",
       "        0.003409  , 0.00083494, 0.00103393, 0.00087671, 0.00100994,\n",
       "        0.00169215, 0.00193667, 0.0024622 , 0.00229764, 0.00244465,\n",
       "        0.00296621, 0.00369682, 0.00337362, 0.00394101, 0.0056107 ,\n",
       "        0.0055891 , 0.00493503, 0.00078964, 0.0013092 , 0.00144677,\n",
       "        0.00169601, 0.00164237, 0.00300379, 0.00281634, 0.00339208,\n",
       "        0.00237374, 0.00418472, 0.00493579, 0.00498919, 0.00351496,\n",
       "        0.00727792, 0.00869145, 0.00926275, 0.00076351, 0.00119157,\n",
       "        0.00167079, 0.00160036, 0.00154109, 0.00323286, 0.00356827,\n",
       "        0.00384378, 0.00235481, 0.00430179, 0.00532269, 0.00573812,\n",
       "        0.00343833, 0.0081171 , 0.00939512, 0.01000776, 0.00075941,\n",
       "        0.00073853, 0.00078211, 0.00069919, 0.0011301 , 0.00129485,\n",
       "        0.0013052 , 0.00141339, 0.00180044, 0.00196323, 0.00174265,\n",
       "        0.00174203, 0.00284104, 0.00307074, 0.00321479, 0.00363874,\n",
       "        0.0007823 , 0.00119104, 0.00089622, 0.00102615, 0.00156064,\n",
       "        0.00224342, 0.00235476, 0.00220499, 0.0023798 , 0.00270462,\n",
       "        0.00303893, 0.00321155, 0.00337124, 0.00499706, 0.00467978,\n",
       "        0.00521374, 0.00068865, 0.00136242, 0.00149603, 0.00177321,\n",
       "        0.00147152, 0.00271616, 0.00312023, 0.00354638, 0.00206113,\n",
       "        0.00400219, 0.00440645, 0.00464778, 0.00338511, 0.00619216,\n",
       "        0.00745411, 0.00799403, 0.00068965, 0.00128021, 0.00148587,\n",
       "        0.00177307, 0.0014082 , 0.00341563, 0.00357237, 0.00378566,\n",
       "        0.00255451, 0.00398545, 0.00482421, 0.00570412, 0.00365224,\n",
       "        0.00666432, 0.00812025, 0.00843701, 0.00065517, 0.00069456,\n",
       "        0.00069165, 0.00072312, 0.00114198, 0.001371  , 0.00128956,\n",
       "        0.0012198 , 0.00197039, 0.00187798, 0.00185161, 0.00179052,\n",
       "        0.0029007 , 0.00286665, 0.00294838, 0.00323796, 0.00073481,\n",
       "        0.00097451, 0.00093217, 0.00121512, 0.00154743, 0.00187783,\n",
       "        0.00204406, 0.00225635, 0.00173354, 0.0025517 , 0.0030344 ,\n",
       "        0.00267119, 0.00313997, 0.00379105, 0.00406499, 0.00436773,\n",
       "        0.00088453, 0.00103831, 0.00130639, 0.00133672, 0.00138588,\n",
       "        0.00218329, 0.00283823, 0.00280013, 0.00218177, 0.00286441,\n",
       "        0.00356741, 0.00506306, 0.00354819, 0.0045702 , 0.0054522 ,\n",
       "        0.00528488, 0.00089049, 0.00127182, 0.00156379, 0.00152164,\n",
       "        0.00162468, 0.00257845, 0.00286894, 0.00297203, 0.00237808,\n",
       "        0.00325809, 0.00379758, 0.00387053, 0.00330191, 0.00485487,\n",
       "        0.0053854 , 0.00539594, 0.00079255, 0.00101757, 0.00077968,\n",
       "        0.00121861, 0.00144014, 0.00158582, 0.00145016, 0.00135231,\n",
       "        0.00210276, 0.00203538, 0.00211673, 0.00180039, 0.00325556,\n",
       "        0.00299201, 0.00299425, 0.00307584, 0.00101418, 0.00108261,\n",
       "        0.00110426, 0.00115042, 0.00190306, 0.00209432, 0.00244322,\n",
       "        0.00279913, 0.00313435, 0.00332322, 0.00316005, 0.00315237,\n",
       "        0.00465498, 0.00541124, 0.00548801, 0.00559874, 0.00106015,\n",
       "        0.00178018, 0.00186367, 0.00189242, 0.00274129, 0.00323439,\n",
       "        0.00360384, 0.00355916, 0.00501895, 0.00440898, 0.00521259,\n",
       "        0.00590253, 0.00579181, 0.01114712, 0.01056204, 0.00986853,\n",
       "        0.0013319 , 0.00153146, 0.00208602, 0.00185804, 0.0023664 ,\n",
       "        0.00390019, 0.00397458, 0.0038136 , 0.00341702, 0.00593319,\n",
       "        0.00637865, 0.00617189, 0.00577178, 0.00930848, 0.01044021,\n",
       "        0.01213622, 0.00072894, 0.00077143, 0.000804  , 0.00078301,\n",
       "        0.0012116 , 0.00148239, 0.00167799, 0.00131764, 0.00191259,\n",
       "        0.00204287, 0.00196028, 0.00211382, 0.00327234, 0.00351262,\n",
       "        0.00455427, 0.00401826, 0.00112076, 0.00115619, 0.00124159,\n",
       "        0.00106406, 0.00203362, 0.00242615, 0.0021698 , 0.00257068,\n",
       "        0.00341754, 0.00350852, 0.00349793, 0.00328541, 0.0092248 ,\n",
       "        0.00706954, 0.00729523, 0.00566301, 0.00116091, 0.00128059,\n",
       "        0.00149517, 0.00181856, 0.00217342, 0.00339203, 0.0037755 ,\n",
       "        0.00416641, 0.00369558, 0.00532966, 0.00527053, 0.00543938,\n",
       "        0.0054894 , 0.00973706, 0.00948057, 0.00917444, 0.00156374,\n",
       "        0.00335846, 0.00246086, 0.00270972, 0.00296755, 0.00391793,\n",
       "        0.00453782, 0.00480909, 0.00354395, 0.00547042, 0.00632572,\n",
       "        0.00630941, 0.00560355, 0.00848427, 0.00909657, 0.00946121,\n",
       "        0.00091105, 0.00125308, 0.00087624, 0.00085888, 0.00190663,\n",
       "        0.00219679, 0.00201993, 0.00187292, 0.00225515, 0.00197678,\n",
       "        0.00217619, 0.0022892 , 0.00435019, 0.0043159 , 0.00471344,\n",
       "        0.00359435, 0.00085306, 0.00139236, 0.00124302, 0.00106945,\n",
       "        0.00177326, 0.00226521, 0.00216002, 0.00218878, 0.00265965,\n",
       "        0.0035831 , 0.00325136, 0.00378976, 0.00448093, 0.00599408,\n",
       "        0.00622258, 0.00542178, 0.00116553, 0.00195212, 0.00207834,\n",
       "        0.00227327, 0.00172372, 0.00289512, 0.00329947, 0.00316606,\n",
       "        0.00295782, 0.00405917, 0.00395412, 0.00463877, 0.00558047,\n",
       "        0.00591145, 0.00600901, 0.00695987, 0.00146627, 0.00247402,\n",
       "        0.00258536, 0.00274401, 0.0019793 , 0.00356636, 0.00321665,\n",
       "        0.00306311, 0.00366907, 0.00422225, 0.00449104, 0.00451984,\n",
       "        0.00414362, 0.00567584, 0.00643563, 0.00721831, 0.00087218,\n",
       "        0.00085344, 0.00091162, 0.00089827, 0.00208659, 0.0018095 ,\n",
       "        0.00177531, 0.00200505, 0.00284476, 0.00213261, 0.0021174 ,\n",
       "        0.00189915, 0.00499105, 0.00468421, 0.00312719, 0.00415988,\n",
       "        0.00144296, 0.00152669, 0.00123844, 0.00164437, 0.00324659,\n",
       "        0.00277724, 0.00225959, 0.00227642, 0.00346413, 0.00393023,\n",
       "        0.00357828, 0.00463562, 0.0066431 , 0.006285  , 0.00589952,\n",
       "        0.00620193, 0.0021554 , 0.00159349, 0.00167975, 0.00173135,\n",
       "        0.00251412, 0.00508819, 0.00417938, 0.0034059 , 0.00366631,\n",
       "        0.00524855, 0.00564198, 0.00576501, 0.00651159, 0.00899568,\n",
       "        0.00967107, 0.00997806, 0.00152383, 0.00195608, 0.00198641,\n",
       "        0.00213027, 0.00313768, 0.00417972, 0.00435553, 0.00411758,\n",
       "        0.00362601, 0.00523005, 0.00602341, 0.00559053, 0.00603323,\n",
       "        0.0088726 , 0.0097147 , 0.01131859, 0.00074553, 0.00067372,\n",
       "        0.00076466, 0.00072379, 0.00128179, 0.00137033, 0.00125237,\n",
       "        0.00157847, 0.00195613, 0.00181737, 0.00185256, 0.00210848,\n",
       "        0.00341463, 0.00340476, 0.00296245, 0.00316029, 0.00093451,\n",
       "        0.00114346, 0.00122094, 0.00118337, 0.00201511, 0.0021347 ,\n",
       "        0.00218649, 0.00224652, 0.0028758 , 0.00345545, 0.00318103,\n",
       "        0.00303369, 0.00541196, 0.0053237 , 0.00501409, 0.00514646,\n",
       "        0.00119958, 0.00150137, 0.00168076, 0.00219736, 0.00227394,\n",
       "        0.00343547, 0.00336142, 0.00349526, 0.00309596, 0.00438662,\n",
       "        0.00492988, 0.00543809, 0.00516548, 0.00815372, 0.00865369,\n",
       "        0.00855474, 0.00100355, 0.00205564, 0.00187082, 0.00198669,\n",
       "        0.00271358, 0.00362668, 0.00371337, 0.00359502, 0.00320396,\n",
       "        0.0048347 , 0.00539885, 0.0056932 , 0.00557747, 0.00813065,\n",
       "        0.00802565, 0.00929995, 0.00087113, 0.00083408, 0.00073385,\n",
       "        0.00092421, 0.0015202 , 0.00129094, 0.00116282, 0.00156941,\n",
       "        0.00214605, 0.00272651, 0.00206871, 0.00193043, 0.00337238,\n",
       "        0.00362439, 0.003441  , 0.00351372, 0.00122609, 0.00124345,\n",
       "        0.00122175, 0.00130377, 0.00174985, 0.0025856 , 0.00215979,\n",
       "        0.00229254, 0.00276604, 0.00307279, 0.00323663, 0.00302362,\n",
       "        0.00419874, 0.00474796, 0.00498018, 0.00518689, 0.00093145,\n",
       "        0.00155716, 0.00154462, 0.00152392, 0.002213  , 0.00270152,\n",
       "        0.00313935, 0.00338717, 0.00327377, 0.00340824, 0.00388474,\n",
       "        0.00397792, 0.00399246, 0.00534134, 0.00523591, 0.00652881,\n",
       "        0.00102262, 0.00149679, 0.00193954, 0.00164132, 0.00249176,\n",
       "        0.00308828, 0.00318432, 0.00303755, 0.00277181, 0.00392942,\n",
       "        0.00401182, 0.00385852, 0.00409598, 0.00527339, 0.00524278,\n",
       "        0.0058497 , 0.00098863, 0.00075178, 0.00096502, 0.00083942,\n",
       "        0.00144601, 0.00262122, 0.00141106, 0.00246062, 0.00273027,\n",
       "        0.002105  , 0.00225492, 0.00227537, 0.00429044, 0.00408435,\n",
       "        0.00404339, 0.00413327, 0.00102582, 0.00115309, 0.00106792,\n",
       "        0.00131688, 0.00237756, 0.0028996 , 0.00263209, 0.00234375,\n",
       "        0.00334201, 0.00409942, 0.00425606, 0.00395255, 0.00664191,\n",
       "        0.00677032, 0.00705981, 0.00567527, 0.00192275, 0.00228863,\n",
       "        0.00197821, 0.00164266, 0.0029151 , 0.00403953, 0.00435224,\n",
       "        0.00397038, 0.00532365, 0.0054296 , 0.00611959, 0.00668163,\n",
       "        0.00690956, 0.01105762, 0.01097083, 0.01048422, 0.00189619,\n",
       "        0.00213499, 0.00214834, 0.00234566, 0.00251408, 0.00506878,\n",
       "        0.00456557, 0.00422683, 0.00420136, 0.00578418, 0.00632215,\n",
       "        0.00689163, 0.00581379, 0.01064329, 0.01166005, 0.01166983,\n",
       "        0.00097489, 0.00087314, 0.00068331, 0.00083218, 0.00199342,\n",
       "        0.00160956, 0.00123219, 0.00148835, 0.00247045, 0.0026895 ,\n",
       "        0.00245905, 0.00207477, 0.00359726, 0.00462852, 0.00374498,\n",
       "        0.0034543 , 0.00113425, 0.00149622, 0.00105863, 0.00124817,\n",
       "        0.0025456 , 0.00247006, 0.00239353, 0.00275068, 0.00467086,\n",
       "        0.00474958, 0.00401111, 0.00404711, 0.00620379, 0.00661602,\n",
       "        0.00694203, 0.00576901, 0.00158677, 0.00169244, 0.0019659 ,\n",
       "        0.0016273 , 0.00267854, 0.00364251, 0.00344381, 0.00375786,\n",
       "        0.00333419, 0.00516691, 0.00512857, 0.00529199, 0.00582166,\n",
       "        0.00881906, 0.00867405, 0.00862136, 0.00136576, 0.00181904,\n",
       "        0.00209584, 0.00182581, 0.00247765, 0.00409021, 0.00424795,\n",
       "        0.00444007, 0.00379896, 0.00542226, 0.00592523, 0.00557852,\n",
       "        0.00516591, 0.00859618, 0.00885744, 0.01024761, 0.00091028,\n",
       "        0.00073829, 0.00091   , 0.00081038, 0.00145268, 0.00158806,\n",
       "        0.00161591, 0.00121675, 0.00253358, 0.00322814, 0.00310998,\n",
       "        0.00241308, 0.00388374, 0.0041398 , 0.00442438, 0.00469937,\n",
       "        0.00127163, 0.00155334, 0.00115061, 0.00102096, 0.00220041,\n",
       "        0.00283051, 0.00257239, 0.00228539, 0.0045208 , 0.00414262,\n",
       "        0.00397463, 0.00446301, 0.00553527, 0.00573587, 0.00577879,\n",
       "        0.00681081, 0.00143514, 0.00246148, 0.00232635, 0.00210447,\n",
       "        0.00244184, 0.00497589, 0.00316906, 0.00374875, 0.00300946,\n",
       "        0.00450664, 0.00423155, 0.00551057, 0.00535955, 0.00619855,\n",
       "        0.00614986, 0.00727072, 0.00156755, 0.00210085, 0.00192213,\n",
       "        0.00185099, 0.00229421, 0.00428195, 0.00443692, 0.00361376,\n",
       "        0.00488315, 0.00532141, 0.00430226, 0.00663595, 0.00745816,\n",
       "        0.00548816, 0.00550323, 0.0064877 ]),\n",
       " 'std_score_time': array([4.34786846e-04, 6.44299291e-05, 2.18525610e-04, 1.22984854e-04,\n",
       "        6.28761100e-05, 1.96773924e-04, 2.38094663e-04, 2.00262773e-04,\n",
       "        4.82685149e-04, 3.85516706e-04, 2.13693230e-04, 1.33769506e-03,\n",
       "        4.32332829e-04, 1.67893720e-04, 6.30208344e-04, 3.87831820e-04,\n",
       "        1.42474921e-04, 1.93497769e-04, 1.14405150e-05, 1.76142252e-04,\n",
       "        3.54679947e-04, 1.22801724e-04, 4.67093076e-04, 1.84201275e-04,\n",
       "        1.47362016e-04, 2.41080087e-04, 1.16930499e-03, 7.36641911e-04,\n",
       "        6.57975073e-04, 6.92834562e-04, 1.04949848e-03, 5.94557964e-04,\n",
       "        2.04975817e-04, 1.63037688e-04, 1.12677579e-04, 2.33893151e-04,\n",
       "        2.26463847e-04, 2.50740430e-04, 1.49491042e-04, 1.53601795e-04,\n",
       "        3.09058148e-04, 3.11483558e-04, 6.28015399e-04, 4.63707046e-04,\n",
       "        2.33494814e-04, 4.85436958e-04, 1.11682134e-03, 1.15127002e-03,\n",
       "        1.47538194e-04, 2.41801109e-05, 2.12367950e-04, 1.58110171e-05,\n",
       "        1.19441702e-04, 2.15911349e-04, 2.59506444e-04, 2.14268039e-04,\n",
       "        2.64874625e-04, 3.12784604e-04, 7.32899022e-04, 3.47797930e-04,\n",
       "        2.33518729e-04, 2.17210584e-04, 8.29046326e-04, 6.15941294e-04,\n",
       "        3.60459877e-05, 1.16993782e-04, 2.18135390e-04, 2.03812641e-04,\n",
       "        8.23935424e-05, 1.90576249e-04, 2.32362145e-04, 2.39659485e-04,\n",
       "        2.22431660e-04, 1.81133522e-04, 1.64864432e-04, 2.35977580e-04,\n",
       "        3.59008743e-04, 1.72646570e-04, 2.78101283e-04, 5.49535658e-04,\n",
       "        2.08998189e-04, 4.40651062e-04, 2.02445314e-05, 2.22241364e-04,\n",
       "        2.15129268e-04, 2.39987769e-04, 4.75673247e-04, 1.68211267e-04,\n",
       "        9.97565216e-05, 6.81264976e-05, 1.58925998e-04, 3.43052685e-04,\n",
       "        2.88807556e-04, 7.75251926e-04, 5.78944460e-04, 3.85844330e-04,\n",
       "        2.17892147e-05, 2.21902749e-04, 1.57448886e-04, 5.05545278e-04,\n",
       "        1.76234846e-04, 2.43840536e-04, 2.07368986e-04, 3.10332261e-04,\n",
       "        1.65520405e-04, 3.37574326e-04, 4.25048508e-04, 2.63967094e-04,\n",
       "        1.54586268e-04, 7.72894675e-04, 6.16685358e-04, 4.97801678e-04,\n",
       "        2.09491664e-05, 1.55204203e-04, 8.50639802e-05, 2.42194854e-04,\n",
       "        1.60422265e-04, 6.29323612e-04, 1.16805892e-04, 3.11721598e-04,\n",
       "        4.00693462e-04, 3.73283295e-04, 1.56534201e-04, 1.09856327e-04,\n",
       "        2.82206741e-04, 3.41601275e-04, 6.79028225e-04, 1.14161753e-03,\n",
       "        9.82428050e-05, 1.62475389e-04, 1.33972829e-04, 1.68338578e-04,\n",
       "        8.54006431e-05, 1.84420283e-04, 1.91141613e-04, 2.26210332e-04,\n",
       "        7.18567809e-05, 2.37246608e-04, 2.49039471e-04, 1.46215877e-04,\n",
       "        2.83572600e-04, 2.36942615e-04, 1.62679802e-04, 1.09343034e-04,\n",
       "        1.37231050e-04, 2.10903765e-04, 1.40351362e-04, 2.67032155e-04,\n",
       "        2.17536937e-04, 1.68208956e-04, 4.67247099e-04, 2.86439713e-04,\n",
       "        1.83295732e-04, 1.75153368e-04, 4.17424822e-04, 1.99930286e-04,\n",
       "        3.76120584e-04, 2.69811656e-04, 3.04163184e-04, 8.41642406e-04,\n",
       "        1.87871843e-04, 3.24512123e-05, 1.44625158e-04, 9.08477532e-05,\n",
       "        1.82238803e-04, 1.54653780e-04, 2.72723869e-04, 2.44877939e-04,\n",
       "        4.45361388e-04, 6.65889732e-05, 5.14199653e-04, 3.17239856e-03,\n",
       "        4.07807865e-04, 6.22806674e-04, 4.08233751e-04, 2.80057899e-04,\n",
       "        2.75443902e-04, 8.31565725e-05, 1.76013701e-04, 1.64353214e-04,\n",
       "        2.52797170e-04, 3.62395657e-04, 3.45313486e-04, 3.56516210e-04,\n",
       "        5.89369427e-04, 2.48860605e-04, 5.84857062e-04, 4.02774003e-04,\n",
       "        5.00326473e-04, 5.40964462e-04, 6.44198000e-04, 2.38822233e-04,\n",
       "        5.53643466e-05, 2.36554055e-04, 1.26774133e-05, 3.28238657e-04,\n",
       "        2.30819507e-04, 2.92367080e-04, 2.11429542e-04, 1.92893862e-04,\n",
       "        1.74140486e-04, 4.46160377e-04, 3.49840004e-04, 1.45730563e-04,\n",
       "        4.01033357e-04, 1.84569159e-04, 1.47796937e-04, 1.86221120e-04,\n",
       "        1.99428236e-04, 2.70024904e-04, 2.68684753e-04, 4.45117371e-04,\n",
       "        2.36821932e-04, 2.76824501e-04, 2.70134315e-04, 4.24224754e-04,\n",
       "        5.38881109e-04, 2.86173994e-04, 3.12783492e-04, 2.69047227e-04,\n",
       "        5.27039606e-04, 3.70295926e-04, 4.35357668e-04, 5.33233701e-04,\n",
       "        7.58737077e-05, 3.00902292e-04, 3.01994694e-04, 3.28865590e-04,\n",
       "        5.90452079e-04, 1.15011205e-04, 1.46272050e-04, 1.04880048e-04,\n",
       "        2.02156981e-03, 3.28569369e-04, 1.72583659e-04, 6.98688109e-04,\n",
       "        1.95483323e-04, 1.14197938e-03, 1.84250866e-03, 1.06003089e-03,\n",
       "        4.58257022e-04, 3.59579483e-05, 5.43704327e-04, 2.41148219e-04,\n",
       "        1.44926934e-04, 5.76127367e-04, 1.77878834e-04, 1.72042662e-04,\n",
       "        1.17207602e-04, 3.79319497e-04, 6.48523551e-04, 4.00130888e-04,\n",
       "        3.77022498e-04, 6.50190758e-04, 1.21275007e-03, 5.64613373e-04,\n",
       "        1.00445450e-04, 1.71236174e-04, 2.09483600e-04, 1.85323631e-04,\n",
       "        7.71158472e-05, 2.77702142e-04, 2.80532260e-04, 1.94893013e-04,\n",
       "        1.06620405e-04, 1.53070269e-04, 1.99184304e-04, 6.23547538e-04,\n",
       "        1.73108794e-04, 9.75721538e-04, 2.04254244e-03, 1.10001064e-03,\n",
       "        1.59460605e-04, 1.65185683e-04, 2.25343504e-04, 1.37945702e-04,\n",
       "        2.38721541e-04, 7.27013523e-04, 8.10343189e-05, 3.96955991e-04,\n",
       "        1.05647340e-03, 6.54222100e-04, 5.50605512e-04, 3.43439294e-04,\n",
       "        7.00933700e-03, 2.76112245e-03, 2.13524147e-03, 3.63830989e-04,\n",
       "        2.01727052e-04, 4.24367390e-05, 1.56025417e-04, 3.13194748e-04,\n",
       "        2.09864533e-04, 3.31061806e-04, 4.26095895e-04, 3.63387784e-04,\n",
       "        3.08606880e-04, 4.69389339e-04, 4.90255328e-04, 3.01398753e-04,\n",
       "        4.05396884e-04, 1.18612684e-03, 7.89881272e-04, 7.74545365e-04,\n",
       "        1.03558163e-04, 1.40972751e-03, 3.59802330e-04, 4.76070804e-04,\n",
       "        2.80586292e-04, 4.54873183e-04, 3.98653289e-04, 6.26249401e-04,\n",
       "        6.38320598e-04, 5.19959330e-04, 5.73609138e-04, 4.61639733e-04,\n",
       "        4.54681446e-04, 4.10562569e-04, 5.79045930e-04, 7.32170723e-04,\n",
       "        2.71208443e-05, 4.29111834e-04, 1.18111322e-05, 2.78762608e-05,\n",
       "        2.02961299e-05, 3.79163326e-04, 9.51125076e-05, 4.82314598e-05,\n",
       "        3.76376434e-04, 2.14127091e-04, 1.89608562e-04, 3.67663051e-04,\n",
       "        5.59841692e-04, 9.26013162e-04, 6.43024545e-04, 5.70980579e-04,\n",
       "        5.39441727e-05, 6.08912799e-04, 1.71606042e-04, 1.90015665e-04,\n",
       "        2.02365982e-04, 3.99232007e-04, 1.23108920e-04, 2.19514129e-04,\n",
       "        2.14775237e-04, 5.45517018e-04, 4.60368222e-04, 1.37846850e-03,\n",
       "        2.95644791e-04, 4.57212071e-04, 9.05685632e-04, 6.54251888e-04,\n",
       "        2.02787895e-04, 1.42675654e-04, 4.25947650e-04, 6.45290180e-05,\n",
       "        1.46189392e-04, 2.27005890e-04, 6.02464379e-04, 1.56260097e-04,\n",
       "        4.08234352e-04, 6.58077909e-04, 3.05852936e-04, 4.28086719e-04,\n",
       "        5.24241750e-04, 5.56832328e-04, 1.04187625e-03, 7.17626068e-04,\n",
       "        2.41382769e-04, 3.23727356e-04, 2.13471555e-04, 4.72191048e-04,\n",
       "        1.93144385e-04, 4.14733175e-04, 2.90717632e-04, 2.44907417e-04,\n",
       "        4.79673993e-04, 6.68939545e-04, 6.02460707e-04, 3.47737028e-04,\n",
       "        2.51193343e-04, 4.43602723e-04, 1.57171091e-03, 1.35910944e-03,\n",
       "        1.86528518e-04, 1.58197984e-04, 4.90383840e-05, 2.99219021e-05,\n",
       "        1.80458479e-04, 1.71653337e-04, 2.58588637e-04, 4.68926505e-04,\n",
       "        3.60813745e-04, 4.37584900e-04, 2.86983920e-04, 2.13533280e-04,\n",
       "        1.42060906e-04, 4.09101078e-04, 1.69167792e-04, 8.52518156e-04,\n",
       "        1.03067820e-04, 3.12354181e-05, 2.45329108e-04, 1.59924814e-04,\n",
       "        2.03759966e-04, 6.11697248e-04, 2.23319117e-04, 2.30632238e-04,\n",
       "        6.77102260e-04, 4.44671771e-04, 7.86064842e-04, 6.29726710e-04,\n",
       "        5.34105648e-04, 5.70836825e-04, 1.29538973e-03, 5.67791539e-04,\n",
       "        4.14823734e-04, 1.23039381e-04, 1.67198963e-04, 2.15335532e-04,\n",
       "        1.95434081e-04, 2.95652405e-04, 8.27471097e-04, 3.27002832e-04,\n",
       "        4.76740269e-04, 3.98557943e-04, 7.43208488e-04, 2.93074933e-04,\n",
       "        1.87279674e-04, 6.81417193e-04, 1.30778801e-03, 5.89944620e-04,\n",
       "        3.29293695e-04, 2.54757034e-04, 9.14181027e-05, 2.53676472e-04,\n",
       "        5.08316677e-04, 1.75196747e-04, 5.28457835e-04, 5.16014230e-04,\n",
       "        3.70031644e-04, 3.06861091e-04, 4.15652800e-04, 4.67238996e-04,\n",
       "        4.62418705e-04, 3.33851760e-04, 6.56369641e-04, 1.04071092e-03,\n",
       "        9.10902511e-05, 7.41584348e-05, 1.51323910e-04, 1.44843508e-04,\n",
       "        1.88856255e-04, 1.70617279e-04, 1.19663081e-04, 1.74800112e-04,\n",
       "        1.90686792e-04, 1.66270020e-04, 2.33770573e-04, 4.69959624e-04,\n",
       "        2.67142970e-04, 5.01792863e-04, 1.38301815e-04, 2.63018700e-04,\n",
       "        1.73458691e-04, 3.14528961e-04, 2.10958978e-04, 3.79258980e-04,\n",
       "        2.62301095e-04, 1.97099623e-04, 2.47220427e-04, 2.00513206e-04,\n",
       "        3.90004322e-04, 1.35181223e-04, 9.48854905e-05, 3.12581593e-04,\n",
       "        4.93539534e-04, 2.63056345e-04, 5.51231708e-04, 3.23313119e-04,\n",
       "        2.33473692e-04, 3.60295784e-04, 1.77961901e-04, 5.43157473e-04,\n",
       "        2.37269666e-04, 3.46671161e-04, 2.81381336e-04, 3.25853377e-04,\n",
       "        2.02711636e-04, 4.18606279e-04, 3.87400673e-04, 1.86943094e-04,\n",
       "        3.09202650e-04, 6.92858082e-04, 5.31338073e-04, 3.54292492e-04,\n",
       "        3.96198023e-05, 4.35338172e-04, 1.76454066e-04, 3.09704813e-04,\n",
       "        5.96514873e-04, 1.22247192e-04, 2.19133472e-04, 3.65520506e-04,\n",
       "        2.89751723e-04, 8.40693666e-04, 1.50040629e-04, 1.76826431e-04,\n",
       "        1.23555888e-03, 9.02436584e-04, 3.46125415e-04, 7.86602960e-04,\n",
       "        1.52650269e-04, 1.75503127e-04, 1.37055392e-04, 2.22387445e-04,\n",
       "        1.41264841e-04, 1.37664267e-04, 1.83785932e-05, 3.36912924e-04,\n",
       "        2.65561415e-04, 4.45405072e-04, 2.30405050e-04, 1.66781620e-04,\n",
       "        1.58364749e-04, 3.10521300e-04, 4.16828876e-04, 1.85796379e-04,\n",
       "        4.42920985e-04, 3.44763936e-04, 2.18646689e-04, 1.64516532e-04,\n",
       "        1.73655727e-04, 4.95680549e-04, 2.31416958e-04, 4.03286802e-04,\n",
       "        3.30986119e-04, 1.42700831e-04, 3.26288508e-04, 5.10704595e-04,\n",
       "        9.93409746e-04, 3.28697159e-04, 4.81235735e-04, 9.16723357e-04,\n",
       "        7.39999037e-05, 2.73935556e-04, 1.62132249e-04, 1.86110062e-04,\n",
       "        4.17999299e-04, 1.75891002e-04, 1.08205334e-03, 4.16648603e-04,\n",
       "        8.30324040e-04, 2.31913290e-04, 5.17216265e-04, 4.56749508e-04,\n",
       "        3.79466987e-04, 3.63951514e-04, 5.45299870e-04, 7.43344479e-04,\n",
       "        1.29953515e-04, 3.03050741e-04, 3.36075120e-04, 1.49945036e-04,\n",
       "        3.86911218e-04, 5.79455163e-04, 1.62136274e-04, 2.38881358e-04,\n",
       "        1.83426704e-04, 9.08990441e-04, 8.73021207e-04, 2.10494113e-04,\n",
       "        2.67909310e-04, 4.39704143e-04, 4.12746162e-04, 2.88024351e-04,\n",
       "        2.31234342e-04, 1.43663637e-04, 1.58322834e-04, 1.53154899e-04,\n",
       "        1.93985596e-04, 2.19711670e-03, 1.36723158e-04, 1.47771083e-03,\n",
       "        3.00763336e-04, 1.58945139e-04, 4.15464563e-04, 5.79126328e-04,\n",
       "        5.69897702e-04, 5.52317498e-04, 7.29611501e-04, 5.79813660e-04,\n",
       "        2.04646298e-04, 1.62649819e-04, 9.14456814e-05, 2.12480136e-04,\n",
       "        2.15909970e-04, 5.48390749e-04, 5.72642604e-04, 3.77105201e-04,\n",
       "        4.90862771e-04, 4.43398420e-04, 4.07072070e-04, 8.25708872e-04,\n",
       "        1.13729247e-03, 7.11227444e-04, 8.76213781e-04, 7.91148023e-04,\n",
       "        1.74543674e-04, 2.05008005e-04, 6.53477280e-04, 1.54234776e-04,\n",
       "        3.93439070e-04, 5.66940031e-04, 4.92635411e-04, 4.62432806e-04,\n",
       "        4.28647644e-04, 3.90300101e-04, 4.10377029e-04, 1.24244033e-03,\n",
       "        3.21485734e-04, 1.73199491e-03, 1.45036392e-03, 1.10325601e-03,\n",
       "        2.03690825e-04, 3.15679142e-04, 3.97410958e-04, 6.83022394e-04,\n",
       "        1.91057255e-04, 4.39654643e-04, 2.32500469e-04, 3.59020497e-04,\n",
       "        4.83870524e-04, 4.72222905e-04, 7.56513280e-04, 3.56200281e-04,\n",
       "        5.21103856e-04, 8.21909748e-04, 1.67470000e-03, 1.32882283e-03,\n",
       "        4.26201389e-04, 1.65077099e-04, 1.16269288e-04, 1.13886104e-04,\n",
       "        4.11676471e-05, 1.99740591e-04, 8.60552235e-05, 2.51823053e-04,\n",
       "        3.33301786e-04, 4.10708722e-04, 4.95424674e-04, 2.92784981e-04,\n",
       "        2.46166369e-04, 1.21770309e-03, 7.60783720e-04, 3.47202309e-04,\n",
       "        2.43502888e-04, 3.76326417e-04, 6.21846481e-05, 6.95361019e-05,\n",
       "        2.97438009e-04, 2.49645880e-04, 1.36956680e-04, 5.32710906e-04,\n",
       "        1.00907458e-03, 6.03786348e-04, 4.79293941e-04, 1.26124381e-03,\n",
       "        8.84654669e-04, 7.31649007e-04, 6.35173209e-04, 6.98506838e-04,\n",
       "        5.22949457e-04, 1.65900042e-04, 5.30751387e-04, 1.60119447e-04,\n",
       "        3.74823234e-04, 5.72621837e-04, 2.35239463e-04, 7.06761894e-04,\n",
       "        2.77335665e-04, 4.86246555e-04, 2.88931031e-04, 7.64910705e-04,\n",
       "        6.00710708e-04, 1.24268200e-03, 4.07089949e-04, 7.12923581e-04,\n",
       "        3.36956316e-04, 3.44384842e-04, 2.08091353e-04, 3.29533911e-04,\n",
       "        2.03691361e-04, 5.17853905e-04, 4.65040934e-04, 2.32349933e-04,\n",
       "        5.60285974e-04, 5.74183254e-04, 4.17627894e-04, 2.37648148e-04,\n",
       "        5.45819763e-04, 1.26676012e-03, 4.57430967e-04, 7.30264511e-04,\n",
       "        2.03442840e-04, 5.79431906e-05, 2.53773139e-04, 9.86001934e-05,\n",
       "        1.70038486e-04, 4.66232330e-04, 5.07428466e-04, 9.94244467e-05,\n",
       "        3.47960154e-04, 1.49489430e-04, 1.82237867e-04, 5.11468535e-04,\n",
       "        5.17112972e-04, 4.36171125e-04, 8.38414294e-04, 8.65540509e-04,\n",
       "        1.96940189e-04, 9.92975343e-05, 1.91401746e-04, 1.07467756e-04,\n",
       "        1.90103045e-04, 3.77057698e-04, 4.23295320e-04, 3.66285330e-04,\n",
       "        9.54540367e-04, 5.68447406e-04, 6.30119104e-04, 8.32633548e-04,\n",
       "        6.81923511e-04, 4.87643809e-04, 2.96358188e-04, 8.83218511e-04,\n",
       "        2.45682987e-04, 5.63934714e-04, 1.32080674e-04, 3.98200124e-04,\n",
       "        2.83191954e-04, 2.80028145e-03, 2.84382379e-04, 3.82531197e-04,\n",
       "        3.11355371e-04, 5.15099252e-04, 5.57828079e-04, 1.47690784e-03,\n",
       "        4.18187373e-04, 8.33026403e-04, 6.30084032e-04, 6.96217484e-04,\n",
       "        3.89174496e-05, 2.60362983e-04, 4.91718699e-04, 1.51669420e-04,\n",
       "        9.05970021e-05, 1.28329732e-03, 6.53347258e-04, 5.62392454e-04,\n",
       "        5.73787082e-04, 1.09642027e-03, 3.37545914e-04, 1.39204089e-03,\n",
       "        2.63346358e-03, 4.50924986e-04, 4.89843127e-04, 1.16372935e-03]),\n",
       " 'param_colsample_bytree': masked_array(data=[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,\n",
       "                    0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,\n",
       "                    0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,\n",
       "                    0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,\n",
       "                    0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,\n",
       "                    0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,\n",
       "                    0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,\n",
       "                    0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,\n",
       "                    0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,\n",
       "                    0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,\n",
       "                    0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,\n",
       "                    0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,\n",
       "                    0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,\n",
       "                    0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,\n",
       "                    0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,\n",
       "                    0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,\n",
       "                    0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,\n",
       "                    0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,\n",
       "                    0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,\n",
       "                    0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,\n",
       "                    0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,\n",
       "                    0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_learning_rate': masked_array(data=[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_depth': masked_array(data=[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 5, 5,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 10, 10, 10,\n",
       "                    10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12,\n",
       "                    12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "                    12, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 5,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 10, 10,\n",
       "                    10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "                    12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "                    12, 12, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 10, 10,\n",
       "                    10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "                    12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "                    12, 12, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 10, 10,\n",
       "                    10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "                    12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "                    12, 12, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 10, 10,\n",
       "                    10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "                    12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "                    12, 12, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 10, 10,\n",
       "                    10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "                    12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "                    12, 12, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 10, 10,\n",
       "                    10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "                    12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "                    12, 12, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 10, 10,\n",
       "                    10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "                    12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "                    12, 12, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 10, 10,\n",
       "                    10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "                    12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "                    12, 12, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 10, 10,\n",
       "                    10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "                    12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "                    12, 12, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 10, 10,\n",
       "                    10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "                    12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "                    12, 12, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 10, 10,\n",
       "                    10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "                    12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "                    12, 12],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_n_estimators': masked_array(data=[100, 100, 100, 100, 300, 300, 300, 300, 500, 500, 500,\n",
       "                    500, 1000, 1000, 1000, 1000, 100, 100, 100, 100, 300,\n",
       "                    300, 300, 300, 500, 500, 500, 500, 1000, 1000, 1000,\n",
       "                    1000, 100, 100, 100, 100, 300, 300, 300, 300, 500, 500,\n",
       "                    500, 500, 1000, 1000, 1000, 1000, 100, 100, 100, 100,\n",
       "                    300, 300, 300, 300, 500, 500, 500, 500, 1000, 1000,\n",
       "                    1000, 1000, 100, 100, 100, 100, 300, 300, 300, 300,\n",
       "                    500, 500, 500, 500, 1000, 1000, 1000, 1000, 100, 100,\n",
       "                    100, 100, 300, 300, 300, 300, 500, 500, 500, 500, 1000,\n",
       "                    1000, 1000, 1000, 100, 100, 100, 100, 300, 300, 300,\n",
       "                    300, 500, 500, 500, 500, 1000, 1000, 1000, 1000, 100,\n",
       "                    100, 100, 100, 300, 300, 300, 300, 500, 500, 500, 500,\n",
       "                    1000, 1000, 1000, 1000, 100, 100, 100, 100, 300, 300,\n",
       "                    300, 300, 500, 500, 500, 500, 1000, 1000, 1000, 1000,\n",
       "                    100, 100, 100, 100, 300, 300, 300, 300, 500, 500, 500,\n",
       "                    500, 1000, 1000, 1000, 1000, 100, 100, 100, 100, 300,\n",
       "                    300, 300, 300, 500, 500, 500, 500, 1000, 1000, 1000,\n",
       "                    1000, 100, 100, 100, 100, 300, 300, 300, 300, 500, 500,\n",
       "                    500, 500, 1000, 1000, 1000, 1000, 100, 100, 100, 100,\n",
       "                    300, 300, 300, 300, 500, 500, 500, 500, 1000, 1000,\n",
       "                    1000, 1000, 100, 100, 100, 100, 300, 300, 300, 300,\n",
       "                    500, 500, 500, 500, 1000, 1000, 1000, 1000, 100, 100,\n",
       "                    100, 100, 300, 300, 300, 300, 500, 500, 500, 500, 1000,\n",
       "                    1000, 1000, 1000, 100, 100, 100, 100, 300, 300, 300,\n",
       "                    300, 500, 500, 500, 500, 1000, 1000, 1000, 1000, 100,\n",
       "                    100, 100, 100, 300, 300, 300, 300, 500, 500, 500, 500,\n",
       "                    1000, 1000, 1000, 1000, 100, 100, 100, 100, 300, 300,\n",
       "                    300, 300, 500, 500, 500, 500, 1000, 1000, 1000, 1000,\n",
       "                    100, 100, 100, 100, 300, 300, 300, 300, 500, 500, 500,\n",
       "                    500, 1000, 1000, 1000, 1000, 100, 100, 100, 100, 300,\n",
       "                    300, 300, 300, 500, 500, 500, 500, 1000, 1000, 1000,\n",
       "                    1000, 100, 100, 100, 100, 300, 300, 300, 300, 500, 500,\n",
       "                    500, 500, 1000, 1000, 1000, 1000, 100, 100, 100, 100,\n",
       "                    300, 300, 300, 300, 500, 500, 500, 500, 1000, 1000,\n",
       "                    1000, 1000, 100, 100, 100, 100, 300, 300, 300, 300,\n",
       "                    500, 500, 500, 500, 1000, 1000, 1000, 1000, 100, 100,\n",
       "                    100, 100, 300, 300, 300, 300, 500, 500, 500, 500, 1000,\n",
       "                    1000, 1000, 1000, 100, 100, 100, 100, 300, 300, 300,\n",
       "                    300, 500, 500, 500, 500, 1000, 1000, 1000, 1000, 100,\n",
       "                    100, 100, 100, 300, 300, 300, 300, 500, 500, 500, 500,\n",
       "                    1000, 1000, 1000, 1000, 100, 100, 100, 100, 300, 300,\n",
       "                    300, 300, 500, 500, 500, 500, 1000, 1000, 1000, 1000,\n",
       "                    100, 100, 100, 100, 300, 300, 300, 300, 500, 500, 500,\n",
       "                    500, 1000, 1000, 1000, 1000, 100, 100, 100, 100, 300,\n",
       "                    300, 300, 300, 500, 500, 500, 500, 1000, 1000, 1000,\n",
       "                    1000, 100, 100, 100, 100, 300, 300, 300, 300, 500, 500,\n",
       "                    500, 500, 1000, 1000, 1000, 1000, 100, 100, 100, 100,\n",
       "                    300, 300, 300, 300, 500, 500, 500, 500, 1000, 1000,\n",
       "                    1000, 1000, 100, 100, 100, 100, 300, 300, 300, 300,\n",
       "                    500, 500, 500, 500, 1000, 1000, 1000, 1000, 100, 100,\n",
       "                    100, 100, 300, 300, 300, 300, 500, 500, 500, 500, 1000,\n",
       "                    1000, 1000, 1000, 100, 100, 100, 100, 300, 300, 300,\n",
       "                    300, 500, 500, 500, 500, 1000, 1000, 1000, 1000, 100,\n",
       "                    100, 100, 100, 300, 300, 300, 300, 500, 500, 500, 500,\n",
       "                    1000, 1000, 1000, 1000, 100, 100, 100, 100, 300, 300,\n",
       "                    300, 300, 500, 500, 500, 500, 1000, 1000, 1000, 1000,\n",
       "                    100, 100, 100, 100, 300, 300, 300, 300, 500, 500, 500,\n",
       "                    500, 1000, 1000, 1000, 1000, 100, 100, 100, 100, 300,\n",
       "                    300, 300, 300, 500, 500, 500, 500, 1000, 1000, 1000,\n",
       "                    1000, 100, 100, 100, 100, 300, 300, 300, 300, 500, 500,\n",
       "                    500, 500, 1000, 1000, 1000, 1000, 100, 100, 100, 100,\n",
       "                    300, 300, 300, 300, 500, 500, 500, 500, 1000, 1000,\n",
       "                    1000, 1000, 100, 100, 100, 100, 300, 300, 300, 300,\n",
       "                    500, 500, 500, 500, 1000, 1000, 1000, 1000, 100, 100,\n",
       "                    100, 100, 300, 300, 300, 300, 500, 500, 500, 500, 1000,\n",
       "                    1000, 1000, 1000, 100, 100, 100, 100, 300, 300, 300,\n",
       "                    300, 500, 500, 500, 500, 1000, 1000, 1000, 1000, 100,\n",
       "                    100, 100, 100, 300, 300, 300, 300, 500, 500, 500, 500,\n",
       "                    1000, 1000, 1000, 1000, 100, 100, 100, 100, 300, 300,\n",
       "                    300, 300, 500, 500, 500, 500, 1000, 1000, 1000, 1000,\n",
       "                    100, 100, 100, 100, 300, 300, 300, 300, 500, 500, 500,\n",
       "                    500, 1000, 1000, 1000, 1000, 100, 100, 100, 100, 300,\n",
       "                    300, 300, 300, 500, 500, 500, 500, 1000, 1000, 1000,\n",
       "                    1000, 100, 100, 100, 100, 300, 300, 300, 300, 500, 500,\n",
       "                    500, 500, 1000, 1000, 1000, 1000],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_subsample': masked_array(data=[0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25,\n",
       "                    0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0,\n",
       "                    0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25,\n",
       "                    0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0,\n",
       "                    0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25,\n",
       "                    0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0,\n",
       "                    0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25,\n",
       "                    0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0,\n",
       "                    0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25,\n",
       "                    0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0,\n",
       "                    0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25,\n",
       "                    0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0,\n",
       "                    0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25,\n",
       "                    0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0,\n",
       "                    0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25,\n",
       "                    0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0,\n",
       "                    0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25,\n",
       "                    0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0,\n",
       "                    0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25,\n",
       "                    0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0,\n",
       "                    0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25,\n",
       "                    0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0,\n",
       "                    0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25,\n",
       "                    0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0,\n",
       "                    0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25,\n",
       "                    0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0,\n",
       "                    0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25,\n",
       "                    0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0,\n",
       "                    0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25,\n",
       "                    0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0,\n",
       "                    0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25,\n",
       "                    0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0,\n",
       "                    0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25,\n",
       "                    0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0,\n",
       "                    0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25,\n",
       "                    0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0,\n",
       "                    0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25,\n",
       "                    0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0,\n",
       "                    0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25,\n",
       "                    0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0,\n",
       "                    0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25,\n",
       "                    0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0,\n",
       "                    0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25,\n",
       "                    0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0,\n",
       "                    0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25,\n",
       "                    0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0,\n",
       "                    0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25,\n",
       "                    0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0,\n",
       "                    0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25,\n",
       "                    0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0,\n",
       "                    0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25,\n",
       "                    0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0,\n",
       "                    0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25,\n",
       "                    0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0,\n",
       "                    0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25,\n",
       "                    0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0,\n",
       "                    0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25,\n",
       "                    0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0,\n",
       "                    0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25,\n",
       "                    0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0,\n",
       "                    0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25,\n",
       "                    0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0,\n",
       "                    0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25,\n",
       "                    0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0,\n",
       "                    0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25,\n",
       "                    0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0,\n",
       "                    0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25,\n",
       "                    0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0,\n",
       "                    0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25,\n",
       "                    0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0,\n",
       "                    0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25,\n",
       "                    0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0,\n",
       "                    0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25,\n",
       "                    0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0,\n",
       "                    0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25,\n",
       "                    0.5, 1.0, 0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0,\n",
       "                    0.05, 0.25, 0.5, 1.0, 0.05, 0.25, 0.5, 1.0],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.05,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.25,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 5,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 100,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 300,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 500,\n",
       "   'subsample': 1.0},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.05},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.25},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 0.5},\n",
       "  {'colsample_bytree': 1.0,\n",
       "   'learning_rate': 0.5,\n",
       "   'max_depth': 12,\n",
       "   'n_estimators': 1000,\n",
       "   'subsample': 1.0}],\n",
       " 'split0_test_score': array([0.66891892, 0.70945946, 0.73648649, 0.72972973, 0.73648649,\n",
       "        0.7972973 , 0.81756757, 0.81418919, 0.77364865, 0.81756757,\n",
       "        0.84459459, 0.83783784, 0.79054054, 0.83783784, 0.83783784,\n",
       "        0.86148649, 0.67567568, 0.70945946, 0.72972973, 0.73310811,\n",
       "        0.73986486, 0.78716216, 0.80067568, 0.80743243, 0.77027027,\n",
       "        0.80405405, 0.81418919, 0.80405405, 0.77702703, 0.81756757,\n",
       "        0.82094595, 0.82094595, 0.6722973 , 0.71621622, 0.71959459,\n",
       "        0.72635135, 0.73986486, 0.78040541, 0.79054054, 0.79391892,\n",
       "        0.77027027, 0.81418919, 0.81081081, 0.80743243, 0.78040541,\n",
       "        0.81081081, 0.81756757, 0.82094595, 0.6722973 , 0.70945946,\n",
       "        0.71283784, 0.72972973, 0.73986486, 0.77364865, 0.79054054,\n",
       "        0.78716216, 0.77027027, 0.81418919, 0.80067568, 0.80405405,\n",
       "        0.78040541, 0.81418919, 0.81418919, 0.80743243, 0.71959459,\n",
       "        0.75      , 0.78040541, 0.79391892, 0.77364865, 0.80743243,\n",
       "        0.85135135, 0.84797297, 0.79054054, 0.83445946, 0.85472973,\n",
       "        0.86486486, 0.80067568, 0.84459459, 0.86148649, 0.85472973,\n",
       "        0.72972973, 0.76689189, 0.78716216, 0.77702703, 0.78378378,\n",
       "        0.81418919, 0.83445946, 0.81081081, 0.78040541, 0.81418919,\n",
       "        0.82432432, 0.83445946, 0.78716216, 0.81081081, 0.83783784,\n",
       "        0.8277027 , 0.73310811, 0.76351351, 0.76013514, 0.77702703,\n",
       "        0.78378378, 0.80067568, 0.80067568, 0.80405405, 0.78716216,\n",
       "        0.80743243, 0.81756757, 0.81081081, 0.78716216, 0.80405405,\n",
       "        0.82094595, 0.81418919, 0.73310811, 0.76351351, 0.77027027,\n",
       "        0.77702703, 0.78378378, 0.81081081, 0.7972973 , 0.7972973 ,\n",
       "        0.78716216, 0.80067568, 0.80743243, 0.80743243, 0.78716216,\n",
       "        0.80405405, 0.80743243, 0.82094595, 0.78378378, 0.77027027,\n",
       "        0.82094595, 0.85135135, 0.75675676, 0.77702703, 0.85472973,\n",
       "        0.86148649, 0.77027027, 0.80067568, 0.84797297, 0.85135135,\n",
       "        0.7972973 , 0.8277027 , 0.83445946, 0.83445946, 0.75337838,\n",
       "        0.7972973 , 0.81418919, 0.8277027 , 0.78040541, 0.79391892,\n",
       "        0.84121622, 0.84121622, 0.75337838, 0.79054054, 0.84121622,\n",
       "        0.83783784, 0.79391892, 0.81081081, 0.84121622, 0.83445946,\n",
       "        0.74324324, 0.79391892, 0.78378378, 0.7972973 , 0.74662162,\n",
       "        0.79391892, 0.81081081, 0.8277027 , 0.76351351, 0.81081081,\n",
       "        0.82094595, 0.83783784, 0.79054054, 0.80405405, 0.80405405,\n",
       "        0.83783784, 0.74324324, 0.77364865, 0.77364865, 0.80743243,\n",
       "        0.74662162, 0.78716216, 0.81081081, 0.82094595, 0.76351351,\n",
       "        0.80405405, 0.82094595, 0.82432432, 0.79054054, 0.81081081,\n",
       "        0.8277027 , 0.83108108, 0.76689189, 0.81081081, 0.82094595,\n",
       "        0.81756757, 0.78716216, 0.8277027 , 0.83445946, 0.84121622,\n",
       "        0.80743243, 0.83108108, 0.85135135, 0.86486486, 0.79391892,\n",
       "        0.85135135, 0.86824324, 0.87162162, 0.76689189, 0.81418919,\n",
       "        0.81418919, 0.84797297, 0.79391892, 0.83108108, 0.86486486,\n",
       "        0.86824324, 0.79054054, 0.84459459, 0.87162162, 0.875     ,\n",
       "        0.79054054, 0.84797297, 0.86824324, 0.86824324, 0.77027027,\n",
       "        0.82094595, 0.83783784, 0.84459459, 0.79391892, 0.83445946,\n",
       "        0.84459459, 0.85135135, 0.80067568, 0.85472973, 0.86148649,\n",
       "        0.85135135, 0.7972973 , 0.83783784, 0.86148649, 0.84459459,\n",
       "        0.77027027, 0.82094595, 0.84121622, 0.83783784, 0.79054054,\n",
       "        0.83108108, 0.85810811, 0.85472973, 0.79391892, 0.84797297,\n",
       "        0.86486486, 0.86148649, 0.79391892, 0.84797297, 0.85810811,\n",
       "        0.85135135, 0.77702703, 0.81081081, 0.84121622, 0.83783784,\n",
       "        0.80743243, 0.84797297, 0.86824324, 0.86824324, 0.7972973 ,\n",
       "        0.84797297, 0.87837838, 0.86824324, 0.79391892, 0.85472973,\n",
       "        0.87837838, 0.875     , 0.78378378, 0.83445946, 0.83783784,\n",
       "        0.86824324, 0.79391892, 0.83445946, 0.86148649, 0.88175676,\n",
       "        0.78378378, 0.83783784, 0.85810811, 0.86486486, 0.78716216,\n",
       "        0.83108108, 0.84459459, 0.84121622, 0.78378378, 0.82432432,\n",
       "        0.84797297, 0.85472973, 0.7972973 , 0.84121622, 0.85135135,\n",
       "        0.85135135, 0.78716216, 0.83783784, 0.84797297, 0.84459459,\n",
       "        0.79391892, 0.82432432, 0.84121622, 0.83108108, 0.78378378,\n",
       "        0.81081081, 0.84797297, 0.85135135, 0.79054054, 0.83445946,\n",
       "        0.85135135, 0.85810811, 0.79054054, 0.84121622, 0.84121622,\n",
       "        0.84121622, 0.77702703, 0.82432432, 0.83783784, 0.8277027 ,\n",
       "        0.77702703, 0.84797297, 0.88851351, 0.87837838, 0.75337838,\n",
       "        0.84459459, 0.85810811, 0.87162162, 0.76013514, 0.83445946,\n",
       "        0.86148649, 0.86486486, 0.76351351, 0.84459459, 0.84121622,\n",
       "        0.85135135, 0.77364865, 0.8277027 , 0.875     , 0.875     ,\n",
       "        0.76689189, 0.82094595, 0.85810811, 0.85135135, 0.73310811,\n",
       "        0.82094595, 0.84121622, 0.85135135, 0.74324324, 0.83445946,\n",
       "        0.83445946, 0.83783784, 0.74662162, 0.82094595, 0.85135135,\n",
       "        0.84797297, 0.76351351, 0.83108108, 0.84797297, 0.83445946,\n",
       "        0.76013514, 0.8277027 , 0.85810811, 0.8277027 , 0.74662162,\n",
       "        0.81756757, 0.83783784, 0.8277027 , 0.74662162, 0.83108108,\n",
       "        0.85472973, 0.84459459, 0.76351351, 0.84121622, 0.86148649,\n",
       "        0.83783784, 0.76013514, 0.83445946, 0.86148649, 0.83108108,\n",
       "        0.74662162, 0.83108108, 0.84121622, 0.82432432, 0.75337838,\n",
       "        0.81756757, 0.82094595, 0.82094595, 0.80067568, 0.83108108,\n",
       "        0.83108108, 0.84459459, 0.80743243, 0.83108108, 0.84459459,\n",
       "        0.86486486, 0.78716216, 0.85135135, 0.86486486, 0.875     ,\n",
       "        0.78716216, 0.82094595, 0.8277027 , 0.84797297, 0.79391892,\n",
       "        0.85472973, 0.86148649, 0.875     , 0.79391892, 0.84797297,\n",
       "        0.86824324, 0.875     , 0.78716216, 0.85135135, 0.87837838,\n",
       "        0.87837838, 0.77702703, 0.83783784, 0.85135135, 0.87837838,\n",
       "        0.79391892, 0.85472973, 0.87837838, 0.88851351, 0.79054054,\n",
       "        0.85472973, 0.87162162, 0.88851351, 0.7972973 , 0.84121622,\n",
       "        0.85810811, 0.86824324, 0.77364865, 0.83445946, 0.85810811,\n",
       "        0.86486486, 0.80067568, 0.85135135, 0.86148649, 0.88175676,\n",
       "        0.79391892, 0.83783784, 0.85472973, 0.88175676, 0.77364865,\n",
       "        0.83783784, 0.85472973, 0.86486486, 0.77364865, 0.81081081,\n",
       "        0.83783784, 0.83108108, 0.80743243, 0.84797297, 0.86486486,\n",
       "        0.86486486, 0.78716216, 0.84121622, 0.87162162, 0.87162162,\n",
       "        0.78378378, 0.85472973, 0.86486486, 0.88175676, 0.80067568,\n",
       "        0.84459459, 0.85472973, 0.875     , 0.78716216, 0.85135135,\n",
       "        0.88175676, 0.87837838, 0.78716216, 0.84797297, 0.87837838,\n",
       "        0.87837838, 0.77702703, 0.84459459, 0.85472973, 0.86148649,\n",
       "        0.78378378, 0.82094595, 0.875     , 0.88175676, 0.77364865,\n",
       "        0.83108108, 0.86824324, 0.875     , 0.77027027, 0.8277027 ,\n",
       "        0.86486486, 0.86486486, 0.78378378, 0.83783784, 0.85810811,\n",
       "        0.85135135, 0.78378378, 0.82432432, 0.86148649, 0.87162162,\n",
       "        0.78378378, 0.84459459, 0.86148649, 0.87162162, 0.77027027,\n",
       "        0.83108108, 0.86486486, 0.85810811, 0.77027027, 0.84121622,\n",
       "        0.85472973, 0.84121622, 0.76013514, 0.83783784, 0.86148649,\n",
       "        0.86824324, 0.77027027, 0.8277027 , 0.875     , 0.86824324,\n",
       "        0.77364865, 0.82432432, 0.87162162, 0.86486486, 0.75      ,\n",
       "        0.82094595, 0.85472973, 0.84459459, 0.75675676, 0.85810811,\n",
       "        0.86486486, 0.87837838, 0.76013514, 0.84459459, 0.84797297,\n",
       "        0.85135135, 0.73986486, 0.84459459, 0.85135135, 0.84459459,\n",
       "        0.76351351, 0.83108108, 0.84121622, 0.83445946, 0.76013514,\n",
       "        0.8277027 , 0.84797297, 0.85135135, 0.73648649, 0.82094595,\n",
       "        0.83445946, 0.8277027 , 0.74324324, 0.80405405, 0.8277027 ,\n",
       "        0.82432432, 0.78040541, 0.80743243, 0.83445946, 0.82432432,\n",
       "        0.76013514, 0.8277027 , 0.85135135, 0.84797297, 0.73648649,\n",
       "        0.83108108, 0.84459459, 0.83108108, 0.74324324, 0.81418919,\n",
       "        0.84459459, 0.82432432, 0.78040541, 0.81756757, 0.85135135,\n",
       "        0.82432432, 0.77364865, 0.81756757, 0.83108108, 0.82432432,\n",
       "        0.80405405, 0.8277027 , 0.84121622, 0.84797297, 0.80405405,\n",
       "        0.83783784, 0.86824324, 0.87162162, 0.7972973 , 0.85135135,\n",
       "        0.86486486, 0.87162162, 0.7972973 , 0.82432432, 0.83445946,\n",
       "        0.84797297, 0.80743243, 0.85810811, 0.86824324, 0.87837838,\n",
       "        0.7972973 , 0.86148649, 0.86824324, 0.87837838, 0.79054054,\n",
       "        0.84797297, 0.875     , 0.875     , 0.77702703, 0.8277027 ,\n",
       "        0.86486486, 0.86148649, 0.7972973 , 0.85472973, 0.87162162,\n",
       "        0.875     , 0.80067568, 0.85472973, 0.86486486, 0.87837838,\n",
       "        0.80067568, 0.84121622, 0.85472973, 0.86824324, 0.78040541,\n",
       "        0.84121622, 0.86148649, 0.86486486, 0.79054054, 0.85135135,\n",
       "        0.85810811, 0.87837838, 0.79391892, 0.84459459, 0.85472973,\n",
       "        0.86486486, 0.80405405, 0.83783784, 0.84459459, 0.86148649,\n",
       "        0.79054054, 0.8277027 , 0.83445946, 0.83108108, 0.81418919,\n",
       "        0.82432432, 0.86824324, 0.86486486, 0.77364865, 0.85135135,\n",
       "        0.86148649, 0.86486486, 0.79391892, 0.85135135, 0.87162162,\n",
       "        0.87162162, 0.80067568, 0.8277027 , 0.86148649, 0.87162162,\n",
       "        0.81081081, 0.85135135, 0.88175676, 0.875     , 0.79391892,\n",
       "        0.85135135, 0.87162162, 0.88175676, 0.79391892, 0.83783784,\n",
       "        0.85810811, 0.86824324, 0.79054054, 0.84121622, 0.86824324,\n",
       "        0.87837838, 0.81081081, 0.84121622, 0.86486486, 0.875     ,\n",
       "        0.80405405, 0.83445946, 0.85472973, 0.86824324, 0.78040541,\n",
       "        0.84459459, 0.84459459, 0.84797297, 0.7972973 , 0.86148649,\n",
       "        0.875     , 0.87162162, 0.79391892, 0.84121622, 0.85810811,\n",
       "        0.87162162, 0.76689189, 0.83783784, 0.84797297, 0.87162162,\n",
       "        0.79054054, 0.83783784, 0.83445946, 0.86148649, 0.78040541,\n",
       "        0.84459459, 0.87162162, 0.875     , 0.74662162, 0.86486486,\n",
       "        0.85135135, 0.87162162, 0.77364865, 0.84797297, 0.85810811,\n",
       "        0.86148649, 0.76689189, 0.84121622, 0.83783784, 0.85472973,\n",
       "        0.77364865, 0.84797297, 0.87837838, 0.87837838, 0.72635135,\n",
       "        0.82094595, 0.84797297, 0.84797297, 0.73648649, 0.8277027 ,\n",
       "        0.84797297, 0.83783784, 0.74324324, 0.81418919, 0.84121622,\n",
       "        0.83783784, 0.76689189, 0.83445946, 0.85135135, 0.86486486,\n",
       "        0.75337838, 0.7972973 , 0.84459459, 0.84121622, 0.77364865,\n",
       "        0.80405405, 0.84797297, 0.84121622, 0.78040541, 0.79391892,\n",
       "        0.84797297, 0.84459459, 0.76689189, 0.83108108, 0.85810811,\n",
       "        0.86824324, 0.75337838, 0.81081081, 0.84459459, 0.85135135,\n",
       "        0.77364865, 0.80743243, 0.83783784, 0.84797297, 0.78040541,\n",
       "        0.79391892, 0.84121622, 0.84121622]),\n",
       " 'split1_test_score': array([0.63851351, 0.65202703, 0.65202703, 0.67905405, 0.66554054,\n",
       "        0.7027027 , 0.71283784, 0.74324324, 0.69256757, 0.76013514,\n",
       "        0.73648649, 0.75      , 0.75      , 0.81756757, 0.81081081,\n",
       "        0.83108108, 0.63175676, 0.65540541, 0.6722973 , 0.67905405,\n",
       "        0.66554054, 0.71283784, 0.72297297, 0.75      , 0.71283784,\n",
       "        0.74324324, 0.74324324, 0.75337838, 0.74662162, 0.79054054,\n",
       "        0.78378378, 0.7972973 , 0.63175676, 0.65202703, 0.66891892,\n",
       "        0.68581081, 0.66554054, 0.70608108, 0.72297297, 0.73310811,\n",
       "        0.71621622, 0.73986486, 0.72635135, 0.73986486, 0.75337838,\n",
       "        0.78040541, 0.77027027, 0.76689189, 0.63175676, 0.65878378,\n",
       "        0.65878378, 0.68581081, 0.66554054, 0.70945946, 0.72297297,\n",
       "        0.72972973, 0.71621622, 0.73986486, 0.72635135, 0.72972973,\n",
       "        0.75337838, 0.78040541, 0.77027027, 0.74324324, 0.65540541,\n",
       "        0.67567568, 0.69594595, 0.70608108, 0.69932432, 0.75337838,\n",
       "        0.77702703, 0.79391892, 0.72297297, 0.79391892, 0.80743243,\n",
       "        0.8277027 , 0.75      , 0.83108108, 0.83108108, 0.84797297,\n",
       "        0.66891892, 0.69256757, 0.7027027 , 0.73648649, 0.70945946,\n",
       "        0.77027027, 0.75337838, 0.76013514, 0.73310811, 0.79054054,\n",
       "        0.77702703, 0.80067568, 0.77364865, 0.81418919, 0.80405405,\n",
       "        0.78716216, 0.66554054, 0.68581081, 0.69594595, 0.72635135,\n",
       "        0.70945946, 0.75      , 0.74662162, 0.75      , 0.73648649,\n",
       "        0.78040541, 0.77027027, 0.77027027, 0.77027027, 0.79054054,\n",
       "        0.79054054, 0.77364865, 0.66554054, 0.67905405, 0.69594595,\n",
       "        0.71959459, 0.70945946, 0.75337838, 0.73986486, 0.73986486,\n",
       "        0.73648649, 0.78040541, 0.77027027, 0.75675676, 0.77027027,\n",
       "        0.80743243, 0.78378378, 0.76351351, 0.71621622, 0.78378378,\n",
       "        0.80067568, 0.8277027 , 0.74662162, 0.8277027 , 0.83445946,\n",
       "        0.84797297, 0.74662162, 0.81756757, 0.81418919, 0.8277027 ,\n",
       "        0.72635135, 0.82094595, 0.81756757, 0.81756757, 0.70945946,\n",
       "        0.78040541, 0.7972973 , 0.80067568, 0.77702703, 0.81756757,\n",
       "        0.82432432, 0.79054054, 0.73648649, 0.80067568, 0.80405405,\n",
       "        0.79391892, 0.73986486, 0.80067568, 0.81081081, 0.79054054,\n",
       "        0.71621622, 0.75337838, 0.78716216, 0.77364865, 0.75675676,\n",
       "        0.78716216, 0.81081081, 0.78378378, 0.74662162, 0.78716216,\n",
       "        0.7972973 , 0.78040541, 0.72972973, 0.78716216, 0.79391892,\n",
       "        0.79054054, 0.71621622, 0.74662162, 0.79054054, 0.74662162,\n",
       "        0.75675676, 0.79054054, 0.82094595, 0.77702703, 0.74662162,\n",
       "        0.78716216, 0.80743243, 0.78040541, 0.72972973, 0.79054054,\n",
       "        0.80405405, 0.78378378, 0.73648649, 0.75337838, 0.76351351,\n",
       "        0.78378378, 0.76351351, 0.75675676, 0.79054054, 0.7972973 ,\n",
       "        0.75337838, 0.76013514, 0.80067568, 0.81418919, 0.77027027,\n",
       "        0.78378378, 0.81756757, 0.83108108, 0.71283784, 0.75337838,\n",
       "        0.78378378, 0.78040541, 0.77702703, 0.77702703, 0.81081081,\n",
       "        0.82432432, 0.76351351, 0.81081081, 0.82094595, 0.83108108,\n",
       "        0.77702703, 0.81081081, 0.81756757, 0.83108108, 0.71959459,\n",
       "        0.75337838, 0.76689189, 0.78040541, 0.78716216, 0.80067568,\n",
       "        0.82432432, 0.81081081, 0.76689189, 0.80067568, 0.81756757,\n",
       "        0.81756757, 0.78040541, 0.80405405, 0.82094595, 0.8277027 ,\n",
       "        0.71959459, 0.75337838, 0.76351351, 0.75337838, 0.78716216,\n",
       "        0.80067568, 0.82432432, 0.81081081, 0.76689189, 0.80405405,\n",
       "        0.8277027 , 0.81418919, 0.78040541, 0.79391892, 0.81081081,\n",
       "        0.81418919, 0.70945946, 0.74324324, 0.77364865, 0.77702703,\n",
       "        0.78040541, 0.77027027, 0.80743243, 0.81418919, 0.77027027,\n",
       "        0.79054054, 0.8277027 , 0.83108108, 0.77027027, 0.80743243,\n",
       "        0.81418919, 0.83783784, 0.74324324, 0.77702703, 0.7972973 ,\n",
       "        0.82432432, 0.78040541, 0.80067568, 0.82094595, 0.8277027 ,\n",
       "        0.78040541, 0.7972973 , 0.81756757, 0.82432432, 0.79054054,\n",
       "        0.80067568, 0.81418919, 0.8277027 , 0.75337838, 0.79391892,\n",
       "        0.81081081, 0.8277027 , 0.78716216, 0.81081081, 0.82094595,\n",
       "        0.82432432, 0.77364865, 0.81081081, 0.81081081, 0.83108108,\n",
       "        0.77702703, 0.81418919, 0.79391892, 0.81756757, 0.75337838,\n",
       "        0.77702703, 0.80405405, 0.82094595, 0.78716216, 0.80743243,\n",
       "        0.80743243, 0.81081081, 0.77364865, 0.80743243, 0.80405405,\n",
       "        0.82432432, 0.77702703, 0.80743243, 0.7972973 , 0.81418919,\n",
       "        0.75675676, 0.7972973 , 0.81756757, 0.8277027 , 0.75675676,\n",
       "        0.79054054, 0.82432432, 0.8277027 , 0.73986486, 0.80067568,\n",
       "        0.81756757, 0.83108108, 0.76013514, 0.80743243, 0.81081081,\n",
       "        0.83445946, 0.76013514, 0.79054054, 0.82094595, 0.82432432,\n",
       "        0.72635135, 0.80067568, 0.81418919, 0.8277027 , 0.74662162,\n",
       "        0.79054054, 0.80743243, 0.81756757, 0.74324324, 0.78040541,\n",
       "        0.78716216, 0.81418919, 0.71959459, 0.79391892, 0.80743243,\n",
       "        0.82432432, 0.75      , 0.76689189, 0.79391892, 0.81756757,\n",
       "        0.71621622, 0.77027027, 0.77027027, 0.81418919, 0.73310811,\n",
       "        0.76689189, 0.77702703, 0.81418919, 0.71959459, 0.78378378,\n",
       "        0.80743243, 0.8277027 , 0.75      , 0.78040541, 0.80743243,\n",
       "        0.81081081, 0.71621622, 0.76689189, 0.79054054, 0.81418919,\n",
       "        0.73310811, 0.76351351, 0.78378378, 0.7972973 , 0.72972973,\n",
       "        0.75675676, 0.77364865, 0.75675676, 0.76351351, 0.76351351,\n",
       "        0.78378378, 0.79391892, 0.76689189, 0.77702703, 0.7972973 ,\n",
       "        0.80743243, 0.77702703, 0.79054054, 0.82094595, 0.83108108,\n",
       "        0.72972973, 0.77027027, 0.78378378, 0.79054054, 0.76689189,\n",
       "        0.78716216, 0.82094595, 0.81081081, 0.76689189, 0.80743243,\n",
       "        0.82432432, 0.8277027 , 0.78040541, 0.81081081, 0.82094595,\n",
       "        0.82094595, 0.71959459, 0.79054054, 0.80067568, 0.81081081,\n",
       "        0.76689189, 0.78716216, 0.81418919, 0.82094595, 0.76689189,\n",
       "        0.79391892, 0.82432432, 0.81756757, 0.77027027, 0.78716216,\n",
       "        0.81756757, 0.81418919, 0.72635135, 0.80067568, 0.7972973 ,\n",
       "        0.80405405, 0.76351351, 0.79391892, 0.81418919, 0.82094595,\n",
       "        0.76689189, 0.79391892, 0.82094595, 0.81418919, 0.76351351,\n",
       "        0.80405405, 0.81756757, 0.81081081, 0.73310811, 0.77027027,\n",
       "        0.77702703, 0.77364865, 0.76013514, 0.78378378, 0.81418919,\n",
       "        0.81756757, 0.76013514, 0.80067568, 0.82094595, 0.83783784,\n",
       "        0.75      , 0.80743243, 0.82432432, 0.83783784, 0.75      ,\n",
       "        0.78716216, 0.79391892, 0.81418919, 0.77027027, 0.80067568,\n",
       "        0.82094595, 0.81756757, 0.76013514, 0.80405405, 0.83445946,\n",
       "        0.81418919, 0.77027027, 0.80405405, 0.81756757, 0.83108108,\n",
       "        0.73648649, 0.79054054, 0.79391892, 0.82094595, 0.75675676,\n",
       "        0.80067568, 0.81418919, 0.81418919, 0.75      , 0.80743243,\n",
       "        0.80405405, 0.81418919, 0.75675676, 0.79054054, 0.80405405,\n",
       "        0.81081081, 0.73648649, 0.79054054, 0.7972973 , 0.82094595,\n",
       "        0.76351351, 0.79391892, 0.81418919, 0.82094595, 0.76351351,\n",
       "        0.79391892, 0.80405405, 0.81081081, 0.76013514, 0.79054054,\n",
       "        0.80743243, 0.80743243, 0.73986486, 0.80405405, 0.82432432,\n",
       "        0.83783784, 0.76351351, 0.80405405, 0.82432432, 0.8277027 ,\n",
       "        0.76351351, 0.80405405, 0.80067568, 0.83783784, 0.75675676,\n",
       "        0.81418919, 0.81756757, 0.81756757, 0.74324324, 0.78378378,\n",
       "        0.81081081, 0.8277027 , 0.74662162, 0.7972973 , 0.80743243,\n",
       "        0.83445946, 0.72297297, 0.80743243, 0.80743243, 0.82432432,\n",
       "        0.69932432, 0.78040541, 0.80743243, 0.81756757, 0.73648649,\n",
       "        0.80743243, 0.80405405, 0.80405405, 0.72635135, 0.79391892,\n",
       "        0.79391892, 0.80743243, 0.73648649, 0.77702703, 0.78378378,\n",
       "        0.80405405, 0.71283784, 0.77027027, 0.78716216, 0.80067568,\n",
       "        0.73648649, 0.78040541, 0.80743243, 0.81418919, 0.72635135,\n",
       "        0.77702703, 0.7972973 , 0.80743243, 0.73648649, 0.77702703,\n",
       "        0.78040541, 0.80067568, 0.71283784, 0.75675676, 0.78716216,\n",
       "        0.79391892, 0.72972973, 0.75337838, 0.77702703, 0.76351351,\n",
       "        0.73648649, 0.77702703, 0.77364865, 0.7972973 , 0.77702703,\n",
       "        0.80067568, 0.78716216, 0.81418919, 0.76351351, 0.80405405,\n",
       "        0.80743243, 0.82432432, 0.73986486, 0.78040541, 0.78040541,\n",
       "        0.78378378, 0.76351351, 0.7972973 , 0.82094595, 0.8277027 ,\n",
       "        0.76351351, 0.81756757, 0.83108108, 0.8277027 , 0.75675676,\n",
       "        0.81756757, 0.8277027 , 0.83108108, 0.75337838, 0.78378378,\n",
       "        0.80743243, 0.81418919, 0.75675676, 0.80067568, 0.82432432,\n",
       "        0.82432432, 0.77027027, 0.80067568, 0.82432432, 0.81756757,\n",
       "        0.76689189, 0.80067568, 0.80743243, 0.81418919, 0.75337838,\n",
       "        0.80067568, 0.81418919, 0.80405405, 0.76351351, 0.79391892,\n",
       "        0.82094595, 0.82432432, 0.76351351, 0.79391892, 0.82094595,\n",
       "        0.80743243, 0.77027027, 0.79391892, 0.80405405, 0.7972973 ,\n",
       "        0.73986486, 0.75675676, 0.77702703, 0.77364865, 0.75      ,\n",
       "        0.78716216, 0.80067568, 0.81756757, 0.76689189, 0.80743243,\n",
       "        0.81418919, 0.83108108, 0.76689189, 0.80743243, 0.81081081,\n",
       "        0.83445946, 0.75675676, 0.78040541, 0.80405405, 0.82094595,\n",
       "        0.76689189, 0.81081081, 0.8277027 , 0.81418919, 0.77027027,\n",
       "        0.81081081, 0.82094595, 0.82432432, 0.77027027, 0.80067568,\n",
       "        0.82094595, 0.82432432, 0.76013514, 0.77702703, 0.81081081,\n",
       "        0.82094595, 0.75675676, 0.78378378, 0.7972973 , 0.82094595,\n",
       "        0.76013514, 0.79054054, 0.79054054, 0.81756757, 0.76689189,\n",
       "        0.78040541, 0.79054054, 0.80067568, 0.75337838, 0.78716216,\n",
       "        0.81418919, 0.82094595, 0.77702703, 0.78378378, 0.81081081,\n",
       "        0.81418919, 0.76689189, 0.77364865, 0.80743243, 0.80067568,\n",
       "        0.74662162, 0.77364865, 0.80067568, 0.80067568, 0.74662162,\n",
       "        0.81081081, 0.82432432, 0.83783784, 0.74662162, 0.81756757,\n",
       "        0.81756757, 0.83445946, 0.75337838, 0.80067568, 0.81756757,\n",
       "        0.83445946, 0.73310811, 0.79054054, 0.80067568, 0.81418919,\n",
       "        0.73648649, 0.80405405, 0.81756757, 0.82432432, 0.70608108,\n",
       "        0.78378378, 0.81081081, 0.80405405, 0.71621622, 0.79054054,\n",
       "        0.80067568, 0.80743243, 0.72297297, 0.76689189, 0.79054054,\n",
       "        0.80743243, 0.75337838, 0.77702703, 0.80743243, 0.81756757,\n",
       "        0.72972973, 0.77364865, 0.7972973 , 0.80743243, 0.7027027 ,\n",
       "        0.76013514, 0.79054054, 0.81418919, 0.71621622, 0.76013514,\n",
       "        0.77702703, 0.81756757, 0.75337838, 0.79391892, 0.7972973 ,\n",
       "        0.7972973 , 0.72972973, 0.76689189, 0.78040541, 0.80067568,\n",
       "        0.7027027 , 0.75337838, 0.78378378, 0.80067568, 0.71621622,\n",
       "        0.74324324, 0.78040541, 0.79054054]),\n",
       " 'split2_test_score': array([0.64527027, 0.70945946, 0.70945946, 0.71959459, 0.71621622,\n",
       "        0.75675676, 0.77364865, 0.77702703, 0.80067568, 0.83108108,\n",
       "        0.81081081, 0.81081081, 0.7972973 , 0.84121622, 0.84459459,\n",
       "        0.85810811, 0.64864865, 0.68581081, 0.70608108, 0.72297297,\n",
       "        0.72635135, 0.75675676, 0.77364865, 0.76689189, 0.7972973 ,\n",
       "        0.7972973 , 0.81756757, 0.79391892, 0.80405405, 0.82432432,\n",
       "        0.84459459, 0.82094595, 0.65202703, 0.69932432, 0.71621622,\n",
       "        0.72635135, 0.72635135, 0.74662162, 0.75337838, 0.76351351,\n",
       "        0.7972973 , 0.7972973 , 0.80067568, 0.77702703, 0.80405405,\n",
       "        0.8277027 , 0.83108108, 0.80067568, 0.65202703, 0.69594595,\n",
       "        0.7027027 , 0.72297297, 0.72635135, 0.76013514, 0.76013514,\n",
       "        0.76013514, 0.7972973 , 0.78716216, 0.80067568, 0.78040541,\n",
       "        0.80405405, 0.82432432, 0.8277027 , 0.80067568, 0.70608108,\n",
       "        0.74662162, 0.76689189, 0.75337838, 0.79391892, 0.83108108,\n",
       "        0.86148649, 0.84459459, 0.79054054, 0.8277027 , 0.84797297,\n",
       "        0.85810811, 0.78716216, 0.84459459, 0.84459459, 0.84797297,\n",
       "        0.71283784, 0.72297297, 0.77027027, 0.74324324, 0.78716216,\n",
       "        0.80405405, 0.82432432, 0.79391892, 0.78378378, 0.82094595,\n",
       "        0.84121622, 0.80743243, 0.78040541, 0.82094595, 0.83783784,\n",
       "        0.81081081, 0.71959459, 0.74324324, 0.76689189, 0.74662162,\n",
       "        0.79391892, 0.78378378, 0.80405405, 0.76689189, 0.78716216,\n",
       "        0.80405405, 0.82094595, 0.80405405, 0.78716216, 0.82432432,\n",
       "        0.82432432, 0.82432432, 0.71959459, 0.74324324, 0.75675676,\n",
       "        0.75337838, 0.79391892, 0.79054054, 0.80743243, 0.77702703,\n",
       "        0.78716216, 0.80743243, 0.83445946, 0.80405405, 0.78716216,\n",
       "        0.82094595, 0.82432432, 0.82094595, 0.73648649, 0.79391892,\n",
       "        0.8277027 , 0.83445946, 0.76351351, 0.81081081, 0.83783784,\n",
       "        0.85472973, 0.80743243, 0.83108108, 0.84121622, 0.84797297,\n",
       "        0.77702703, 0.8277027 , 0.83445946, 0.84797297, 0.75      ,\n",
       "        0.80405405, 0.82432432, 0.81418919, 0.78378378, 0.81418919,\n",
       "        0.83783784, 0.81081081, 0.79054054, 0.82094595, 0.83445946,\n",
       "        0.81756757, 0.75675676, 0.81756757, 0.83445946, 0.81756757,\n",
       "        0.75      , 0.7972973 , 0.79391892, 0.80067568, 0.78378378,\n",
       "        0.82094595, 0.81418919, 0.83108108, 0.78040541, 0.82094595,\n",
       "        0.82094595, 0.83108108, 0.77702703, 0.82432432, 0.80743243,\n",
       "        0.83108108, 0.75      , 0.79391892, 0.80405405, 0.79391892,\n",
       "        0.78378378, 0.80743243, 0.81081081, 0.8277027 , 0.78040541,\n",
       "        0.83108108, 0.81756757, 0.83108108, 0.77702703, 0.82094595,\n",
       "        0.81756757, 0.83108108, 0.77027027, 0.7972973 , 0.7972973 ,\n",
       "        0.7972973 , 0.79391892, 0.80743243, 0.84121622, 0.83108108,\n",
       "        0.79054054, 0.82094595, 0.84121622, 0.86148649, 0.80405405,\n",
       "        0.83783784, 0.85810811, 0.86148649, 0.77027027, 0.8277027 ,\n",
       "        0.84459459, 0.85135135, 0.78716216, 0.82094595, 0.84459459,\n",
       "        0.86486486, 0.80067568, 0.83108108, 0.84797297, 0.86824324,\n",
       "        0.79391892, 0.83108108, 0.85135135, 0.87162162, 0.76689189,\n",
       "        0.83445946, 0.83783784, 0.83783784, 0.79054054, 0.83108108,\n",
       "        0.83783784, 0.85135135, 0.80405405, 0.83445946, 0.84459459,\n",
       "        0.85472973, 0.79054054, 0.83108108, 0.84459459, 0.85135135,\n",
       "        0.76689189, 0.83445946, 0.8277027 , 0.81418919, 0.79391892,\n",
       "        0.83445946, 0.84459459, 0.84797297, 0.80067568, 0.84459459,\n",
       "        0.83783784, 0.85472973, 0.78716216, 0.84121622, 0.84797297,\n",
       "        0.84459459, 0.77702703, 0.80743243, 0.81418919, 0.81756757,\n",
       "        0.79054054, 0.83445946, 0.86486486, 0.86824324, 0.7972973 ,\n",
       "        0.84459459, 0.84797297, 0.86486486, 0.80067568, 0.84459459,\n",
       "        0.84459459, 0.85472973, 0.77364865, 0.81418919, 0.84459459,\n",
       "        0.85135135, 0.78716216, 0.84459459, 0.84459459, 0.86486486,\n",
       "        0.79054054, 0.81756757, 0.85472973, 0.875     , 0.78040541,\n",
       "        0.81756757, 0.85135135, 0.86148649, 0.78716216, 0.83783784,\n",
       "        0.84797297, 0.85472973, 0.79054054, 0.8277027 , 0.83445946,\n",
       "        0.85472973, 0.7972973 , 0.83108108, 0.83783784, 0.85135135,\n",
       "        0.7972973 , 0.83108108, 0.84121622, 0.84459459, 0.78716216,\n",
       "        0.81756757, 0.84459459, 0.83783784, 0.79054054, 0.83783784,\n",
       "        0.83108108, 0.84459459, 0.7972973 , 0.8277027 , 0.83783784,\n",
       "        0.83783784, 0.7972973 , 0.83445946, 0.84121622, 0.8277027 ,\n",
       "        0.76013514, 0.84797297, 0.84459459, 0.86824324, 0.78040541,\n",
       "        0.85135135, 0.84797297, 0.85472973, 0.77027027, 0.83783784,\n",
       "        0.84121622, 0.84797297, 0.77702703, 0.83445946, 0.83783784,\n",
       "        0.84797297, 0.78040541, 0.82432432, 0.84459459, 0.85135135,\n",
       "        0.77702703, 0.83108108, 0.84459459, 0.84459459, 0.78716216,\n",
       "        0.82432432, 0.84121622, 0.84459459, 0.75      , 0.8277027 ,\n",
       "        0.83445946, 0.84121622, 0.77364865, 0.81081081, 0.84459459,\n",
       "        0.84459459, 0.78040541, 0.8277027 , 0.83783784, 0.83783784,\n",
       "        0.79054054, 0.82094595, 0.83783784, 0.84121622, 0.78040541,\n",
       "        0.83445946, 0.83108108, 0.84459459, 0.77364865, 0.84121622,\n",
       "        0.83783784, 0.84797297, 0.78040541, 0.82432432, 0.84459459,\n",
       "        0.84459459, 0.79054054, 0.82432432, 0.84121622, 0.84121622,\n",
       "        0.78040541, 0.82432432, 0.83108108, 0.83445946, 0.77702703,\n",
       "        0.80743243, 0.80405405, 0.78716216, 0.80067568, 0.8277027 ,\n",
       "        0.84459459, 0.83108108, 0.79054054, 0.84797297, 0.85810811,\n",
       "        0.86148649, 0.80067568, 0.83108108, 0.85810811, 0.85472973,\n",
       "        0.77364865, 0.82094595, 0.84459459, 0.84459459, 0.80743243,\n",
       "        0.83783784, 0.84797297, 0.85472973, 0.7972973 , 0.83445946,\n",
       "        0.85135135, 0.86148649, 0.79054054, 0.84797297, 0.85810811,\n",
       "        0.86824324, 0.78040541, 0.8277027 , 0.83445946, 0.84797297,\n",
       "        0.80743243, 0.83108108, 0.85810811, 0.85472973, 0.79054054,\n",
       "        0.84121622, 0.85472973, 0.86824324, 0.80405405, 0.84459459,\n",
       "        0.84459459, 0.85472973, 0.78378378, 0.83108108, 0.84121622,\n",
       "        0.84459459, 0.80405405, 0.83445946, 0.84797297, 0.85810811,\n",
       "        0.79054054, 0.83783784, 0.85135135, 0.86824324, 0.80067568,\n",
       "        0.84121622, 0.84121622, 0.85135135, 0.76689189, 0.81081081,\n",
       "        0.81418919, 0.81081081, 0.81081081, 0.83783784, 0.85472973,\n",
       "        0.86824324, 0.80743243, 0.84121622, 0.85135135, 0.85135135,\n",
       "        0.80743243, 0.85135135, 0.84459459, 0.85472973, 0.77702703,\n",
       "        0.82432432, 0.83783784, 0.85810811, 0.79054054, 0.84459459,\n",
       "        0.85135135, 0.86148649, 0.80743243, 0.84459459, 0.86148649,\n",
       "        0.86486486, 0.78716216, 0.83445946, 0.84459459, 0.85472973,\n",
       "        0.77027027, 0.83445946, 0.84459459, 0.85472973, 0.80743243,\n",
       "        0.84459459, 0.85472973, 0.86148649, 0.80405405, 0.84121622,\n",
       "        0.85472973, 0.85472973, 0.79391892, 0.84121622, 0.84459459,\n",
       "        0.84459459, 0.76351351, 0.82432432, 0.84121622, 0.85135135,\n",
       "        0.81418919, 0.84459459, 0.84459459, 0.85810811, 0.80067568,\n",
       "        0.84121622, 0.83783784, 0.84459459, 0.79054054, 0.84121622,\n",
       "        0.83783784, 0.84459459, 0.80067568, 0.83445946, 0.84459459,\n",
       "        0.86486486, 0.79391892, 0.8277027 , 0.86148649, 0.86824324,\n",
       "        0.78378378, 0.84459459, 0.85135135, 0.83783784, 0.77027027,\n",
       "        0.83108108, 0.84121622, 0.84797297, 0.78716216, 0.83445946,\n",
       "        0.84797297, 0.86148649, 0.76689189, 0.83108108, 0.84121622,\n",
       "        0.85135135, 0.78378378, 0.83445946, 0.84121622, 0.84459459,\n",
       "        0.78716216, 0.83108108, 0.85135135, 0.84121622, 0.80067568,\n",
       "        0.81756757, 0.84121622, 0.84797297, 0.78716216, 0.8277027 ,\n",
       "        0.83445946, 0.84121622, 0.78378378, 0.8277027 , 0.83445946,\n",
       "        0.84459459, 0.75675676, 0.83783784, 0.84121622, 0.84459459,\n",
       "        0.80067568, 0.83445946, 0.83445946, 0.84459459, 0.78716216,\n",
       "        0.82432432, 0.83108108, 0.84121622, 0.78378378, 0.8277027 ,\n",
       "        0.82094595, 0.84121622, 0.75675676, 0.84459459, 0.82432432,\n",
       "        0.84459459, 0.78716216, 0.80405405, 0.80405405, 0.79054054,\n",
       "        0.80405405, 0.83108108, 0.85135135, 0.82432432, 0.7972973 ,\n",
       "        0.83108108, 0.84797297, 0.85135135, 0.7972973 , 0.84797297,\n",
       "        0.84459459, 0.84797297, 0.78040541, 0.81418919, 0.83108108,\n",
       "        0.83783784, 0.80743243, 0.82432432, 0.86148649, 0.86824324,\n",
       "        0.79391892, 0.84459459, 0.85472973, 0.87837838, 0.79054054,\n",
       "        0.84797297, 0.85472973, 0.86486486, 0.77702703, 0.81081081,\n",
       "        0.85472973, 0.86148649, 0.80067568, 0.84121622, 0.85135135,\n",
       "        0.87837838, 0.78378378, 0.84459459, 0.84459459, 0.87162162,\n",
       "        0.79054054, 0.84121622, 0.84121622, 0.86148649, 0.78040541,\n",
       "        0.8277027 , 0.85810811, 0.86148649, 0.80067568, 0.83783784,\n",
       "        0.86148649, 0.87837838, 0.77702703, 0.83445946, 0.84797297,\n",
       "        0.87162162, 0.78716216, 0.83445946, 0.84121622, 0.85472973,\n",
       "        0.77364865, 0.80067568, 0.83445946, 0.80743243, 0.7972973 ,\n",
       "        0.83108108, 0.86148649, 0.85810811, 0.7972973 , 0.84121622,\n",
       "        0.85135135, 0.85135135, 0.79054054, 0.84459459, 0.85135135,\n",
       "        0.85135135, 0.77702703, 0.83783784, 0.85135135, 0.86486486,\n",
       "        0.80405405, 0.84459459, 0.85472973, 0.85810811, 0.79391892,\n",
       "        0.84459459, 0.84797297, 0.86486486, 0.76689189, 0.83445946,\n",
       "        0.84121622, 0.85472973, 0.78040541, 0.84459459, 0.84797297,\n",
       "        0.86148649, 0.80405405, 0.83783784, 0.85135135, 0.87162162,\n",
       "        0.79391892, 0.84459459, 0.84797297, 0.86148649, 0.79054054,\n",
       "        0.83445946, 0.83783784, 0.85472973, 0.77364865, 0.82094595,\n",
       "        0.84797297, 0.875     , 0.80743243, 0.83783784, 0.84459459,\n",
       "        0.86486486, 0.79391892, 0.8277027 , 0.84121622, 0.84797297,\n",
       "        0.77702703, 0.82094595, 0.84121622, 0.84459459, 0.77027027,\n",
       "        0.82094595, 0.85135135, 0.85810811, 0.79391892, 0.84797297,\n",
       "        0.85135135, 0.86148649, 0.7972973 , 0.83783784, 0.84459459,\n",
       "        0.85810811, 0.76013514, 0.84121622, 0.8277027 , 0.84797297,\n",
       "        0.77702703, 0.84121622, 0.83108108, 0.86148649, 0.75337838,\n",
       "        0.8277027 , 0.82432432, 0.83783784, 0.73986486, 0.82094595,\n",
       "        0.83108108, 0.83783784, 0.76351351, 0.82432432, 0.82094595,\n",
       "        0.84121622, 0.75675676, 0.83445946, 0.84459459, 0.85472973,\n",
       "        0.76351351, 0.82094595, 0.83783784, 0.86148649, 0.77364865,\n",
       "        0.82432432, 0.83445946, 0.85472973, 0.75337838, 0.82432432,\n",
       "        0.83108108, 0.86148649, 0.75675676, 0.80405405, 0.83108108,\n",
       "        0.84797297, 0.76351351, 0.80067568, 0.81081081, 0.84797297,\n",
       "        0.77364865, 0.80743243, 0.81418919, 0.84459459, 0.75337838,\n",
       "        0.81418919, 0.80405405, 0.84459459]),\n",
       " 'split3_test_score': array([0.61824324, 0.65540541, 0.66891892, 0.67567568, 0.64527027,\n",
       "        0.70945946, 0.72297297, 0.71283784, 0.70945946, 0.76351351,\n",
       "        0.75675676, 0.73310811, 0.77027027, 0.80405405, 0.7972973 ,\n",
       "        0.80405405, 0.61824324, 0.66891892, 0.68243243, 0.6722973 ,\n",
       "        0.66554054, 0.72635135, 0.73648649, 0.73310811, 0.73648649,\n",
       "        0.75      , 0.74324324, 0.75337838, 0.76351351, 0.80405405,\n",
       "        0.79391892, 0.79054054, 0.61824324, 0.65878378, 0.66216216,\n",
       "        0.66554054, 0.66554054, 0.72635135, 0.71959459, 0.72297297,\n",
       "        0.74324324, 0.74662162, 0.73986486, 0.73648649, 0.76351351,\n",
       "        0.78716216, 0.77364865, 0.77364865, 0.61824324, 0.65878378,\n",
       "        0.66554054, 0.66891892, 0.66554054, 0.72635135, 0.71283784,\n",
       "        0.72297297, 0.74324324, 0.74662162, 0.73310811, 0.73310811,\n",
       "        0.76351351, 0.78716216, 0.78040541, 0.77364865, 0.68581081,\n",
       "        0.67905405, 0.68243243, 0.70608108, 0.74662162, 0.79054054,\n",
       "        0.78716216, 0.78378378, 0.75337838, 0.79391892, 0.80405405,\n",
       "        0.80067568, 0.77364865, 0.80405405, 0.80405405, 0.81756757,\n",
       "        0.69256757, 0.69932432, 0.7027027 , 0.71621622, 0.75337838,\n",
       "        0.77364865, 0.76351351, 0.75337838, 0.76013514, 0.78040541,\n",
       "        0.7972973 , 0.78716216, 0.77702703, 0.79054054, 0.7972973 ,\n",
       "        0.7972973 , 0.69256757, 0.70945946, 0.7027027 , 0.69932432,\n",
       "        0.75337838, 0.77027027, 0.74662162, 0.73986486, 0.75337838,\n",
       "        0.78716216, 0.77702703, 0.78040541, 0.78378378, 0.79054054,\n",
       "        0.78378378, 0.79391892, 0.69256757, 0.72297297, 0.7027027 ,\n",
       "        0.7027027 , 0.75337838, 0.77364865, 0.75675676, 0.73648649,\n",
       "        0.75337838, 0.78716216, 0.78378378, 0.76689189, 0.78378378,\n",
       "        0.79054054, 0.79054054, 0.78716216, 0.74324324, 0.78040541,\n",
       "        0.78040541, 0.80067568, 0.76013514, 0.79391892, 0.81081081,\n",
       "        0.8277027 , 0.78040541, 0.77702703, 0.81418919, 0.82094595,\n",
       "        0.76351351, 0.79391892, 0.80743243, 0.81756757, 0.75337838,\n",
       "        0.76351351, 0.76351351, 0.78378378, 0.75337838, 0.78378378,\n",
       "        0.7972973 , 0.79054054, 0.74324324, 0.77027027, 0.80067568,\n",
       "        0.79054054, 0.76013514, 0.77027027, 0.80067568, 0.80067568,\n",
       "        0.75337838, 0.75675676, 0.76351351, 0.75      , 0.75675676,\n",
       "        0.76689189, 0.78716216, 0.77364865, 0.76689189, 0.78040541,\n",
       "        0.78378378, 0.78716216, 0.76689189, 0.79054054, 0.7972973 ,\n",
       "        0.78716216, 0.75337838, 0.76013514, 0.77702703, 0.74662162,\n",
       "        0.75675676, 0.75675676, 0.79391892, 0.77702703, 0.76689189,\n",
       "        0.77027027, 0.79054054, 0.78716216, 0.76689189, 0.76689189,\n",
       "        0.80743243, 0.78040541, 0.72297297, 0.75675676, 0.76013514,\n",
       "        0.75337838, 0.76689189, 0.78378378, 0.79391892, 0.78716216,\n",
       "        0.76013514, 0.79391892, 0.80405405, 0.80743243, 0.78378378,\n",
       "        0.80067568, 0.8277027 , 0.82432432, 0.75      , 0.79054054,\n",
       "        0.80743243, 0.7972973 , 0.75675676, 0.80743243, 0.81081081,\n",
       "        0.7972973 , 0.77364865, 0.81081081, 0.82432432, 0.80405405,\n",
       "        0.77702703, 0.81756757, 0.83445946, 0.80405405, 0.75      ,\n",
       "        0.80743243, 0.79391892, 0.78378378, 0.77364865, 0.80743243,\n",
       "        0.81418919, 0.79391892, 0.77364865, 0.80743243, 0.81756757,\n",
       "        0.80067568, 0.78716216, 0.80743243, 0.82432432, 0.81418919,\n",
       "        0.75      , 0.79054054, 0.79054054, 0.76013514, 0.77027027,\n",
       "        0.82094595, 0.81418919, 0.78378378, 0.77364865, 0.81081081,\n",
       "        0.82094595, 0.79391892, 0.78040541, 0.80405405, 0.81418919,\n",
       "        0.81418919, 0.75      , 0.76013514, 0.78378378, 0.77364865,\n",
       "        0.78040541, 0.79054054, 0.81081081, 0.81756757, 0.79054054,\n",
       "        0.80405405, 0.83445946, 0.8277027 , 0.78040541, 0.80405405,\n",
       "        0.8277027 , 0.81756757, 0.76013514, 0.79054054, 0.81081081,\n",
       "        0.80067568, 0.76351351, 0.81418919, 0.81756757, 0.81756757,\n",
       "        0.77027027, 0.81756757, 0.8277027 , 0.82094595, 0.77364865,\n",
       "        0.81756757, 0.81756757, 0.80405405, 0.74324324, 0.81081081,\n",
       "        0.80067568, 0.80067568, 0.77027027, 0.80743243, 0.82094595,\n",
       "        0.80743243, 0.77027027, 0.81418919, 0.81081081, 0.80405405,\n",
       "        0.78378378, 0.80743243, 0.81418919, 0.80405405, 0.74324324,\n",
       "        0.81756757, 0.81081081, 0.79391892, 0.77027027, 0.81418919,\n",
       "        0.8277027 , 0.79391892, 0.77027027, 0.81081081, 0.81756757,\n",
       "        0.80067568, 0.78378378, 0.81418919, 0.81756757, 0.80743243,\n",
       "        0.75      , 0.80405405, 0.80405405, 0.81418919, 0.73648649,\n",
       "        0.80405405, 0.82432432, 0.81081081, 0.76351351, 0.81756757,\n",
       "        0.82432432, 0.81756757, 0.75337838, 0.82432432, 0.81756757,\n",
       "        0.7972973 , 0.75675676, 0.82094595, 0.81756757, 0.81418919,\n",
       "        0.76013514, 0.80405405, 0.81756757, 0.81756757, 0.76013514,\n",
       "        0.82094595, 0.81756757, 0.82094595, 0.76689189, 0.79391892,\n",
       "        0.81418919, 0.80405405, 0.73648649, 0.80405405, 0.80405405,\n",
       "        0.80743243, 0.74324324, 0.79054054, 0.80067568, 0.80743243,\n",
       "        0.75      , 0.7972973 , 0.79054054, 0.80405405, 0.77027027,\n",
       "        0.79054054, 0.79054054, 0.80067568, 0.73648649, 0.80743243,\n",
       "        0.81081081, 0.80743243, 0.74324324, 0.80405405, 0.81756757,\n",
       "        0.80067568, 0.75      , 0.80405405, 0.80743243, 0.80067568,\n",
       "        0.77027027, 0.7972973 , 0.81418919, 0.80405405, 0.73648649,\n",
       "        0.76013514, 0.76351351, 0.73648649, 0.76351351, 0.79391892,\n",
       "        0.80743243, 0.78040541, 0.78378378, 0.80405405, 0.80743243,\n",
       "        0.80067568, 0.79054054, 0.81081081, 0.82432432, 0.81418919,\n",
       "        0.73986486, 0.77702703, 0.79054054, 0.78040541, 0.77027027,\n",
       "        0.81081081, 0.81081081, 0.81756757, 0.77702703, 0.80743243,\n",
       "        0.81418919, 0.81081081, 0.79391892, 0.81418919, 0.80743243,\n",
       "        0.81418919, 0.74324324, 0.80067568, 0.80067568, 0.80743243,\n",
       "        0.78716216, 0.81418919, 0.82094595, 0.81418919, 0.78378378,\n",
       "        0.81081081, 0.81418919, 0.81756757, 0.79054054, 0.82432432,\n",
       "        0.81756757, 0.80743243, 0.74324324, 0.79391892, 0.80743243,\n",
       "        0.81081081, 0.78040541, 0.80743243, 0.81418919, 0.82094595,\n",
       "        0.78716216, 0.81756757, 0.81756757, 0.81418919, 0.78378378,\n",
       "        0.81756757, 0.81418919, 0.81081081, 0.75      , 0.76689189,\n",
       "        0.78040541, 0.76351351, 0.77364865, 0.81081081, 0.81081081,\n",
       "        0.82094595, 0.79054054, 0.81081081, 0.81418919, 0.82094595,\n",
       "        0.81081081, 0.81418919, 0.81418919, 0.83108108, 0.74662162,\n",
       "        0.7972973 , 0.7972973 , 0.81418919, 0.77364865, 0.81081081,\n",
       "        0.81418919, 0.81756757, 0.78040541, 0.82094595, 0.81081081,\n",
       "        0.81756757, 0.80743243, 0.82432432, 0.80743243, 0.81081081,\n",
       "        0.73986486, 0.80405405, 0.81081081, 0.81081081, 0.78040541,\n",
       "        0.80743243, 0.80743243, 0.81081081, 0.78040541, 0.80067568,\n",
       "        0.80067568, 0.80405405, 0.78040541, 0.80743243, 0.81418919,\n",
       "        0.80405405, 0.74324324, 0.80405405, 0.81756757, 0.81081081,\n",
       "        0.76689189, 0.80405405, 0.82094595, 0.81418919, 0.77702703,\n",
       "        0.81418919, 0.81081081, 0.80743243, 0.78040541, 0.81418919,\n",
       "        0.82094595, 0.81081081, 0.78040541, 0.81081081, 0.82094595,\n",
       "        0.81418919, 0.77364865, 0.81756757, 0.82094595, 0.82432432,\n",
       "        0.75      , 0.81418919, 0.81418919, 0.81756757, 0.77702703,\n",
       "        0.80067568, 0.82432432, 0.80067568, 0.76351351, 0.80067568,\n",
       "        0.81756757, 0.81756757, 0.75675676, 0.80067568, 0.81081081,\n",
       "        0.81756757, 0.78378378, 0.7972973 , 0.81756757, 0.81418919,\n",
       "        0.76689189, 0.78716216, 0.81418919, 0.81081081, 0.74662162,\n",
       "        0.80743243, 0.81756757, 0.81756757, 0.75      , 0.7972973 ,\n",
       "        0.81756757, 0.80405405, 0.76013514, 0.79391892, 0.82432432,\n",
       "        0.81081081, 0.7972973 , 0.78040541, 0.82094595, 0.81081081,\n",
       "        0.73310811, 0.80743243, 0.81418919, 0.81081081, 0.74324324,\n",
       "        0.79054054, 0.82432432, 0.81418919, 0.75337838, 0.80067568,\n",
       "        0.81418919, 0.81081081, 0.75      , 0.78040541, 0.81081081,\n",
       "        0.81418919, 0.71959459, 0.76689189, 0.76351351, 0.75675676,\n",
       "        0.76351351, 0.80405405, 0.81081081, 0.78040541, 0.77702703,\n",
       "        0.81418919, 0.81756757, 0.7972973 , 0.79391892, 0.82094595,\n",
       "        0.81081081, 0.81081081, 0.73986486, 0.78716216, 0.79391892,\n",
       "        0.78378378, 0.77364865, 0.81081081, 0.81418919, 0.81756757,\n",
       "        0.78040541, 0.81418919, 0.81756757, 0.81756757, 0.81081081,\n",
       "        0.81756757, 0.81081081, 0.80743243, 0.74662162, 0.81418919,\n",
       "        0.81756757, 0.81756757, 0.77027027, 0.80743243, 0.82094595,\n",
       "        0.82094595, 0.77702703, 0.81081081, 0.8277027 , 0.81418919,\n",
       "        0.7972973 , 0.82094595, 0.82432432, 0.81418919, 0.75      ,\n",
       "        0.81081081, 0.82432432, 0.81418919, 0.77027027, 0.81418919,\n",
       "        0.82432432, 0.81418919, 0.78040541, 0.81756757, 0.82094595,\n",
       "        0.81756757, 0.7972973 , 0.81756757, 0.81756757, 0.81756757,\n",
       "        0.73310811, 0.77702703, 0.78716216, 0.76689189, 0.77702703,\n",
       "        0.80405405, 0.82432432, 0.80067568, 0.7972973 , 0.81756757,\n",
       "        0.81081081, 0.81418919, 0.81418919, 0.82094595, 0.81418919,\n",
       "        0.82432432, 0.78040541, 0.80743243, 0.81081081, 0.81418919,\n",
       "        0.78378378, 0.83108108, 0.81756757, 0.82432432, 0.80067568,\n",
       "        0.82094595, 0.80743243, 0.81418919, 0.7972973 , 0.81418919,\n",
       "        0.81756757, 0.80743243, 0.77702703, 0.81418919, 0.8277027 ,\n",
       "        0.83108108, 0.78716216, 0.81756757, 0.81418919, 0.81756757,\n",
       "        0.80067568, 0.82432432, 0.81756757, 0.80405405, 0.79054054,\n",
       "        0.8277027 , 0.8277027 , 0.81081081, 0.77702703, 0.81418919,\n",
       "        0.82094595, 0.82094595, 0.77364865, 0.8277027 , 0.8277027 ,\n",
       "        0.81081081, 0.79391892, 0.83783784, 0.83108108, 0.81081081,\n",
       "        0.78040541, 0.81081081, 0.83445946, 0.80743243, 0.78378378,\n",
       "        0.80743243, 0.82432432, 0.82094595, 0.80743243, 0.84121622,\n",
       "        0.7972973 , 0.8277027 , 0.78716216, 0.8277027 , 0.7972973 ,\n",
       "        0.82094595, 0.78378378, 0.80743243, 0.81081081, 0.81081081,\n",
       "        0.73310811, 0.81756757, 0.81081081, 0.82432432, 0.75      ,\n",
       "        0.80067568, 0.80405405, 0.81756757, 0.73310811, 0.80067568,\n",
       "        0.81756757, 0.81081081, 0.73986486, 0.80743243, 0.81418919,\n",
       "        0.81081081, 0.75      , 0.80743243, 0.80067568, 0.80067568,\n",
       "        0.73986486, 0.79391892, 0.7972973 , 0.80743243, 0.75675676,\n",
       "        0.80067568, 0.80067568, 0.80743243, 0.77364865, 0.80405405,\n",
       "        0.80067568, 0.81756757, 0.75      , 0.82432432, 0.8277027 ,\n",
       "        0.80405405, 0.73986486, 0.82432432, 0.81756757, 0.80743243,\n",
       "        0.75675676, 0.80067568, 0.82432432, 0.80743243, 0.77364865,\n",
       "        0.7972973 , 0.81418919, 0.80405405]),\n",
       " 'split4_test_score': array([0.66554054, 0.67567568, 0.7027027 , 0.68918919, 0.69256757,\n",
       "        0.73648649, 0.76013514, 0.75675676, 0.75      , 0.80743243,\n",
       "        0.78378378, 0.80405405, 0.78716216, 0.81418919, 0.81418919,\n",
       "        0.83108108, 0.65878378, 0.69256757, 0.70945946, 0.7027027 ,\n",
       "        0.69932432, 0.75337838, 0.76689189, 0.76351351, 0.78378378,\n",
       "        0.77702703, 0.77702703, 0.78716216, 0.79391892, 0.7972973 ,\n",
       "        0.80743243, 0.79054054, 0.65878378, 0.69256757, 0.69932432,\n",
       "        0.69594595, 0.69594595, 0.74324324, 0.77364865, 0.77364865,\n",
       "        0.78378378, 0.77702703, 0.77702703, 0.76351351, 0.79054054,\n",
       "        0.7972973 , 0.79054054, 0.76689189, 0.65878378, 0.68243243,\n",
       "        0.69594595, 0.69256757, 0.69594595, 0.74324324, 0.78040541,\n",
       "        0.76013514, 0.78378378, 0.77702703, 0.77702703, 0.75      ,\n",
       "        0.79054054, 0.78040541, 0.78716216, 0.75675676, 0.69256757,\n",
       "        0.71959459, 0.73310811, 0.72635135, 0.75675676, 0.81081081,\n",
       "        0.80067568, 0.82432432, 0.77364865, 0.82094595, 0.82094595,\n",
       "        0.83783784, 0.78040541, 0.83108108, 0.84121622, 0.83445946,\n",
       "        0.7027027 , 0.72635135, 0.74662162, 0.75675676, 0.76351351,\n",
       "        0.77702703, 0.80067568, 0.79054054, 0.79054054, 0.80405405,\n",
       "        0.80743243, 0.78378378, 0.78040541, 0.80405405, 0.7972973 ,\n",
       "        0.81418919, 0.7027027 , 0.71959459, 0.74662162, 0.74324324,\n",
       "        0.75675676, 0.78716216, 0.79054054, 0.76351351, 0.79391892,\n",
       "        0.80067568, 0.78716216, 0.77364865, 0.78378378, 0.7972973 ,\n",
       "        0.79391892, 0.78040541, 0.7027027 , 0.71959459, 0.75337838,\n",
       "        0.74662162, 0.75675676, 0.78040541, 0.78716216, 0.76689189,\n",
       "        0.79391892, 0.80405405, 0.79054054, 0.76013514, 0.78378378,\n",
       "        0.7972973 , 0.80067568, 0.78378378, 0.73310811, 0.81081081,\n",
       "        0.81418919, 0.83108108, 0.76351351, 0.83108108, 0.84121622,\n",
       "        0.84121622, 0.76689189, 0.83783784, 0.83445946, 0.84797297,\n",
       "        0.78378378, 0.82094595, 0.82432432, 0.85135135, 0.73310811,\n",
       "        0.79391892, 0.81418919, 0.80067568, 0.76351351, 0.7972973 ,\n",
       "        0.81756757, 0.82094595, 0.75337838, 0.81756757, 0.82432432,\n",
       "        0.82432432, 0.76013514, 0.81756757, 0.82432432, 0.82094595,\n",
       "        0.73310811, 0.79054054, 0.80405405, 0.76689189, 0.77702703,\n",
       "        0.80067568, 0.82432432, 0.7972973 , 0.77364865, 0.82094595,\n",
       "        0.80743243, 0.7972973 , 0.77027027, 0.79391892, 0.7972973 ,\n",
       "        0.79054054, 0.73310811, 0.78716216, 0.78040541, 0.76689189,\n",
       "        0.77702703, 0.80743243, 0.80743243, 0.7972973 , 0.77364865,\n",
       "        0.81418919, 0.81081081, 0.79391892, 0.77027027, 0.79054054,\n",
       "        0.80067568, 0.79054054, 0.77027027, 0.80743243, 0.79054054,\n",
       "        0.79391892, 0.78040541, 0.80067568, 0.82432432, 0.81418919,\n",
       "        0.80743243, 0.8277027 , 0.83108108, 0.84121622, 0.80067568,\n",
       "        0.8277027 , 0.85135135, 0.86486486, 0.77702703, 0.80405405,\n",
       "        0.7972973 , 0.81081081, 0.79054054, 0.81418919, 0.83108108,\n",
       "        0.84121622, 0.7972973 , 0.8277027 , 0.83445946, 0.84121622,\n",
       "        0.79391892, 0.82432432, 0.84121622, 0.85472973, 0.76013514,\n",
       "        0.80067568, 0.78716216, 0.80067568, 0.79391892, 0.80743243,\n",
       "        0.81418919, 0.81418919, 0.78378378, 0.81418919, 0.82432432,\n",
       "        0.8277027 , 0.7972973 , 0.82094595, 0.82094595, 0.83108108,\n",
       "        0.76013514, 0.78040541, 0.77702703, 0.78716216, 0.79391892,\n",
       "        0.80405405, 0.82432432, 0.81081081, 0.78378378, 0.81756757,\n",
       "        0.82432432, 0.83108108, 0.7972973 , 0.82094595, 0.82094595,\n",
       "        0.83108108, 0.77364865, 0.80405405, 0.80405405, 0.80405405,\n",
       "        0.79391892, 0.84459459, 0.83783784, 0.85135135, 0.80067568,\n",
       "        0.83783784, 0.85135135, 0.86486486, 0.81081081, 0.84121622,\n",
       "        0.84797297, 0.85810811, 0.78378378, 0.82432432, 0.81756757,\n",
       "        0.82094595, 0.77364865, 0.82094595, 0.83445946, 0.85135135,\n",
       "        0.77364865, 0.8277027 , 0.84121622, 0.85135135, 0.81081081,\n",
       "        0.82432432, 0.83445946, 0.85135135, 0.76689189, 0.81081081,\n",
       "        0.81418919, 0.80405405, 0.79391892, 0.81081081, 0.8277027 ,\n",
       "        0.83108108, 0.7972973 , 0.8277027 , 0.8277027 , 0.83783784,\n",
       "        0.80405405, 0.81081081, 0.81756757, 0.83783784, 0.76689189,\n",
       "        0.80405405, 0.81418919, 0.80405405, 0.79391892, 0.78716216,\n",
       "        0.82432432, 0.83445946, 0.7972973 , 0.82432432, 0.81756757,\n",
       "        0.83445946, 0.80405405, 0.81081081, 0.81418919, 0.82094595,\n",
       "        0.75      , 0.7972973 , 0.84121622, 0.85135135, 0.76013514,\n",
       "        0.79391892, 0.84797297, 0.85472973, 0.77027027, 0.80405405,\n",
       "        0.84459459, 0.85810811, 0.76689189, 0.81418919, 0.83783784,\n",
       "        0.86824324, 0.73648649, 0.81756757, 0.81418919, 0.84797297,\n",
       "        0.76351351, 0.80405405, 0.81418919, 0.84121622, 0.76689189,\n",
       "        0.80405405, 0.81081081, 0.83445946, 0.77702703, 0.80405405,\n",
       "        0.80067568, 0.8277027 , 0.74662162, 0.80405405, 0.80743243,\n",
       "        0.83445946, 0.77364865, 0.80405405, 0.80405405, 0.82094595,\n",
       "        0.78716216, 0.80743243, 0.80067568, 0.81418919, 0.78378378,\n",
       "        0.7972973 , 0.79391892, 0.80067568, 0.74662162, 0.79054054,\n",
       "        0.81756757, 0.83445946, 0.77364865, 0.7972973 , 0.80405405,\n",
       "        0.82094595, 0.78716216, 0.81418919, 0.80405405, 0.81756757,\n",
       "        0.78378378, 0.80405405, 0.78716216, 0.81418919, 0.77027027,\n",
       "        0.79391892, 0.78716216, 0.78716216, 0.7972973 , 0.80405405,\n",
       "        0.81756757, 0.82432432, 0.81081081, 0.82432432, 0.83445946,\n",
       "        0.83783784, 0.7972973 , 0.83445946, 0.84797297, 0.85810811,\n",
       "        0.76351351, 0.77702703, 0.80067568, 0.80743243, 0.78040541,\n",
       "        0.80405405, 0.83783784, 0.8277027 , 0.78378378, 0.82094595,\n",
       "        0.85135135, 0.83445946, 0.79054054, 0.8277027 , 0.85135135,\n",
       "        0.85472973, 0.77364865, 0.80743243, 0.80405405, 0.81756757,\n",
       "        0.79054054, 0.81081081, 0.83445946, 0.83445946, 0.78716216,\n",
       "        0.82094595, 0.84797297, 0.84797297, 0.7972973 , 0.81418919,\n",
       "        0.83445946, 0.83783784, 0.76013514, 0.81756757, 0.81756757,\n",
       "        0.81756757, 0.78716216, 0.81756757, 0.82432432, 0.83445946,\n",
       "        0.79054054, 0.82432432, 0.83445946, 0.83445946, 0.79391892,\n",
       "        0.81756757, 0.83108108, 0.83445946, 0.78378378, 0.79054054,\n",
       "        0.80067568, 0.81081081, 0.80067568, 0.81418919, 0.83783784,\n",
       "        0.85810811, 0.79391892, 0.83108108, 0.85810811, 0.86824324,\n",
       "        0.80743243, 0.83108108, 0.85472973, 0.86486486, 0.78716216,\n",
       "        0.81418919, 0.8277027 , 0.81756757, 0.79054054, 0.81756757,\n",
       "        0.84121622, 0.83783784, 0.78378378, 0.83108108, 0.84459459,\n",
       "        0.85135135, 0.80067568, 0.81756757, 0.83783784, 0.84797297,\n",
       "        0.78040541, 0.81418919, 0.82432432, 0.83108108, 0.80067568,\n",
       "        0.81756757, 0.83445946, 0.83108108, 0.78378378, 0.81756757,\n",
       "        0.83108108, 0.83108108, 0.7972973 , 0.80405405, 0.8277027 ,\n",
       "        0.8277027 , 0.78040541, 0.82432432, 0.82094595, 0.84121622,\n",
       "        0.7972973 , 0.8277027 , 0.82432432, 0.83108108, 0.77364865,\n",
       "        0.82432432, 0.82432432, 0.8277027 , 0.80743243, 0.81418919,\n",
       "        0.81756757, 0.82432432, 0.77027027, 0.79391892, 0.84121622,\n",
       "        0.86824324, 0.76689189, 0.81756757, 0.8277027 , 0.85810811,\n",
       "        0.78716216, 0.82094595, 0.83445946, 0.85472973, 0.78040541,\n",
       "        0.82094595, 0.8277027 , 0.85472973, 0.74324324, 0.7972973 ,\n",
       "        0.8277027 , 0.84121622, 0.77702703, 0.78716216, 0.81081081,\n",
       "        0.83445946, 0.76013514, 0.80067568, 0.81418919, 0.8277027 ,\n",
       "        0.77702703, 0.80067568, 0.81418919, 0.82432432, 0.72297297,\n",
       "        0.78040541, 0.81756757, 0.83445946, 0.76351351, 0.78716216,\n",
       "        0.81418919, 0.81081081, 0.76013514, 0.78378378, 0.80067568,\n",
       "        0.81081081, 0.75      , 0.77364865, 0.80405405, 0.80405405,\n",
       "        0.77364865, 0.79391892, 0.80067568, 0.83783784, 0.74324324,\n",
       "        0.78378378, 0.80067568, 0.81081081, 0.74662162, 0.78716216,\n",
       "        0.79391892, 0.80743243, 0.77364865, 0.79391892, 0.7972973 ,\n",
       "        0.80405405, 0.77364865, 0.79391892, 0.79054054, 0.79054054,\n",
       "        0.80067568, 0.81756757, 0.83108108, 0.82432432, 0.80067568,\n",
       "        0.82432432, 0.8277027 , 0.85135135, 0.80067568, 0.82094595,\n",
       "        0.83783784, 0.86148649, 0.80405405, 0.81081081, 0.81756757,\n",
       "        0.82094595, 0.81081081, 0.8277027 , 0.84459459, 0.83445946,\n",
       "        0.79391892, 0.8277027 , 0.84121622, 0.84459459, 0.80405405,\n",
       "        0.81756757, 0.83783784, 0.85472973, 0.79054054, 0.82432432,\n",
       "        0.82432432, 0.83108108, 0.81081081, 0.83108108, 0.83783784,\n",
       "        0.84797297, 0.81081081, 0.82094595, 0.83445946, 0.85135135,\n",
       "        0.80067568, 0.81081081, 0.82094595, 0.8277027 , 0.80067568,\n",
       "        0.82432432, 0.83445946, 0.81756757, 0.80067568, 0.8277027 ,\n",
       "        0.83108108, 0.84459459, 0.80743243, 0.81756757, 0.83445946,\n",
       "        0.84797297, 0.7972973 , 0.81418919, 0.82094595, 0.83108108,\n",
       "        0.78378378, 0.80067568, 0.80067568, 0.81418919, 0.79054054,\n",
       "        0.82432432, 0.83783784, 0.84459459, 0.80743243, 0.83108108,\n",
       "        0.84797297, 0.85810811, 0.82094595, 0.83108108, 0.84797297,\n",
       "        0.86148649, 0.79391892, 0.81081081, 0.83445946, 0.83445946,\n",
       "        0.78716216, 0.83445946, 0.84121622, 0.85135135, 0.79391892,\n",
       "        0.83445946, 0.83445946, 0.85472973, 0.81418919, 0.82432432,\n",
       "        0.8277027 , 0.84459459, 0.77364865, 0.8277027 , 0.83783784,\n",
       "        0.84121622, 0.7972973 , 0.81756757, 0.8277027 , 0.84121622,\n",
       "        0.78040541, 0.81081081, 0.8277027 , 0.83108108, 0.80405405,\n",
       "        0.81081081, 0.82094595, 0.82094595, 0.78040541, 0.81756757,\n",
       "        0.84121622, 0.84459459, 0.76689189, 0.82432432, 0.82432432,\n",
       "        0.83445946, 0.78716216, 0.82432432, 0.82094595, 0.8277027 ,\n",
       "        0.78040541, 0.82094595, 0.81756757, 0.82432432, 0.78040541,\n",
       "        0.8277027 , 0.84121622, 0.86148649, 0.79054054, 0.82094595,\n",
       "        0.84459459, 0.86148649, 0.77027027, 0.81081081, 0.83445946,\n",
       "        0.86486486, 0.77027027, 0.81418919, 0.83445946, 0.86486486,\n",
       "        0.75      , 0.81418919, 0.82432432, 0.84121622, 0.77027027,\n",
       "        0.81418919, 0.80067568, 0.8277027 , 0.76689189, 0.81081081,\n",
       "        0.80743243, 0.82432432, 0.78040541, 0.80067568, 0.80405405,\n",
       "        0.81756757, 0.77364865, 0.80743243, 0.82094595, 0.82432432,\n",
       "        0.79054054, 0.80067568, 0.81756757, 0.81756757, 0.76689189,\n",
       "        0.80405405, 0.81418919, 0.82432432, 0.79391892, 0.79391892,\n",
       "        0.7972973 , 0.81756757, 0.77364865, 0.81081081, 0.81081081,\n",
       "        0.8277027 , 0.79054054, 0.81756757, 0.81081081, 0.83108108,\n",
       "        0.76689189, 0.81418919, 0.81081081, 0.8277027 , 0.79391892,\n",
       "        0.79054054, 0.7972973 , 0.82432432]),\n",
       " 'mean_test_score': array([0.6472973 , 0.68040541, 0.69391892, 0.69864865, 0.69121622,\n",
       "        0.74054054, 0.75743243, 0.76081081, 0.74527027, 0.79594595,\n",
       "        0.78648649, 0.78716216, 0.77905405, 0.82297297, 0.82094595,\n",
       "        0.83716216, 0.64662162, 0.68243243, 0.7       , 0.70202703,\n",
       "        0.69932432, 0.7472973 , 0.76013514, 0.76418919, 0.76013514,\n",
       "        0.77432432, 0.77905405, 0.77837838, 0.77702703, 0.80675676,\n",
       "        0.81013514, 0.80405405, 0.64662162, 0.68378378, 0.69324324,\n",
       "        0.7       , 0.69864865, 0.74054054, 0.75202703, 0.75743243,\n",
       "        0.76216216, 0.775     , 0.77094595, 0.76486486, 0.77837838,\n",
       "        0.80067568, 0.79662162, 0.78581081, 0.64662162, 0.68108108,\n",
       "        0.68716216, 0.7       , 0.69864865, 0.74256757, 0.75337838,\n",
       "        0.75202703, 0.76216216, 0.77297297, 0.76756757, 0.75945946,\n",
       "        0.77837838, 0.7972973 , 0.79594595, 0.77635135, 0.69189189,\n",
       "        0.71418919, 0.73175676, 0.73716216, 0.75405405, 0.79864865,\n",
       "        0.81554054, 0.81891892, 0.76621622, 0.81418919, 0.82702703,\n",
       "        0.83783784, 0.77837838, 0.83108108, 0.83648649, 0.84054054,\n",
       "        0.70135135, 0.72162162, 0.74189189, 0.74594595, 0.75945946,\n",
       "        0.78783784, 0.79527027, 0.78175676, 0.76959459, 0.80202703,\n",
       "        0.80945946, 0.8027027 , 0.77972973, 0.80810811, 0.81486486,\n",
       "        0.80743243, 0.7027027 , 0.72432432, 0.73445946, 0.73851351,\n",
       "        0.75945946, 0.77837838, 0.7777027 , 0.76486486, 0.77162162,\n",
       "        0.79594595, 0.79459459, 0.78783784, 0.78243243, 0.80135135,\n",
       "        0.8027027 , 0.7972973 , 0.7027027 , 0.72567568, 0.73581081,\n",
       "        0.73986486, 0.75945946, 0.78175676, 0.7777027 , 0.76351351,\n",
       "        0.77162162, 0.79594595, 0.7972973 , 0.77905405, 0.78243243,\n",
       "        0.80405405, 0.80135135, 0.79527027, 0.74256757, 0.78783784,\n",
       "        0.80878378, 0.82905405, 0.75810811, 0.80810811, 0.83581081,\n",
       "        0.84662162, 0.77432432, 0.81283784, 0.83040541, 0.83918919,\n",
       "        0.76959459, 0.81824324, 0.82364865, 0.83378378, 0.73986486,\n",
       "        0.78783784, 0.8027027 , 0.80540541, 0.77162162, 0.80135135,\n",
       "        0.82364865, 0.81081081, 0.75540541, 0.8       , 0.82094595,\n",
       "        0.81283784, 0.76216216, 0.80337838, 0.8222973 , 0.81283784,\n",
       "        0.73918919, 0.77837838, 0.78648649, 0.7777027 , 0.76418919,\n",
       "        0.79391892, 0.80945946, 0.8027027 , 0.76621622, 0.80405405,\n",
       "        0.80608108, 0.80675676, 0.76689189, 0.8       , 0.8       ,\n",
       "        0.80743243, 0.73918919, 0.7722973 , 0.78513514, 0.7722973 ,\n",
       "        0.76418919, 0.78986486, 0.80878378, 0.8       , 0.76621622,\n",
       "        0.80135135, 0.80945946, 0.80337838, 0.76689189, 0.79594595,\n",
       "        0.81148649, 0.80337838, 0.75337838, 0.78513514, 0.78648649,\n",
       "        0.78918919, 0.77837838, 0.79527027, 0.81689189, 0.81418919,\n",
       "        0.78378378, 0.80675676, 0.82567568, 0.83783784, 0.79054054,\n",
       "        0.82027027, 0.84459459, 0.85067568, 0.75540541, 0.79797297,\n",
       "        0.80945946, 0.81756757, 0.78108108, 0.81013514, 0.83243243,\n",
       "        0.83918919, 0.78513514, 0.825     , 0.83986486, 0.84391892,\n",
       "        0.78648649, 0.82635135, 0.84256757, 0.84594595, 0.75337838,\n",
       "        0.80337838, 0.80472973, 0.80945946, 0.78783784, 0.81621622,\n",
       "        0.82702703, 0.82432432, 0.78581081, 0.8222973 , 0.83310811,\n",
       "        0.83040541, 0.79054054, 0.82027027, 0.83445946, 0.83378378,\n",
       "        0.75337838, 0.79594595, 0.8       , 0.79054054, 0.78716216,\n",
       "        0.81824324, 0.83310811, 0.82162162, 0.78378378, 0.825     ,\n",
       "        0.83513514, 0.83108108, 0.78783784, 0.82162162, 0.83040541,\n",
       "        0.83108108, 0.75743243, 0.78513514, 0.80337838, 0.80202703,\n",
       "        0.79054054, 0.81756757, 0.83783784, 0.84391892, 0.79121622,\n",
       "        0.825     , 0.84797297, 0.85135135, 0.79121622, 0.83040541,\n",
       "        0.84256757, 0.84864865, 0.76891892, 0.80810811, 0.82162162,\n",
       "        0.83310811, 0.77972973, 0.82297297, 0.83581081, 0.84864865,\n",
       "        0.77972973, 0.81959459, 0.83986486, 0.8472973 , 0.78851351,\n",
       "        0.81824324, 0.83243243, 0.83716216, 0.76689189, 0.81554054,\n",
       "        0.82432432, 0.82837838, 0.78783784, 0.81959459, 0.83108108,\n",
       "        0.83378378, 0.78513514, 0.82432432, 0.82702703, 0.83378378,\n",
       "        0.79121622, 0.81756757, 0.82162162, 0.82702703, 0.76689189,\n",
       "        0.80540541, 0.82432432, 0.82162162, 0.78648649, 0.81621622,\n",
       "        0.82837838, 0.82837838, 0.78581081, 0.8222973 , 0.82364865,\n",
       "        0.8277027 , 0.78783784, 0.81824324, 0.82162162, 0.81959459,\n",
       "        0.75878378, 0.81891892, 0.83918919, 0.84797297, 0.75743243,\n",
       "        0.81689189, 0.84054054, 0.84391892, 0.76081081, 0.81891892,\n",
       "        0.83783784, 0.84391892, 0.76418919, 0.825     , 0.82905405,\n",
       "        0.83986486, 0.76148649, 0.81621622, 0.83445946, 0.84256757,\n",
       "        0.75878378, 0.81216216, 0.82972973, 0.83648649, 0.75878378,\n",
       "        0.81216216, 0.82364865, 0.83378378, 0.75608108, 0.80810811,\n",
       "        0.81418919, 0.825     , 0.74459459, 0.80675676, 0.82297297,\n",
       "        0.83175676, 0.76216216, 0.80405405, 0.81689189, 0.82364865,\n",
       "        0.76081081, 0.80472973, 0.81148649, 0.82027027, 0.76283784,\n",
       "        0.80135135, 0.80608108, 0.81756757, 0.74459459, 0.81081081,\n",
       "        0.82567568, 0.83243243, 0.76216216, 0.80945946, 0.82702703,\n",
       "        0.82297297, 0.76081081, 0.80878378, 0.82094595, 0.82094595,\n",
       "        0.76283784, 0.80405405, 0.81148649, 0.81486486, 0.75337838,\n",
       "        0.78716216, 0.78986486, 0.7777027 , 0.78513514, 0.80405405,\n",
       "        0.81689189, 0.81486486, 0.79189189, 0.81689189, 0.82837838,\n",
       "        0.83445946, 0.79054054, 0.82364865, 0.84324324, 0.84662162,\n",
       "        0.75878378, 0.79324324, 0.80945946, 0.81418919, 0.78378378,\n",
       "        0.81891892, 0.83581081, 0.83716216, 0.78378378, 0.82364865,\n",
       "        0.84189189, 0.84189189, 0.78851351, 0.83040541, 0.84324324,\n",
       "        0.8472973 , 0.75878378, 0.81283784, 0.81824324, 0.83243243,\n",
       "        0.78918919, 0.81959459, 0.84121622, 0.84256757, 0.78378378,\n",
       "        0.82432432, 0.84256757, 0.84797297, 0.79189189, 0.8222973 ,\n",
       "        0.83445946, 0.83648649, 0.75743243, 0.81554054, 0.82432432,\n",
       "        0.82837838, 0.78716216, 0.82094595, 0.83243243, 0.84324324,\n",
       "        0.78581081, 0.8222973 , 0.83581081, 0.84256757, 0.78310811,\n",
       "        0.82364865, 0.83175676, 0.83445946, 0.76148649, 0.78986486,\n",
       "        0.80202703, 0.79797297, 0.79054054, 0.81891892, 0.83648649,\n",
       "        0.84594595, 0.78783784, 0.825     , 0.84324324, 0.85      ,\n",
       "        0.79189189, 0.83175676, 0.84054054, 0.85405405, 0.7722973 ,\n",
       "        0.81351351, 0.8222973 , 0.83581081, 0.78243243, 0.825     ,\n",
       "        0.84189189, 0.84256757, 0.78378378, 0.82972973, 0.84594595,\n",
       "        0.84527027, 0.78851351, 0.825     , 0.83243243, 0.84121622,\n",
       "        0.76216216, 0.81283784, 0.82972973, 0.83986486, 0.78378378,\n",
       "        0.82027027, 0.83581081, 0.83851351, 0.7777027 , 0.81891892,\n",
       "        0.83108108, 0.83378378, 0.78243243, 0.81621622, 0.82972973,\n",
       "        0.8277027 , 0.76148649, 0.81351351, 0.8277027 , 0.83918919,\n",
       "        0.78513514, 0.82297297, 0.83310811, 0.83918919, 0.77702703,\n",
       "        0.82094595, 0.82837838, 0.82972973, 0.78175676, 0.82027027,\n",
       "        0.8277027 , 0.82567568, 0.77027027, 0.81621622, 0.83851351,\n",
       "        0.85067568, 0.77364865, 0.81891892, 0.84189189, 0.84932432,\n",
       "        0.77162162, 0.82162162, 0.83445946, 0.84256757, 0.76689189,\n",
       "        0.81756757, 0.83310811, 0.83310811, 0.75878378, 0.81486486,\n",
       "        0.83378378, 0.84527027, 0.76148649, 0.81216216, 0.82364865,\n",
       "        0.83783784, 0.75810811, 0.81689189, 0.82635135, 0.83108108,\n",
       "        0.75878378, 0.80608108, 0.82567568, 0.82567568, 0.75337838,\n",
       "        0.80810811, 0.82567568, 0.83108108, 0.7527027 , 0.80540541,\n",
       "        0.81891892, 0.81824324, 0.75675676, 0.7972973 , 0.81418919,\n",
       "        0.81891892, 0.75945946, 0.79391892, 0.81756757, 0.81689189,\n",
       "        0.76081081, 0.80878378, 0.82162162, 0.83108108, 0.7472973 ,\n",
       "        0.80135135, 0.81959459, 0.82094595, 0.7527027 , 0.80135135,\n",
       "        0.81081081, 0.81689189, 0.75472973, 0.79864865, 0.81418919,\n",
       "        0.81621622, 0.75675676, 0.78716216, 0.79324324, 0.78513514,\n",
       "        0.78175676, 0.81148649, 0.82162162, 0.81486486, 0.79121622,\n",
       "        0.82162162, 0.82972973, 0.83716216, 0.79054054, 0.82905405,\n",
       "        0.83310811, 0.84324324, 0.7722973 , 0.80337838, 0.81148649,\n",
       "        0.81486486, 0.79256757, 0.82364865, 0.84189189, 0.84527027,\n",
       "        0.78581081, 0.83310811, 0.84256757, 0.84932432, 0.79054054,\n",
       "        0.82972973, 0.84121622, 0.84662162, 0.76891892, 0.81216216,\n",
       "        0.83378378, 0.83716216, 0.78716216, 0.82702703, 0.84121622,\n",
       "        0.84932432, 0.78851351, 0.82635135, 0.83918919, 0.84662162,\n",
       "        0.79121622, 0.82297297, 0.82972973, 0.83716216, 0.77297297,\n",
       "        0.82094595, 0.83851351, 0.83243243, 0.78513514, 0.825     ,\n",
       "        0.83918919, 0.84797297, 0.78445946, 0.82162162, 0.83581081,\n",
       "        0.84189189, 0.79121622, 0.81959459, 0.82567568, 0.83243243,\n",
       "        0.76418919, 0.79256757, 0.80675676, 0.79864865, 0.78581081,\n",
       "        0.81418919, 0.83851351, 0.83716216, 0.78851351, 0.82972973,\n",
       "        0.83716216, 0.84391892, 0.7972973 , 0.83108108, 0.83918919,\n",
       "        0.84864865, 0.78175676, 0.81283784, 0.83243243, 0.84121622,\n",
       "        0.79054054, 0.83445946, 0.84459459, 0.84459459, 0.79054054,\n",
       "        0.83243243, 0.83648649, 0.84797297, 0.78851351, 0.8222973 ,\n",
       "        0.83310811, 0.83986486, 0.77635135, 0.82094595, 0.83851351,\n",
       "        0.84662162, 0.79121622, 0.81959459, 0.83108108, 0.84527027,\n",
       "        0.78783784, 0.82094595, 0.8277027 , 0.83648649, 0.78648649,\n",
       "        0.81959459, 0.82432432, 0.82702703, 0.77635135, 0.82027027,\n",
       "        0.83986486, 0.84662162, 0.78378378, 0.82297297, 0.83310811,\n",
       "        0.83918919, 0.78175676, 0.82027027, 0.82972973, 0.83175676,\n",
       "        0.775     , 0.81283784, 0.82567568, 0.8277027 , 0.7722973 ,\n",
       "        0.8222973 , 0.84256757, 0.85067568, 0.77702703, 0.83851351,\n",
       "        0.83243243, 0.85135135, 0.77635135, 0.825     , 0.83040541,\n",
       "        0.84797297, 0.76283784, 0.81891892, 0.8222973 , 0.83851351,\n",
       "        0.75405405, 0.825     , 0.83243243, 0.84594595, 0.74121622,\n",
       "        0.80945946, 0.81756757, 0.82702703, 0.73851351, 0.81013514,\n",
       "        0.82094595, 0.82364865, 0.75      , 0.8027027 , 0.81418919,\n",
       "        0.82297297, 0.76013514, 0.81216216, 0.825     , 0.83243243,\n",
       "        0.75540541, 0.7972973 , 0.81891892, 0.82702703, 0.75472973,\n",
       "        0.79864865, 0.81756757, 0.82837838, 0.76351351, 0.79527027,\n",
       "        0.81081081, 0.83175676, 0.76013514, 0.81283784, 0.825     ,\n",
       "        0.82905405, 0.75540541, 0.80405405, 0.81283784, 0.8277027 ,\n",
       "        0.75472973, 0.79662162, 0.81418919, 0.82567568, 0.76351351,\n",
       "        0.78783784, 0.80743243, 0.82094595]),\n",
       " 'std_test_score': array([0.01857801, 0.02506383, 0.03003528, 0.02193606, 0.03300453,\n",
       "        0.03434666, 0.03758361, 0.03387825, 0.03989348, 0.02887278,\n",
       "        0.03836505, 0.03928228, 0.01703989, 0.0142694 , 0.01761947,\n",
       "        0.02097861, 0.02011199, 0.01874924, 0.02042732, 0.02373536,\n",
       "        0.03051781, 0.02585287, 0.02761191, 0.02467835, 0.03111044,\n",
       "        0.02439928, 0.03251678, 0.02110878, 0.02060534, 0.01253192,\n",
       "        0.02130254, 0.01401111, 0.01930116, 0.02451129, 0.02373536,\n",
       "        0.02363899, 0.03054772, 0.02458568, 0.02774387, 0.02611642,\n",
       "        0.029109  , 0.02855479, 0.03305981, 0.0260464 , 0.01820567,\n",
       "        0.01695932, 0.02402215, 0.021537  , 0.01930116, 0.02011199,\n",
       "        0.02121665, 0.02295309, 0.03054772, 0.02295309, 0.03074139,\n",
       "        0.02326916, 0.029109  , 0.02722898, 0.03214967, 0.02861867,\n",
       "        0.01820567, 0.01838037, 0.021537  , 0.02467835, 0.02162162,\n",
       "        0.03187872, 0.0382578 , 0.03326631, 0.0317208 , 0.0260464 ,\n",
       "        0.03435995, 0.0260464 , 0.02558662, 0.01709339, 0.02075985,\n",
       "        0.02291328, 0.01676983, 0.01480331, 0.01894303, 0.01324049,\n",
       "        0.02033773, 0.02613389, 0.03449256, 0.02031527, 0.0279733 ,\n",
       "        0.01779992, 0.03216387, 0.02164273, 0.02086952, 0.01489555,\n",
       "        0.02206058, 0.01807985, 0.00448193, 0.01033585, 0.01891892,\n",
       "        0.01401111, 0.02321022, 0.02690852, 0.02949849, 0.02551515,\n",
       "        0.02940548, 0.01717333, 0.02576443, 0.02185265, 0.02257202,\n",
       "        0.01037993, 0.02086952, 0.01646765, 0.00626596, 0.01253192,\n",
       "        0.01663315, 0.01934841, 0.02321022, 0.02813603, 0.0303979 ,\n",
       "        0.02608143, 0.02940548, 0.01894303, 0.02540756, 0.02291328,\n",
       "        0.02257202, 0.01037993, 0.02210193, 0.02206058, 0.00626596,\n",
       "        0.01024713, 0.01407613, 0.02247066, 0.02245033, 0.01374797,\n",
       "        0.01676983, 0.01635638, 0.00626596, 0.02042732, 0.0142694 ,\n",
       "        0.01162476, 0.01986073, 0.02195686, 0.01391301, 0.01238534,\n",
       "        0.02421145, 0.01253192, 0.01033585, 0.01439681, 0.01695932,\n",
       "        0.01439681, 0.02143075, 0.0147415 , 0.01142671, 0.01271276,\n",
       "        0.01575933, 0.01923007, 0.01870048, 0.01855342, 0.01613154,\n",
       "        0.01807985, 0.01756757, 0.01767121, 0.01489555, 0.01549641,\n",
       "        0.01341178, 0.01918253, 0.01341178, 0.01903919, 0.01391301,\n",
       "        0.01761947, 0.01219964, 0.02307212, 0.01138669, 0.01709339,\n",
       "        0.0142694 , 0.0233475 , 0.02027027, 0.01341178, 0.00496518,\n",
       "        0.02220497, 0.01341178, 0.01730574, 0.01101994, 0.02469685,\n",
       "        0.01391301, 0.01855342, 0.00870547, 0.02130254, 0.01138669,\n",
       "        0.02108714, 0.01059756, 0.02042732, 0.02027027, 0.01870048,\n",
       "        0.00988428, 0.02285343, 0.0198147 , 0.02497259, 0.02255178,\n",
       "        0.02100037, 0.01162476, 0.02385049, 0.02086952, 0.02015734,\n",
       "        0.02301268, 0.02672124, 0.02011199, 0.02360034, 0.01227426,\n",
       "        0.02467835, 0.01899117, 0.01915871, 0.02307212, 0.02540756,\n",
       "        0.02033773, 0.02794064, 0.01341178, 0.0183057 , 0.0206717 ,\n",
       "        0.02639463, 0.0142694 , 0.01289107, 0.01845473, 0.02576443,\n",
       "        0.00782151, 0.01274862, 0.0169054 , 0.02533559, 0.01825575,\n",
       "        0.02756227, 0.0284587 , 0.02690852, 0.00752401, 0.01378113,\n",
       "        0.01234842, 0.02311166, 0.01458583, 0.01976857, 0.01730574,\n",
       "        0.02042732, 0.00641002, 0.01306695, 0.01613154, 0.01306695,\n",
       "        0.01825575, 0.02895174, 0.02971437, 0.03197881, 0.00880973,\n",
       "        0.01374797, 0.0159322 , 0.02629064, 0.01245884, 0.0179277 ,\n",
       "        0.01590352, 0.02510024, 0.00689057, 0.02075985, 0.01903919,\n",
       "        0.01525891, 0.02602887, 0.02790794, 0.02373536, 0.02432432,\n",
       "        0.01002189, 0.03132979, 0.02572896, 0.02373536, 0.01097843,\n",
       "        0.02324953, 0.01748943, 0.01800394, 0.01439681, 0.0206496 ,\n",
       "        0.02164273, 0.01951288, 0.01549641, 0.02130254, 0.01741094,\n",
       "        0.02385049, 0.01055439, 0.0153484 , 0.01607483, 0.02354223,\n",
       "        0.00721424, 0.01344578, 0.01549641, 0.02151579, 0.01256829,\n",
       "        0.01011259, 0.01458583, 0.01997533, 0.01695932, 0.0147415 ,\n",
       "        0.0198147 , 0.02344507, 0.00941107, 0.01292644, 0.01130622,\n",
       "        0.01754156, 0.01142671, 0.01024713, 0.0147105 , 0.01632844,\n",
       "        0.00965058, 0.00880973, 0.0179277 , 0.0145545 , 0.01695932,\n",
       "        0.01504801, 0.01825575, 0.02108714, 0.00838491, 0.01857801,\n",
       "        0.01407613, 0.02315114, 0.01162476, 0.01219964, 0.01391301,\n",
       "        0.0146483 , 0.01097843, 0.00988428, 0.01618804, 0.00787966,\n",
       "        0.00993036, 0.02385049, 0.02887278, 0.02407909, 0.01407613,\n",
       "        0.02585287, 0.01374797, 0.02172694, 0.01118442, 0.01519895,\n",
       "        0.01555522, 0.01741094, 0.00782151, 0.01341178, 0.01238534,\n",
       "        0.02385049, 0.01519895, 0.01327492, 0.02291328, 0.02143075,\n",
       "        0.01717333, 0.0118195 , 0.01820567, 0.01219964, 0.01833062,\n",
       "        0.01292644, 0.0147105 , 0.01306695, 0.01358091, 0.02031527,\n",
       "        0.01862709, 0.01407613, 0.01756757, 0.00891277, 0.02056098,\n",
       "        0.0147105 , 0.01394579, 0.02388874, 0.02172694, 0.01118442,\n",
       "        0.02714501, 0.02020259, 0.03200735, 0.01289107, 0.01974546,\n",
       "        0.02315114, 0.02394601, 0.01682419, 0.01756757, 0.02230753,\n",
       "        0.01795315, 0.01442848, 0.01394579, 0.02121665, 0.02234843,\n",
       "        0.01635638, 0.02714501, 0.02326916, 0.02625589, 0.01401111,\n",
       "        0.01974546, 0.023793  , 0.02295309, 0.01341178, 0.01838037,\n",
       "        0.02464133, 0.0206496 , 0.02893596, 0.01769703, 0.02464133,\n",
       "        0.02075985, 0.02392693, 0.01607483, 0.02439928, 0.02275332,\n",
       "        0.02660138, 0.0082753 , 0.02097861, 0.01769703, 0.02143075,\n",
       "        0.02121665, 0.02275332, 0.02307212, 0.02761191, 0.01510857,\n",
       "        0.0242303 , 0.01820567, 0.02411698, 0.01110248, 0.01575933,\n",
       "        0.01974546, 0.02324953, 0.00458266, 0.01674258, 0.02569345,\n",
       "        0.02549725, 0.0236583 , 0.01743715, 0.02086952, 0.02707766,\n",
       "        0.01310184, 0.02247066, 0.02388874, 0.02682356, 0.00880973,\n",
       "        0.02157935, 0.02078183, 0.02794064, 0.01162476, 0.02078183,\n",
       "        0.01570128, 0.02317085, 0.0206496 , 0.01607483, 0.02230753,\n",
       "        0.02285343, 0.0146483 , 0.02015734, 0.01906316, 0.02356162,\n",
       "        0.00969777, 0.01621622, 0.01519895, 0.02782603, 0.01341178,\n",
       "        0.01391301, 0.01501764, 0.02157935, 0.01795315, 0.01891892,\n",
       "        0.02247066, 0.02531756, 0.02004378, 0.02247066, 0.02143075,\n",
       "        0.02206058, 0.01546692, 0.01646765, 0.02206058, 0.01894303,\n",
       "        0.02307212, 0.01903919, 0.01879788, 0.0183057 , 0.02100037,\n",
       "        0.02020259, 0.02346453, 0.02569345, 0.00870547, 0.01962951,\n",
       "        0.02402215, 0.02413591, 0.01510857, 0.01607483, 0.02307212,\n",
       "        0.02549725, 0.01394579, 0.01391301, 0.01743715, 0.01825575,\n",
       "        0.02011199, 0.01489555, 0.02807105, 0.02551515, 0.01838037,\n",
       "        0.01590352, 0.02317085, 0.02558662, 0.01767121, 0.01442848,\n",
       "        0.0259058 , 0.02315114, 0.0142694 , 0.0198837 , 0.01965276,\n",
       "        0.01838037, 0.01906316, 0.01391301, 0.02189439, 0.02164273,\n",
       "        0.01894303, 0.02078183, 0.01743715, 0.02206058, 0.01264073,\n",
       "        0.01613154, 0.02162162, 0.01941906, 0.01635638, 0.01915871,\n",
       "        0.01668796, 0.01519895, 0.02027027, 0.01717333, 0.0147105 ,\n",
       "        0.02151579, 0.01068337, 0.00870547, 0.02203987, 0.01941906,\n",
       "        0.01361449, 0.01341178, 0.0253716 , 0.01621622, 0.01170305,\n",
       "        0.01002189, 0.01327492, 0.02056098, 0.01621622, 0.02731268,\n",
       "        0.01997533, 0.0221432 , 0.01015763, 0.02185265, 0.01727934,\n",
       "        0.01264073, 0.02404114, 0.01903919, 0.0169054 , 0.0118965 ,\n",
       "        0.03085997, 0.02143075, 0.01730574, 0.01101994, 0.02660138,\n",
       "        0.01575933, 0.01635638, 0.01800394, 0.02130254, 0.0159322 ,\n",
       "        0.01504801, 0.01407613, 0.01641211, 0.01774855, 0.01899117,\n",
       "        0.01442848, 0.02877776, 0.02555091, 0.0198147 , 0.01604641,\n",
       "        0.0249543 , 0.02022518, 0.01867605, 0.01555522, 0.02086952,\n",
       "        0.02203987, 0.01807985, 0.01299688, 0.01646765, 0.01818057,\n",
       "        0.02220497, 0.01439681, 0.0236583 , 0.03026243, 0.02240963,\n",
       "        0.01743715, 0.02685758, 0.02369686, 0.02324953, 0.02394601,\n",
       "        0.02732939, 0.01962951, 0.02747932, 0.02354223, 0.01178081,\n",
       "        0.01306695, 0.02749593, 0.02722898, 0.01368139, 0.01795315,\n",
       "        0.021537  , 0.02267292, 0.02757883, 0.01674258, 0.02108714,\n",
       "        0.02680653, 0.0198837 , 0.02031527, 0.02140943, 0.02373536,\n",
       "        0.01256829, 0.01769703, 0.01769703, 0.02524533, 0.01862709,\n",
       "        0.01489555, 0.02210193, 0.02441799, 0.01635638, 0.01549641,\n",
       "        0.0221432 , 0.0206496 , 0.02027027, 0.02031527, 0.01862709,\n",
       "        0.0242303 , 0.01504801, 0.02033773, 0.01458583, 0.02665281,\n",
       "        0.01271276, 0.01621622, 0.01649535, 0.02324953, 0.01891892,\n",
       "        0.01401111, 0.01855342, 0.02551515, 0.01549641, 0.01974546,\n",
       "        0.01717333, 0.02668705, 0.01501764, 0.01727934, 0.01378113,\n",
       "        0.02540756, 0.01178081, 0.01578827, 0.01519895, 0.0236583 ,\n",
       "        0.0233475 , 0.02404114, 0.02383134, 0.02451129, 0.021537  ,\n",
       "        0.01627243, 0.02467835, 0.02439928, 0.01549641, 0.01578827,\n",
       "        0.0206496 , 0.01867605, 0.01911099, 0.015846  , 0.02326916,\n",
       "        0.01727934, 0.01519895, 0.01965276, 0.02226656, 0.02311166,\n",
       "        0.01555522, 0.01384723, 0.02240963, 0.02230753, 0.01046752,\n",
       "        0.01489555, 0.02216381, 0.02519102, 0.01769703, 0.01361449,\n",
       "        0.01489555, 0.02164273, 0.00988428, 0.02445535, 0.01927749,\n",
       "        0.02078183, 0.01891892, 0.02044966, 0.02445535, 0.02430555,\n",
       "        0.01604641, 0.01887059, 0.02291328, 0.0247891 , 0.01234842,\n",
       "        0.02247066, 0.01874924, 0.02097861, 0.01407613, 0.02383134,\n",
       "        0.021537  , 0.02346453, 0.01480331, 0.02056098, 0.01649535,\n",
       "        0.02515474, 0.01238534, 0.02392693, 0.01442848, 0.02558662,\n",
       "        0.01489555, 0.02143075, 0.0147415 , 0.02271316, 0.01361449,\n",
       "        0.01327492, 0.01782555, 0.01903919, 0.02546141, 0.01754156,\n",
       "        0.021537  , 0.01709339, 0.01501764, 0.01727934, 0.02119512,\n",
       "        0.01722642, 0.01674258, 0.01976857, 0.0142694 , 0.02193606,\n",
       "        0.0183057 , 0.01674258, 0.02394601, 0.02121665, 0.02247066,\n",
       "        0.01564302, 0.01722642, 0.01531863, 0.01635638, 0.01341178,\n",
       "        0.01695932, 0.01289107, 0.01992957, 0.01953626, 0.01695932,\n",
       "        0.01394579, 0.00880973, 0.02132396, 0.01997533, 0.02385049,\n",
       "        0.02100037, 0.01510857, 0.01976857, 0.02119512, 0.02673832,\n",
       "        0.02100037, 0.02115199, 0.01741094, 0.02702703, 0.02078183,\n",
       "        0.0253716 , 0.01818057, 0.00880973, 0.01344578, 0.02053876,\n",
       "        0.02656703, 0.02100037, 0.02015734, 0.02044966, 0.02060534,\n",
       "        0.02673832, 0.02203987, 0.0178767 , 0.01906316, 0.02702703,\n",
       "        0.02373536, 0.02015734, 0.02093505]),\n",
       " 'rank_test_score': array([765, 764, 756, 753, 759, 729, 692, 668, 722, 482, 547, 541, 592,\n",
       "        261, 289, 102, 766, 762, 750, 747, 752, 719, 675, 649, 675, 616,\n",
       "        592, 595, 608, 423, 398, 434, 766, 761, 757, 749, 753, 730, 716,\n",
       "        691, 658, 614, 630, 645, 595, 462, 481, 557, 766, 763, 760, 750,\n",
       "        753, 725, 712, 716, 658, 619, 636, 677, 595, 477, 482, 610, 758,\n",
       "        744, 740, 737, 706, 468, 355, 316, 642, 365, 209,  99, 601, 167,\n",
       "        108,  70, 748, 743, 727, 721, 677, 530, 488, 582, 632, 452, 401,\n",
       "        447, 591, 413, 358, 418, 745, 742, 739, 735, 677, 595, 605, 646,\n",
       "        626, 482, 492, 533, 578, 455, 448, 477, 745, 741, 738, 732, 677,\n",
       "        584, 602, 652, 626, 482, 474, 592, 578, 434, 459, 488, 725, 533,\n",
       "        411, 194, 689, 414, 114,  22, 616, 380, 175,  79, 632, 328, 253,\n",
       "        129, 731, 533, 448, 429, 626, 459, 250, 397, 702, 463, 289, 375,\n",
       "        658, 446, 269, 380, 733, 595, 548, 602, 647, 493, 401, 448, 642,\n",
       "        434, 427, 423, 637, 467, 463, 418, 733, 621, 562, 621, 647, 519,\n",
       "        409, 463, 642, 455, 401, 441, 637, 482, 389, 441, 708, 562, 548,\n",
       "        523, 595, 488, 341, 364, 569, 421, 225,  95, 509, 301,  38,   4,\n",
       "        699, 473, 401, 333, 588, 398, 147,  80, 559, 231,  73,  39, 548,\n",
       "        218,  49,  31, 708, 441, 433, 401, 530, 352, 209, 243, 553, 269,\n",
       "        138, 175, 509, 301, 122, 134, 708, 482, 463, 509, 545, 328, 138,\n",
       "        279, 569, 231, 121, 167, 533, 279, 175, 167, 692, 562, 441, 452,\n",
       "        518, 333,  95,  39, 502, 231,  14,   3, 502, 175,  51,  13, 635,\n",
       "        414, 278, 138, 589, 261, 114,  11, 589, 308,  73,  20, 525, 328,\n",
       "        147, 102, 637, 355, 243, 195, 533, 308, 165, 134, 559, 243, 209,\n",
       "        129, 502, 333, 279, 209, 637, 429, 243, 279, 548, 352, 195, 195,\n",
       "        553, 269, 253, 202, 533, 328, 279, 308, 685, 326,  80,  14, 692,\n",
       "        341,  70,  39, 671, 316,  95,  39, 651, 230, 191,  73, 667, 352,\n",
       "        122,  51, 685, 384, 181, 108, 682, 384, 253, 129, 698, 414, 365,\n",
       "        231, 723, 421, 261, 160, 658, 434, 341, 253, 668, 432, 389, 301,\n",
       "        655, 455, 426, 333, 723, 394, 221, 147, 658, 408, 209, 261, 668,\n",
       "        411, 289, 289, 655, 434, 389, 363, 712, 541, 519, 604, 559, 434,\n",
       "        341, 358, 499, 348, 195, 122, 509, 253,  44,  24, 685, 495, 401,\n",
       "        365, 572, 316, 114, 100, 572, 253,  59,  59, 525, 175,  46,  21,\n",
       "        685, 375, 327, 147, 522, 308,  68,  51, 572, 243,  51,  14, 499,\n",
       "        269, 122, 108, 692, 355, 248, 195, 545, 289, 147,  46, 553, 269,\n",
       "        114,  51, 577, 253, 163, 122, 664, 519, 452, 472, 509, 316, 108,\n",
       "         28, 533, 231,  46,   7, 499, 163,  70,   1, 621, 373, 269, 114,\n",
       "        578, 231,  59,  51, 572, 181,  28,  33, 525, 231, 147,  65, 658,\n",
       "        380, 181,  73, 576, 301, 114,  90, 605, 316, 167, 134, 578, 349,\n",
       "        181, 202, 664, 373, 206,  80, 562, 261, 138,  80, 607, 289, 195,\n",
       "        181, 584, 301, 202, 221, 631, 349,  88,   4, 618, 316,  59,   8,\n",
       "        626, 279, 122,  51, 637, 333, 138, 138, 682, 358, 129,  33, 664,\n",
       "        384, 250,  95, 689, 341, 218, 167, 682, 427, 225, 225, 708, 414,\n",
       "        221, 167, 714, 429, 316, 328, 696, 477, 365, 316, 677, 494, 333,\n",
       "        341, 671, 409, 279, 165, 720, 459, 308, 289, 714, 455, 394, 341,\n",
       "        705, 470, 365, 349, 696, 541, 496, 562, 582, 389, 279, 358, 502,\n",
       "        279, 188, 102, 509, 191, 137,  44, 621, 441, 389, 358, 497, 250,\n",
       "         64,  32, 553, 138,  51,   8, 509, 181,  65,  24, 634, 384, 129,\n",
       "        102, 541, 209,  65,   8, 524, 218,  80,  24, 502, 261, 188, 102,\n",
       "        619, 289,  90, 159, 562, 231,  80,  14, 568, 279, 114,  59, 502,\n",
       "        308, 225, 147, 649, 498, 423, 468, 557, 365,  90, 102, 525, 188,\n",
       "        100,  39, 474, 167,  80,  11, 584, 375, 147,  68, 509, 122,  36,\n",
       "         36, 509, 147, 108,  14, 525, 277, 138,  73, 610, 289,  90,  22,\n",
       "        502, 308, 167,  33, 540, 289, 206, 108, 548, 308, 248, 209, 610,\n",
       "        301,  73,  24, 569, 261, 138,  80, 584, 301, 181, 160, 614, 375,\n",
       "        225, 202, 621, 269,  49,   4, 608,  90, 147,   2, 610, 231, 175,\n",
       "         14, 655, 316, 269,  88, 706, 231, 147,  28, 728, 401, 333, 209,\n",
       "        735, 398, 289, 253, 718, 448, 365, 261, 673, 384, 231, 147, 699,\n",
       "        474, 316, 209, 703, 471, 333, 195, 652, 491, 394, 160, 673, 380,\n",
       "        231, 191, 699, 434, 375, 206, 703, 480, 365, 221, 652, 530, 420,\n",
       "        289], dtype=int32)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f984dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid, scoring='accuracy', cv=5, refit=True)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Now cv_results_ will contain train and test set scores\n",
    "cv_results = grid_search.cv_results_\n",
    "print('Train Set Mean Scores:')\n",
    "print(cv_results['mean_train_score'])\n",
    "print('Test Set Mean Scores:')\n",
    "print(cv_results['mean_test_score'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7d716f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cm_svc = confusion_matrix(y_test, predicted_xgb)\n",
    "labels = ['Negative', 'Neutral', 'Positive']\n",
    "sns.heatmap(cm_svc, annot=True, cmap='Blues', fmt='g', xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel('Predicted sentiment')\n",
    "plt.ylabel('True sentiment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe729b38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d305c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b16d9e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e17070",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e28b82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c56066",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.5],\n",
    "    'max_depth': [2, 5, 10, 15, 20],\n",
    "    'min_child_weight': [2, 4, 6, 8],\n",
    "    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2, 0.3, 0.4],\n",
    "    'n_estimators': [100, 200, 300, 400, 500]\n",
    "}\n",
    "\n",
    "# Initialize XGBClassifier\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(estimator = xgb, param_distributions = param_grid, n_iter = 50,\n",
    "                                   scoring = 'accuracy', cv = 5, verbose = 1, random_state = 42)\n",
    "\n",
    "# Perform the random search\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = random_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Get the best estimator\n",
    "best_xgb = random_search.best_estimator_\n",
    "\n",
    "# Evaluate the best estimator on the test set\n",
    "y_pred = best_xgb.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy on Test Set:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55870b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_svc = confusion_matrix(y_test, y_pred)\n",
    "labels = ['Negative', 'Neutral', 'Positive']\n",
    "sns.heatmap(cm_svc, annot=True, cmap='Blues', fmt='g', xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel('Predicted sentiment')\n",
    "plt.ylabel('True sentiment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2bd687",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdc2043",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfa3575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c569e99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "931ba264",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35d4bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "predicted_rf = rf.predict(X_test)\n",
    "accuracy_rf = accuracy_score(y_test, predicted_rf)\n",
    "print('Accuracy:', accuracy_rf)\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, predicted_rf))\n",
    "\n",
    "y_train_pred = rf.predict(X_train)\n",
    "y_test_pred = rf.predict(X_test)\n",
    "accuracy_train = accuracy_score(y_train, y_train_pred)\n",
    "accuracy_test = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Accuracy on Training Set:\", accuracy_train)\n",
    "print(\"Accuracy on Test Set:\", accuracy_test)\n",
    "\n",
    "cm_rf = confusion_matrix(y_test, predicted_rf)\n",
    "labels = ['Negative', 'Neutral', 'Positive']\n",
    "sns.heatmap(cm_rf, annot=True, cmap='Blues', fmt='g', xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel('Predicted sentiment')\n",
    "plt.ylabel('True sentiment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a22c23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d03da2d",
   "metadata": {},
   "source": [
    "## Naive Bayes Multinominal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a719450",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "predicted_nb = nb.predict(X_test)\n",
    "accuracy_nb = accuracy_score(y_test, predicted_nb)\n",
    "print('Accuracy:', accuracy_nb)\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, predicted_nb))\n",
    "\n",
    "y_train_pred = nb.predict(X_train)\n",
    "y_test_pred = nb.predict(X_test)\n",
    "accuracy_train = accuracy_score(y_train, y_train_pred)\n",
    "accuracy_test = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Accuracy on Training Set:\", accuracy_train)\n",
    "print(\"Accuracy on Test Set:\", accuracy_test)\n",
    "\n",
    "cm_nb = confusion_matrix(y_test, predicted_nb)\n",
    "labels = ['Negative', 'Neutral', 'Positive']\n",
    "sns.heatmap(cm_nb, annot=True, cmap='Blues', fmt='g', xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel('Predicted sentiment')\n",
    "plt.ylabel('True sentiment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a04b83",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eae878",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4a6d451",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ef136a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier()\n",
    "xgb.fit(X_train,y_train)\n",
    "predicted_xgb = xgb.predict(X_test)\n",
    "accuracy_xgb = accuracy_score(y_test, predicted_xgb)\n",
    "print('Accuracy:', accuracy_xgb)\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, predicted_xgb))\n",
    "\n",
    "y_train_pred = xgb.predict(X_train)\n",
    "y_test_pred = xgb.predict(X_test)\n",
    "accuracy_train = accuracy_score(y_train, y_train_pred)\n",
    "accuracy_test = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Accuracy on Training Set:\", accuracy_train)\n",
    "print(\"Accuracy on Test Set:\", accuracy_test)\n",
    "\n",
    "\n",
    "cm_xgb = confusion_matrix(y_test, predicted_xgb)\n",
    "labels = ['Negative', 'Neutral', 'Positive']\n",
    "sns.heatmap(cm_xgb, annot = True, cmap = 'Blues', fmt = 'g', xticklabels = labels, yticklabels = labels)\n",
    "plt.xlabel('Predicted sentiment')\n",
    "plt.ylabel('True sentiment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4b62b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f10383f9",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bac193f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(max_iter=500)\n",
    "lr.fit(X_train, y_train)\n",
    "predicted_lr = lr.predict(X_test)\n",
    "accuracy_lr = accuracy_score(y_test, predicted_lr)\n",
    "print('Accuracy:', accuracy_lr)\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, predicted_lr))\n",
    "\n",
    "y_train_pred = lr.predict(X_train)\n",
    "y_test_pred = lr.predict(X_test)\n",
    "accuracy_train = accuracy_score(y_train, y_train_pred)\n",
    "accuracy_test = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Accuracy on Training Set:\", accuracy_train)\n",
    "print(\"Accuracy on Test Set:\", accuracy_test)\n",
    "\n",
    "cm_lr = confusion_matrix(y_test, predicted_lr)\n",
    "labels = ['Negative', 'Neutral', 'Positive']\n",
    "sns.heatmap(cm_lr, annot=True, cmap='Blues', fmt='g', xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel('Predicted sentiment')\n",
    "plt.ylabel('True sentiment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f101729",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21690427",
   "metadata": {},
   "source": [
    "## Linear Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6d9f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = LinearSVC(random_state=12)\n",
    "svc.fit(X_train, y_train)\n",
    "predicted_svc = svc.predict(X_test)\n",
    "accuracy_svc = accuracy_score(y_test, predicted_svc)\n",
    "print('Accuracy:', accuracy_svc)\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, predicted_svc))\n",
    "\n",
    "y_train_pred = svc.predict(X_train)\n",
    "y_test_pred = svc.predict(X_test)\n",
    "accuracy_train = accuracy_score(y_train, y_train_pred)\n",
    "accuracy_test = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Accuracy on Training Set:\", accuracy_train)\n",
    "print(\"Accuracy on Test Set:\", accuracy_test)\n",
    "\n",
    "\n",
    "cm_svc = confusion_matrix(y_test, predicted_svc)\n",
    "labels = ['Negative', 'Neutral', 'Positive']\n",
    "sns.heatmap(cm_svc, annot=True, cmap='Blues', fmt='g', xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel('Predicted sentiment')\n",
    "plt.ylabel('True sentiment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f66cf5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8294b04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f39baa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee61da5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecda25b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba93be31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f7e7fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fb6441",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b9e865",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
